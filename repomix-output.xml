This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
appendix-A/
  01_main-chapter-code/
    code-part1.ipynb
    code-part2.ipynb
    DDP-script.py
    exercise-solutions.ipynb
  02_setup-recommendations/
    README.md
appendix-D/
  01_main-chapter-code/
    appendix-D.ipynb
    previous_chapters.py
    the-verdict.txt
  README.md
appendix-E/
  01_main-chapter-code/
    appendix-E.ipynb
    gpt_download.py
    previous_chapters.py
  README.md
ch01/
  README.md
ch02/
  01_main-chapter-code/
    ch02.ipynb
    dataloader.ipynb
    exercise-solutions.ipynb
    README.md
    the-verdict.txt
  02_bonus_bytepair-encoder/
    bpe_openai_gpt2.py
    compare-bpe-tiktoken.ipynb
    README.md
    requirements-extra.txt
  03_bonus_embedding-vs-matmul/
    embeddings-and-linear-layers.ipynb
    README.md
  04_bonus_dataloader-intuition/
    dataloader-intuition.ipynb
    README.md
  05_bpe-from-scratch/
    bpe-from-scratch.ipynb
  image/
    01.webp
    02.webp
    03.webp
    04.webp
    05.webp
    06.webp
    07.webp
    08.webp
    09.webp
    10.webp
    11.webp
    12.webp
    13.webp
    14.webp
    15.webp
    16.webp
    17.webp
    18.webp
    19.webp
    bpe-overview.webp
    cover-small.webp
    tiktokenizer.webp
  README.md
ch03/
  01_main-chapter-code/
    ch03.ipynb
    exercise-solutions.ipynb
    multihead-attention.ipynb
    README.md
    small-text-sample.txt
  02_bonus_efficient-multihead-attention/
    mha-implementations.ipynb
    README.md
  03_understanding-buffers/
    README.md
    understanding-buffers.ipynb
  image/
    01.webp
    02.webp
    03.webp
    04.webp
    05.webp
    06.webp
    07.webp
    08.webp
    09.webp
    10.webp
    11.webp
    12.webp
    13.webp
    14.webp
    15.webp
    16.webp
    17.webp
    18.webp
    19.webp
    20.webp
    21.webp
    22.webp
    23.webp
    24.webp
    25.webp
    26.webp
    cover-small.webp
  README.md
ch04/
  01_main-chapter-code/
    ch04.ipynb
    gpt.py
    previous_chapters.py
    README.md
    tests.py
  02_performance-analysis/
    flops-analysis.ipynb
    previous_chapters.py
    README.md
    requirements-extra.txt
  image/
    01.webp
    02.webp
    03.webp
    04.webp
    05.webp
    06.webp
    07.webp
    09.webp
    10.webp
    11.webp
    12.webp
    13.webp
    14.webp
    15.webp
    16.webp
    17.webp
    18.webp
    cover-small.webp
  exercise-solutions.ipynb
  README.md
ch05/
  01_main-chapter-code/
    ch05.ipynb
    exercise-solutions.ipynb
    gpt_download.py
    gpt_generate.py
    gpt_train.py
    previous_chapters.py
    README.md
    tests.py
  02_alternative_weight_loading/
    previous_chapters.py
    README.md
    weight-loading-hf-transformers.ipynb
  03_bonus_pretraining_on_gutenberg/
    prepare_dataset.py
    pretraining_simple.py
    previous_chapters.py
    README.md
    tests.py
  04_learning_rate_schedulers/
    README.md
  05_bonus_hparam_tuning/
    hparam_search.py
    previous_chapters.py
    README.md
    the-verdict.txt
  06_user_interface/
    app_orig.py
    app_own.py
    previous_chapters.py
    README.md
    requirements-extra.txt
  07_gpt_to_llama/
    tests/
      test-requirements-extra.txt
      tests.py
    config.json
    converting-gpt-to-llama2.ipynb
    converting-llama2-to-llama3.ipynb
    previous_chapters.py
    README.md
    requirements-extra.txt
    standalone-llama32.ipynb
  08_memory_efficient_weight_loading/
    memory-efficient-state-dict.ipynb
    previous_chapters.py
    README.md
  09_extending-tokenizers/
    extend-tiktoken.ipynb
    gpt_download.py
    previous_chapters.py
    README.md
  image/
    access-token.webp
    batching.webp
    chapter-overview.webp
    cover-small.webp
    cross-entropy.webp
    gpt-and-all-llamas.webp
    gpt-process.webp
    gpt-sizes.webp
    gpt-updates.webp
    gpt2-to-llama2-llama3.webp
    grouped-query-attention.webp
    llama3-to-llama31.webp
    llama31-to-llama32.webp
    llama32.webp
    memory-efficient-loading.webp
    mental-model--0.webp
    mental-model-1.webp
    mental-model-2.webp
    mental-model-3.webp
    proba-index.webp
    proba-to-text.webp
    settings.webp
    topk.webp
    train-steps.webp
  README.md
ch06/
  01_main-chapter-code/
    ch06.ipynb
    exercise-solutions.ipynb
    gpt_class_finetune.py
    gpt_download.py
    load-finetuned-model.ipynb
    previous_chapters.py
    README.md
    tests.py
  02_bonus_additional-experiments/
    additional_experiments.py
    gpt_download.py
    previous_chapters.py
    README.md
  03_bonus_imdb-classification/
    download_prepare_dataset.py
    gpt_download.py
    previous_chapters.py
    README.md
    requirements-extra.txt
    sklearn-baseline.ipynb
    train_bert_hf_spam.py
    train_bert_hf.py
    train_gpt.py
    train_sklearn_logreg.py
  04_user_interface/
    app.py
    previous_chapters.py
    README.md
    requirements-extra.txt
  image/
    cover-small.webp
  README.md
ch07/
  01_main-chapter-code/
    ch07.ipynb
    exercise_experiments.py
    exercise-solutions.ipynb
    gpt_download.py
    gpt_instruction_finetuning.py
    load-finetuned-model.ipynb
    ollama_evaluate.py
    previous_chapters.py
    README.md
    tests.py
  02_dataset-utilities/
    config.json
    create-passive-voice-entries.ipynb
    find-near-duplicates.py
    instruction-examples-modified.json
    instruction-examples.json
    README.md
    requirements-extra.txt
  03_model-evaluation/
    scores/
      correlation-analysis.ipynb
      gpt4-model-1-response.json
      gpt4-model-2-response.json
      llama3-8b-model-1-response.json
      llama3-8b-model-2-response.json
    config.json
    eval-example-data.json
    llm-instruction-eval-ollama.ipynb
    llm-instruction-eval-openai.ipynb
    README.md
    requirements-extra.txt
  04_preference-tuning-with-dpo/
    create-preference-data-ollama.ipynb
    dpo-from-scratch.ipynb
    previous_chapters.py
    README.md
  05_dataset-generation/
    config.json
    llama3-ollama.ipynb
    README.md
    reflection-gpt4.ipynb
    requirements-extra.txt
  06_user_interface/
    app.py
    previous_chapters.py
    README.md
    requirements-extra.txt
  image/
    1.webp
    2.webp
    3.webp
    4.webp
    5.webp
    cover-small.webp
    ollama-run.webp
    reflection-tuning.webp
  README.md
setup/
  01_optional-python-setup-preferences/
    README.md
  02_installing-python-libraries/
    python_environment_check.ipynb
    python_environment_check.py
    README.md
    tests.py
  03_optional-docker-environment/
    .devcontainer/
      devcontainer.json
      Dockerfile
      README.md
    README.md
  04_optional-aws-sagemaker-notebook/
    cloudformation-template.yml
    README.md
  README.md
.repomixignore
AGENTS.md
CITATION.cff
LICENSE.txt
README.md
requirements.txt
the-verdict.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="appendix-A/01_main-chapter-code/DDP-script.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# Appendix A: Introduction to PyTorch (Part 3)

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# NEW imports:
import os
import platform
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group


# NEW: function to initialize a distributed process group (1 process / GPU)
# this allows communication among processes
def ddp_setup(rank, world_size):
    """
    Arguments:
        rank: a unique process ID
        world_size: total number of processes in the group
    """
    # rank of machine running rank:0 process
    # here, we assume all GPUs are on the same machine
    os.environ["MASTER_ADDR"] = "localhost"
    # any free port on the machine
    os.environ["MASTER_PORT"] = "12345"
    if platform.system() == "Windows":
        # Disable libuv because PyTorch for Windows isn't built with support
        os.environ["USE_LIBUV"] = "0"

    # initialize process group
    if platform.system() == "Windows":
        # Windows users may have to use "gloo" instead of "nccl" as backend
        # gloo: Facebook Collective Communication Library
        init_process_group(backend="gloo", rank=rank, world_size=world_size)
    else:
        # nccl: NVIDIA Collective Communication Library
        init_process_group(backend="nccl", rank=rank, world_size=world_size)

    torch.cuda.set_device(rank)


class ToyDataset(Dataset):
    def __init__(self, X, y):
        self.features = X
        self.labels = y

    def __getitem__(self, index):
        one_x = self.features[index]
        one_y = self.labels[index]
        return one_x, one_y

    def __len__(self):
        return self.labels.shape[0]


class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super().__init__()

        self.layers = torch.nn.Sequential(
            # 1st hidden layer
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),

            # 2nd hidden layer
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),

            # output layer
            torch.nn.Linear(20, num_outputs),
        )

    def forward(self, x):
        logits = self.layers(x)
        return logits


def prepare_dataset():
    X_train = torch.tensor([
        [-1.2, 3.1],
        [-0.9, 2.9],
        [-0.5, 2.6],
        [2.3, -1.1],
        [2.7, -1.5]
    ])
    y_train = torch.tensor([0, 0, 0, 1, 1])

    X_test = torch.tensor([
        [-0.8, 2.8],
        [2.6, -1.6],
    ])
    y_test = torch.tensor([0, 1])

    train_ds = ToyDataset(X_train, y_train)
    test_ds = ToyDataset(X_test, y_test)

    train_loader = DataLoader(
        dataset=train_ds,
        batch_size=2,
        shuffle=False,  # NEW: False because of DistributedSampler below
        pin_memory=True,
        drop_last=True,
        # NEW: chunk batches across GPUs without overlapping samples:
        sampler=DistributedSampler(train_ds)  # NEW
    )
    test_loader = DataLoader(
        dataset=test_ds,
        batch_size=2,
        shuffle=False,
    )
    return train_loader, test_loader


# NEW: wrapper
def main(rank, world_size, num_epochs):

    ddp_setup(rank, world_size)  # NEW: initialize process groups

    train_loader, test_loader = prepare_dataset()
    model = NeuralNetwork(num_inputs=2, num_outputs=2)
    model.to(rank)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

    model = DDP(model, device_ids=[rank])  # NEW: wrap model with DDP
    # the core model is now accessible as model.module

    for epoch in range(num_epochs):
        # NEW: Set sampler to ensure each epoch has a different shuffle order
        train_loader.sampler.set_epoch(epoch)

        model.train()
        for features, labels in train_loader:

            features, labels = features.to(rank), labels.to(rank)  # New: use rank
            logits = model(features)
            loss = F.cross_entropy(logits, labels)  # Loss function

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # LOGGING
            print(f"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}"
                  f" | Batchsize {labels.shape[0]:03d}"
                  f" | Train/Val Loss: {loss:.2f}")

    model.eval()
    train_acc = compute_accuracy(model, train_loader, device=rank)
    print(f"[GPU{rank}] Training accuracy", train_acc)
    test_acc = compute_accuracy(model, test_loader, device=rank)
    print(f"[GPU{rank}] Test accuracy", test_acc)

    destroy_process_group()  # NEW: cleanly exit distributed mode


def compute_accuracy(model, dataloader, device):
    model = model.eval()
    correct = 0.0
    total_examples = 0

    for idx, (features, labels) in enumerate(dataloader):
        features, labels = features.to(device), labels.to(device)

        with torch.no_grad():
            logits = model(features)
        predictions = torch.argmax(logits, dim=1)
        compare = labels == predictions
        correct += torch.sum(compare)
        total_examples += len(compare)
    return (correct / total_examples).item()


if __name__ == "__main__":
    print("PyTorch version:", torch.__version__)
    print("CUDA available:", torch.cuda.is_available())
    print("Number of GPUs available:", torch.cuda.device_count())

    torch.manual_seed(123)

    # NEW: spawn new processes
    # note that spawn will automatically pass the rank
    num_epochs = 3
    world_size = torch.cuda.device_count()
    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size)
    # nprocs=world_size spawns one process per GPU
</file>

<file path="appendix-A/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本仓库中的[Python设置技巧](../../setup/01_optional-python-setup-preferences/README.md)文档，提供了更多关于设置Python环境的建议和技巧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[本书中使用的库安装文档](../../setup/02_installing-python-libraries/README.md)和[目录](../../setup/02_installing-python-libraries/)包含了检查您的环境是否设置正确的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 752\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(2, 2)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qGgnamiyLJxp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(100, 200)\n",
    "b = torch.rand(200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvGvIeVkLzXE",
    "outputId": "44d027be-0787-4348-9c06-4e559d94d0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.8 µs ± 8.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OmRtZLa9L2ZG"
   },
   "outputs": [],
   "source": [
    "a, b = a.to(\"cuda\"), b.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duLEhXDPL6k0",
    "outputId": "3486471d-fd62-446f-9855-2d01f41fd101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8 µs ± 425 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a @ b"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="appendix-A/02_setup-recommendations/README.md">
## Python 和环境设置建议

请参阅[setup](../../setup)目录中的[README.md](../../setup/README.md)文件，了解有关Python安装和设置的建议。
</file>

<file path="appendix-D/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt


#####################################
# Chapter 2
#####################################

class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################

class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################

class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
####################################


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size)
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    # plt.show()


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())
</file>

<file path="appendix-D/01_main-chapter-code/the-verdict.txt">
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. "Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of." The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's "Moon-dancers" to say, with tears in her eyes: "We shall not look upon its like again"?

Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome "obituary" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of "Gisburns" went up.

It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had "dragged him down." For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.

Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to "lift him up"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.

The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.

I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was "interesting": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the "deadening atmosphere of mediocrity" (I quote Miss Croft) was having on him.

I have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.

"Money's only excuse is to put beauty into circulation," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: "Jack is so morbidly sensitive to every form of beauty."

Poor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.

"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle," was his only protest, as he rose from the table and strolled out onto the sunlit terrace.

I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend's feet, and I wondered if a tinge of jealousy underlay the latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their "Grindles."

I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.

"Why _has_ he chucked painting?" I asked abruptly.

She raised her eyebrows with a hint of good-humoured surprise.

"Oh, he doesn't _have_ to now, you know; and I want him to enjoy himself," she said quite simply.

I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.

"Has he chucked his pictures too? I haven't seen a single one in the house."

A slight shade of constraint crossed Mrs. Gisburn's open countenance. "It's his ridiculous modesty, you know. He says they're not fit to have about; he's sent them all away except one--my portrait--and that I have to keep upstairs."

His ridiculous modesty--Jack's modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: "I must really see your portrait, you know."

She glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound's head between his knees.

"Well, come while he's not looking," she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.

In the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!

Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: "If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay."

Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's "strongest," as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown's ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted "strongly" because she was tired of being painted "sweetly"--and yet not to lose an atom of the sweetness.

"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride. "The last but one," she corrected herself--"but the other doesn't count, because he destroyed it."

"Destroyed it?" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.

As he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.

His wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.

"Mr. Rickham wanted to see it," she began, as if excusing herself. He shrugged his shoulders, still smiling.

"Oh, Rickham found me out long ago," he said lightly; then, passing his arm through mine: "Come and see the rest of the house."

He showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire's domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: "Yes, I really don't see how people manage to live without that."

Well--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: "Be dissatisfied with your leisure!" as once one had longed to say: "Be dissatisfied with your work!"

But, with the cry on my lips, my diagnosis suffered an unexpected check.

"This is my own lair," he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no "effects"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.

The fact brought home to me the absolute finality of Jack's break with his old life.

"Don't you ever dabble with paint any more?" I asked, still looking about for a trace of such activity.

"Never," he said briefly.

"Or water-colour--or etching?"

His confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.

"Never think of it, my dear fellow--any more than if I'd never touched a brush."

And his tone told me in a flash that he never thought of anything else.

I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.

"Oh, by Jove!" I said.

It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.

"By Jove--a Stroud!" I cried.

He was silent; but I felt him close behind me, breathing a little quickly.

"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?"

He answered slowly: "Mrs. Stroud gave it to me."

"Ah--I didn't know you even knew the Strouds. He was such an inflexible hermit."

"I didn't--till after. . . . She sent for me to paint him when he was dead."

"When he was dead? You?"

I must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: "Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter."

"Ah, poor Stroud--as you say. Was _that_ his history?"

"That was his history. She believed in him, gloried in him--or thought she did. But she couldn't bear not to have all the drawing-rooms with her. She couldn't bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She's just a fragment groping for other fragments. Stroud is the only whole I ever knew."

"You ever knew? But you just said--"

Gisburn had a curious smile in his eyes.

"Oh, I knew him, and he knew me--only it happened after he was dead."

I dropped my voice instinctively. "When she sent for you?"

"Yes--quite insensible to the irony. She wanted him vindicated--and by me!"

He laughed again, and threw back his head to look up at the sketch of the donkey. "There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. That's the reason why I don't dabble any more, my dear Rickham; or rather Stroud himself is the reason."

For the first time my idle curiosity about my companion turned into a serious desire to understand him better.

"I wish you'd tell me how it happened," I said.

He stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.

"I'd rather like to tell you--because I've always suspected you of loathing my work."

I made a deprecating gesture, which he negatived with a good-humoured shrug.

"Oh, I didn't care a straw when I believed in myself--and now it's an added tie between us!"

He laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. "There: make yourself comfortable--and here are the cigars you like."

He placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.

"How it happened? I can tell you in five minutes--and it didn't take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud's note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.

"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud's career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.

"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.

"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a 'subject.' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.

"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn't watching the showy bits--I couldn't distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!

"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .

"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn't been born of me--I had just adopted them. . . .

"Hang it, Rickham, with that face watching me I couldn't do another stroke. The plain truth was, I didn't know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don't you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my 'technique' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'

"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?

"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . ."

He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.

"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day."

And, in answer to a question I put half-mechanically--"Begin again?" he flashed out. "When the one thing that brings me anywhere near him is that I knew enough to leave off?"

He stood up and laid his hand on my shoulder with a laugh. "Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art."
</file>

<file path="appendix-D/README.md">
# 附录D：为训练循环增添功能

- [01_main-chapter-code](01_main-chapter-code)包含了本章的主要代码。
</file>

<file path="appendix-E/01_main-chapter-code/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="appendix-E/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-6.
# This file can be run as a standalone script.

import os
from pathlib import Path
import urllib
import zipfile

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


#####################################
# Chapter 6
#####################################


def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download and extraction.")
        return

    # Downloading the file
    with urllib.request.urlopen(url) as response:
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())

    # Unzipping the file
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extracted_path)

    # Add .tsv file extension
    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)
    print(f"File downloaded and saved as {data_file_path}")


def create_balanced_dataset(df):

    # Count the instances of "spam"
    num_spam = df[df["Label"] == "spam"].shape[0]

    # Randomly sample "ham' instances to match the number of 'spam' instances
    ham_subset = df[df["Label"] == "ham"].sample(num_spam, random_state=123)

    # Combine ham "subset" with "spam"
    balanced_df = pd.concat([ham_subset, df[df["Label"] == "spam"]])

    return balanced_df


def random_split(df, train_frac, validation_frac):
    # Shuffle the entire DataFrame
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Calculate split indices
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # Split the DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df


class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            # Truncate sequences if they are longer than max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]

        # Pad sequences to the longest sequence
        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = 0
        for encoded_text in self.encoded_texts:
            encoded_length = len(encoded_text)
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


@torch.no_grad()  # Disable gradient tracking for efficiency
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)
            logits = model(input_batch)[:, -1, :]  # Logits of last output token
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # Logits of last output token
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss


# Overall the same as `train_model_simple` in chapter 5
def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen


def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_values, label=f"Training {label}")
    ax1.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Examples seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig(f"{label}-plot.pdf")
    plt.show()
</file>

<file path="appendix-E/README.md">
# 附录E：使用LoRA进行参数高效微调

- [01_main-chapter-code](01_main-chapter-code)包含了本章的主要代码。
</file>

<file path="ch01/README.md">
# 第一章：理解大型语言模型

&nbsp;
## 章节主要代码

本章没有代码。

&nbsp;
## 额外材料

作为可选的额外材料，下面是一个视频教程，我在其中讲解了本书中涉及的 LLM 开发生命周期：

<br>
<br>

[![链接到视频](https://img.youtube.com/vi/kPGTx4wcm_w/0.jpg)](https://www.youtube.com/watch?v=kPGTx4wcm_w)
</file>

<file path="ch02/01_main-chapter-code/README.md">
# 第二章：处理文本数据

### 章节主要代码

- [ch02.ipynb](ch02.ipynb) 包含了本章节中所有的代码。

### 可选代码

- [dataloader.ipynb](dataloader.ipynb) 是一个简洁的笔记本，包含了本章节实现的主要数据加载管道。
</file>

<file path="ch02/01_main-chapter-code/the-verdict.txt">
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. "Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of." The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's "Moon-dancers" to say, with tears in her eyes: "We shall not look upon its like again"?

Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome "obituary" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of "Gisburns" went up.

It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had "dragged him down." For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.

Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to "lift him up"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.

The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.

I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was "interesting": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the "deadening atmosphere of mediocrity" (I quote Miss Croft) was having on him.

I have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.

"Money's only excuse is to put beauty into circulation," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: "Jack is so morbidly sensitive to every form of beauty."

Poor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.

"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle," was his only protest, as he rose from the table and strolled out onto the sunlit terrace.

I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend's feet, and I wondered if a tinge of jealousy underlay the latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their "Grindles."

I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.

"Why _has_ he chucked painting?" I asked abruptly.

She raised her eyebrows with a hint of good-humoured surprise.

"Oh, he doesn't _have_ to now, you know; and I want him to enjoy himself," she said quite simply.

I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.

"Has he chucked his pictures too? I haven't seen a single one in the house."

A slight shade of constraint crossed Mrs. Gisburn's open countenance. "It's his ridiculous modesty, you know. He says they're not fit to have about; he's sent them all away except one--my portrait--and that I have to keep upstairs."

His ridiculous modesty--Jack's modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: "I must really see your portrait, you know."

She glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound's head between his knees.

"Well, come while he's not looking," she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.

In the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!

Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: "If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay."

Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's "strongest," as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown's ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted "strongly" because she was tired of being painted "sweetly"--and yet not to lose an atom of the sweetness.

"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride. "The last but one," she corrected herself--"but the other doesn't count, because he destroyed it."

"Destroyed it?" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.

As he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.

His wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.

"Mr. Rickham wanted to see it," she began, as if excusing herself. He shrugged his shoulders, still smiling.

"Oh, Rickham found me out long ago," he said lightly; then, passing his arm through mine: "Come and see the rest of the house."

He showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire's domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: "Yes, I really don't see how people manage to live without that."

Well--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: "Be dissatisfied with your leisure!" as once one had longed to say: "Be dissatisfied with your work!"

But, with the cry on my lips, my diagnosis suffered an unexpected check.

"This is my own lair," he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no "effects"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.

The fact brought home to me the absolute finality of Jack's break with his old life.

"Don't you ever dabble with paint any more?" I asked, still looking about for a trace of such activity.

"Never," he said briefly.

"Or water-colour--or etching?"

His confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.

"Never think of it, my dear fellow--any more than if I'd never touched a brush."

And his tone told me in a flash that he never thought of anything else.

I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.

"Oh, by Jove!" I said.

It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.

"By Jove--a Stroud!" I cried.

He was silent; but I felt him close behind me, breathing a little quickly.

"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?"

He answered slowly: "Mrs. Stroud gave it to me."

"Ah--I didn't know you even knew the Strouds. He was such an inflexible hermit."

"I didn't--till after. . . . She sent for me to paint him when he was dead."

"When he was dead? You?"

I must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: "Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter."

"Ah, poor Stroud--as you say. Was _that_ his history?"

"That was his history. She believed in him, gloried in him--or thought she did. But she couldn't bear not to have all the drawing-rooms with her. She couldn't bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She's just a fragment groping for other fragments. Stroud is the only whole I ever knew."

"You ever knew? But you just said--"

Gisburn had a curious smile in his eyes.

"Oh, I knew him, and he knew me--only it happened after he was dead."

I dropped my voice instinctively. "When she sent for you?"

"Yes--quite insensible to the irony. She wanted him vindicated--and by me!"

He laughed again, and threw back his head to look up at the sketch of the donkey. "There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. That's the reason why I don't dabble any more, my dear Rickham; or rather Stroud himself is the reason."

For the first time my idle curiosity about my companion turned into a serious desire to understand him better.

"I wish you'd tell me how it happened," I said.

He stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.

"I'd rather like to tell you--because I've always suspected you of loathing my work."

I made a deprecating gesture, which he negatived with a good-humoured shrug.

"Oh, I didn't care a straw when I believed in myself--and now it's an added tie between us!"

He laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. "There: make yourself comfortable--and here are the cigars you like."

He placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.

"How it happened? I can tell you in five minutes--and it didn't take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud's note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.

"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud's career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.

"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.

"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a 'subject.' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.

"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn't watching the showy bits--I couldn't distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!

"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .

"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn't been born of me--I had just adopted them. . . .

"Hang it, Rickham, with that face watching me I couldn't do another stroke. The plain truth was, I didn't know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don't you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my 'technique' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'

"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?

"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . ."

He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.

"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day."

And, in answer to a question I put half-mechanically--"Begin again?" he flashed out. "When the one thing that brings me anywhere near him is that I knew enough to leave off?"

He stood up and laid his hand on my shoulder with a laugh. "Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art."
</file>

<file path="ch02/02_bonus_bytepair-encoder/bpe_openai_gpt2.py">
# Source: https://github.com/openai/gpt-2/blob/master/src/encoder.py
# License:
# Modified MIT License

# Software Copyright (c) 2019 OpenAI

# We don’t claim ownership of the content you create with GPT-2, so it is yours to do with as you please.
# We only ask that you use GPT-2 responsibly and clearly indicate your content was created using GPT-2.

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:

# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
# The above copyright notice and this permission notice need not be included
# with content created by the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE
# OR OTHER DEALINGS IN THE SOFTWARE.

import os
import json
import regex as re
import requests
from tqdm import tqdm
from functools import lru_cache


@lru_cache()
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a significant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~") + 1)) + list(range(ord("¡"), ord("¬") + 1)) + list(range(ord("®"), ord("ÿ") + 1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


def get_pairs(word):
    """
    Return set of symbol pairs in a word.
    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


class Encoder:
    def __init__(self, encoder, bpe_merges, errors='replace'):
        self.encoder = encoder
        self.decoder = {v: k for k, v in self.encoder.items()}
        self.errors = errors  # how to handle errors in decoding
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        self.cache = {}

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

    def bpe(self, token):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token)
        pairs = get_pairs(word)

        if not pairs:
            return token

        while True:
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except ValueError:
                    new_word.extend(word[i:])
                    break

                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        word = ' '.join(word)
        self.cache[token] = word
        return word

    def encode(self, text):
        bpe_tokens = []
        for token in re.findall(self.pat, text):
            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
        return bpe_tokens

    def decode(self, tokens):
        text = ''.join([self.decoder[token] for token in tokens])
        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)
        return text


def get_encoder(model_name, models_dir):
    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:
        encoder = json.load(f)
    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding="utf-8") as f:
        bpe_data = f.read()
    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]
    return Encoder(encoder=encoder, bpe_merges=bpe_merges)


def download_vocab():
    # Modified code from
    subdir = 'gpt2_model'
    if not os.path.exists(subdir):
        os.makedirs(subdir)
    subdir = subdir.replace('\\', '/')  # needed for Windows

    for filename in ['encoder.json', 'vocab.bpe']:
        r = requests.get("https://openaipublic.blob.core.windows.net/gpt-2/models/117M/" + filename, stream=True)

        with open(os.path.join(subdir, filename), 'wb') as f:
            file_size = int(r.headers["content-length"])
            chunk_size = 1000
            with tqdm(ncols=100, desc="Fetching " + filename, total=file_size, unit_scale=True) as pbar:
                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes
                for chunk in r.iter_content(chunk_size=chunk_size):
                    f.write(chunk)
                    pbar.update(chunk_size)
</file>

<file path="ch02/02_bonus_bytepair-encoder/README.md">
# 第二章：处理文本数据

- [compare-bpe-tiktoken.ipynb](compare-bpe-tiktoken.ipynb) 基准测试了各种字节对编码实现
- [bpe_openai_gpt2.py](bpe_openai_gpt2.py) 是OpenAI使用的原始字节对编码器代码
</file>

<file path="ch02/02_bonus_bytepair-encoder/requirements-extra.txt">
requests
tqdm
transformers>=4.33.2
</file>

<file path="ch02/03_bonus_embedding-vs-matmul/README.md">
# 第二章：处理文本数据

- [embeddings-and-linear-layers.ipynb](embeddings-and-linear-layers.ipynb) 包含了可选的（附加）代码，用于解释嵌入层和全连接层在应用于独热编码向量时是等效的。
</file>

<file path="ch02/04_bonus_dataloader-intuition/dataloader-intuition.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## 使用滑动窗口进行数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed23175-41be-4a7e-8c45-1f100b35a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "为了更直观地理解使用滑动窗口方法的数据加载器，我们可以考虑一个仅由数字组成的数据集：\n",
    "\n",
    "```\n",
    "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 ... 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e3f5d3c-95fe-42b2-8051-205f7803675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"number-data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for number in range(1001):\n",
    "        f.write(f\"{number} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becae19-a5a0-4236-87d5-f5eb9b6eb045",
   "metadata": {},
   "source": [
    "接下来，我们对`token_ids`进行小的修改：不再使用分词器，而是直接从文本文件中解析整数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # 修改\n",
    "        # token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = [int(i) for i in txt.strip().split()]\n",
    "\n",
    "        # 使用滑动窗口将文本切分为重叠的max_length序列\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    "让我们使用批量大小为1的dataloader来测试一个上下文大小为4的LLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"number-data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 1, 2, 3]]), tensor([[1, 2, 3, 4]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 2, 3, 4]]), tensor([[2, 3, 4, 5]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85a6c312-0144-4128-8d2c-06a4dc223ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2, 3, 4, 5]]), tensor([[3, 4, 5, 6]])]\n"
     ]
    }
   ],
   "source": [
    "third_batch = next(data_iter)\n",
    "print(third_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14b7ec67-083a-4b28-bcb9-f4c8e97e250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[996, 997, 998, 999]]), tensor([[ 997,  998,  999, 1000]])]\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    pass\n",
    "\n",
    "last_batch = batch\n",
    "print(last_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "现在，让我们来看一下批处理后的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[992, 993, 994, 995],\n",
      "        [996, 997, 998, 999]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 993,  994,  995,  996],\n",
      "        [ 997,  998,  999, 1000]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66560-25d5-4800-acc1-432735dfc7d6",
   "metadata": {},
   "source": [
    "随机打乱data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39dd4952-5333-45f0-9032-f93007d742b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[880, 881, 882, 883],\n",
      "        [112, 113, 114, 115]])\n",
      "\n",
      "Targets:\n",
      " tensor([[881, 882, 883, 884],\n",
      "        [113, 114, 115, 116]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=True)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/04_bonus_dataloader-intuition/README.md">
# 第二章：处理文本数据

- [dataloader-intuition.ipynb](dataloader-intuition.ipynb) 包含了可选的（附加）代码，用简单的数字而非文本来更直观地解释数据加载器。
</file>

<file path="ch02/README.md">
# 第二章：处理文本数据

&nbsp;
## 章节主要代码

- [01_main-chapter-code](01_main-chapter-code) 包含了章节的主要代码和习题解答。

&nbsp;
## 额外材料

- [02_bonus_bytepair-encoder](02_bonus_bytepair-encoder) 包含了可选的代码，用于基准测试不同的字节对编码实现。

- [03_bonus_embedding-vs-matmul](03_bonus_embedding-vs-matmul) 包含了可选（额外）代码，用于解释嵌入层和应用于独热编码向量的全连接层是等效的。

- [04_bonus_dataloader-intuition](04_bonus_dataloader-intuition) 包含了可选（额外）代码，用更直观的简单数字而非文本来解释数据加载器。
</file>

<file path="ch03/01_main-chapter-code/README.md">
# 第三章：编码注意力机制

### 章节主要代码

- [ch03.ipynb](ch03.ipynb) 包含了章节中出现的所有代码。

### 可选代码

- [multihead-attention.ipynb](multihead-attention.ipynb) 是一个简洁的笔记本，包含了本章节实现的主要多头注意力数据加载管道。
</file>

<file path="ch03/01_main-chapter-code/small-text-sample.txt">
Once upon a time in a quiet village nestled among rolling hills and whispering forests, there lived a young girl named Elara. Elara was known for her boundless curiosity and her love for the stars. Every night, she would climb to the highest hill near her home to gaze at the glittering sky, dreaming of distant worlds and galaxies.

In the heart of the village, there was an ancient library, tended by an old, wise librarian named Mr. Bramwell. This library was a treasure trove of books on every subject, but most importantly, it housed a collection of old star maps and celestial guides. Elara, fascinated by these books, spent countless hours with Mr. Bramwell, learning about constellations, planets, and the mysteries of the universe.

One evening, while studying an old star map, Elara noticed a small, uncharted star that twinkled differently. She shared this discovery with Mr. Bramwell, who was equally intrigued. They decided to observe this star every night, noting its unique patterns and movements. This small, mysterious star, which they named "Elara's Star," became the center of their nightly adventures.

As days turned into weeks, the villagers began to take notice of Elara's star. The uncharted star brought the community together, with people of all ages joining Elara and Mr. Bramwell on the hill each night to gaze at the sky. The nightly gatherings turned into a festival of stars, where stories were shared, friendships were formed, and the mysteries of the cosmos were contemplated.

The story of Elara and her star spread far and wide, attracting astronomers and dreamers from distant lands. The once quiet village became a beacon of wonder, a place where the sky seemed a little closer and the stars a bit friendlier. Elara's curiosity had not only unveiled a hidden star but had also brought her community together, reminding everyone that sometimes, the most extraordinary discoveries are waiting just above us, in the starlit sky.
</file>

<file path="ch03/02_bonus_efficient-multihead-attention/README.md">
# 更高效的多头注意力实现

- [mha-implementations.ipynb](mha-implementations.ipynb) 包含并比较了不同的多头注意力实现方式

### 总结

下图总结了性能基准测试（越低越好）。

&nbsp;
#### 仅前向传播

<a href="mha-implementations.ipynb"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/mha-benchmark/1_forward-only.webp?1" width="500px"></a>

&nbsp;
#### 前向和反向传播

<a href="mha-implementations.ipynb"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/mha-benchmark/2_forward-and-backward.webp?1" width="500px"></a>

&nbsp;
#### 编译后的前向和反向传播

<a href="mha-implementations.ipynb"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/mha-benchmark/3_forward-and-backward-compiled.webp?1" width="500px"></a>
</file>

<file path="ch03/03_understanding-buffers/README.md">
# 理解PyTorch缓冲区

- [understanding-buffers.ipynb](understanding-buffers.ipynb) 解释了PyTorch缓冲区的概念，缓冲区用于实现第三章中的因果注意力机制。

<br>
下面是我录制的一个实操视频教程，用于解释代码：

<br>
<br>

[![视频链接](https://img.youtube.com/vi/PetlIokI9Ao/0.jpg)](https://www.youtube.com/watch?v=PetlIokI9Ao)
</file>

<file path="ch03/README.md">
# 第三章：编码注意力机制

&nbsp;
## 章节主要代码

- [01_main-chapter-code](01_main-chapter-code) 包含了章节的主要代码。

&nbsp;
## 额外材料

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) 实现并比较了多头注意力的不同实现变体。

- [03_understanding-buffers](03_understanding-buffers) 解释了 PyTorch 缓冲区的概念，这些缓冲区用于实现第三章中的因果注意力机制。
</file>

<file path="ch04/01_main-chapter-code/ch04.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f4321d-d32a-4a90-bfc7-e923f316b2f8",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# 第四章: 从零开始构建 GPT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.0\n",
      "torch version: 2.4.0\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "#加载并确认版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- 在这一章我们要用类GPT LLM架构\n",
    "- 下一章就是训练LLM了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"../image/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 4.1 LLM架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- 第 1 章讨论了 GPT 和 Llama 等模型，这些模型基于原始 Transformer 架构的解码器部分，按顺序生成单词。\n",
    "- 因此，这些大语言模型（LLM）通常被称为“类解码器”的 LLM。\n",
    "- 与传统的深度学习模型相比，LLM 的规模更大，这主要是由于其参数数量庞大，而非代码量的增加。\n",
    "- 我们将看到，在 LLM 的架构中，许多元素是重复的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"../image/02.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- 在前几章中，为了便于说明，我们使用了较小的嵌入维度对标记输入和输出进行处理，以确保内容能够显示在单页内。\n",
    "- 本章将讨论与小型 GPT-2 模型类似的嵌入和模型规模。\n",
    "- 我们将具体实现最小的 GPT-2 模型架构（1.24 亿参数）。该架构来源于 Radford 等人的报告 [《Language Models are Unsupervised Multitask Learners》](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)（需要注意的是，初始报告中参数数量被列为 1.17 亿，但这一错误在模型权重库中已被更正）。\n",
    "- 第 6 章将展示如何将预训练权重加载到我们的实现中，这些权重可兼容 3.45 亿、7.62 亿和 15.42 亿参数规模的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- 123million参数的GPT-2配置如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "#初始化定义需要的各种超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 我们使用简短的变量名，以避免代行过长。\n",
    "- `\"vocab_size\"` 表示词汇表大小，共 50,257 个单词，由第 2 章介绍的 BPE 分词器支持。\n",
    "- `\"context_length\"` 表示模型的最大输入标记数，依赖于第 2 章的位置信息嵌入。\n",
    "- `\"emb_dim\"` 是标记输入的嵌入维度，将每个标记转换为 768 维向量。\n",
    "- `\"n_heads\"` 是多头注意力机制中的注意力头数量，详见第 3 章。\n",
    "- `\"n_layers\"` 是模型中 Transformer 块的数量，后续章节会详细实现。\n",
    "- `\"drop_rate\"` 是 dropout 机制的强度，设置为 0.1，表示训练时丢弃 10% 的隐藏单元以防止过拟合（第 3 章讨论）。\n",
    "- `\"qkv_bias\"` 决定多头注意力机制中的 `Linear` 层是否包含偏置向量。现代 LLM 通常禁用此选项，但在第 5 章加载 OpenAI 的 GPT-2 预训练权重时，会重新启用以保持兼容性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"../image/03.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # 词嵌入层，将输入索引转换为词向量，词表大小由字典大小和特征维度决定。\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # 位置信息嵌入层，基于文本长度和特征维度生成位置信息。\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        # Dropout 层，用于随机丢弃一部分嵌入信息以减少过拟合。\n",
    "\n",
    "        # 使用多个 Transformer 块（占位符）\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        # Transformer 模块的堆叠，模型核心部分。\n",
    "\n",
    "        # 使用归一化层（占位符）\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        # 最终归一化层，用于调整特征分布。\n",
    "\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        # 输出层，将特征映射到词表分布，最终预测输出单词。\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        # 获取批次大小和序列长度。\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        # 根据输入索引生成词嵌入。\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        # 生成对应的位置信息嵌入。\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        # 将词嵌入和位置信息嵌入相加。\n",
    "        x = self.drop_emb(x)\n",
    "        # 应用 Dropout 随机丢弃部分信息。\n",
    "        x = self.trf_blocks(x)\n",
    "        # 通过多个 Transformer 块处理特征。\n",
    "        x = self.final_norm(x)\n",
    "        # 应用最终的归一化层。\n",
    "        logits = self.out_head(x)\n",
    "        # 将隐藏状态映射到词表分布，生成预测结果。\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    # Transformer 块的占位类。\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 占位，实际模型应实现注意力机制和前馈网络。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 此块不执行任何操作，仅返回输入。\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    # 归一化层的占位类。\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # 参数用于模拟 LayerNorm 的接口。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 此层不执行任何操作，仅返回输入。\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e8ab-20ca-4100-b9b9-50d9bdee33be",
   "metadata": {},
   "source": [
    "<img src=\"../image/04.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794b6b6c-d36f-411e-a7db-8ac566a87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "#召唤gpt大神\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "#编码输入文本\n",
    "batch = torch.stack(batch, dim=0)\n",
    "#按照横向来叠加两个向量\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fad0fe-895d-4493-9e48-962e2d46c66f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**\n",
    "\n",
    "- 系统为Windows或者Linux, 运行结果如下所示:\n",
    "    \n",
    "```\n",
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
    "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
    "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
    "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
    "\n",
    "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
    "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
    "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
    "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)\n",
    "```\n",
    "\n",
    "- Since these are just random numbers, this is not a reason for concern, and you can proceed with the remainder of the chapter without issues\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 4.2 归一化操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- **层归一化（LayerNorm）**，也称为层标准化（[Ba et al. 2016](https://arxiv.org/abs/1607.06450)），将神经网络层的激活值中心化为均值为 0，并将其方差归一化为 1。\n",
    "- 这种方法能够稳定训练过程，并加速权重的高效收敛。\n",
    "- 在 Transformer 块中，层归一化会在多头注意力模块的前后应用（我们将在后续实现），并在最终输出层之前再次应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"../image/05.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1b463-dc3f-44ac-9cdb-9d5b6f64eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "#一个按照顺序执行的神经网络\n",
    "#具体: 全链接层跟,激活函数\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- 计算上述信息的均值与方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- 归一化会单独对两个输入（行）进行处理；\n",
    "- 设置 `dim=-1` 的意思是让计算沿着最后一个维度进行（在这里是特征维度），而不是按行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "<img src=\"../image/06.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "source": [
    "- 通过减去均值并除以方差的平方根（即标准差），可以让输入在列（特征）维度上的均值变为 0，方差变为 1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "#执行归一化操作\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b90c-7156-4979-9a79-ce1fb92969c1",
   "metadata": {},
   "source": [
    "- 每个输入的均值都会被调整为 0，方差被归一化为 1。\n",
    "- 为了结果容易阅读，我们可以禁用 PyTorch 的科学计数法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e06c34b-c68a-4b36-afbe-b30eda4eca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "- 上面我们对每个输入的特征进行了归一化。\n",
    "- 现在，基于相同的思路，我们可以实现一个 `LayerNorm` 类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3333a305-aa3d-460a-bcce-b80662d464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    #layer归一化的函数,可以避免信息泄露也可以稳定\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 #避免0的产生导致崩溃\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) #动态的缩放参数\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) #动态的偏移参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)#算平均值\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)#算方差\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)#归一化\n",
    "        return self.scale * norm_x + self.shift #通过Ω和  œ 调整归一化后的值范围和位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3908-7544-4808-b8cb-5d0a55bcca72",
   "metadata": {},
   "source": [
    "## 缩放与平移\n",
    "\n",
    "- 除了通过减去均值并除以方差来执行归一化操作外，我们还引入了两个可训练参数：`scale`（缩放参数）和 `shift`（平移参数）。\n",
    "- 初始时，`scale` 值为 1，`shift` 值为 0，不会对结果产生影响；但在训练过程中，LLM 会自动调整这两个参数，以提升模型在任务中的表现。\n",
    "- 这种设计使模型能够学习到最适合其数据的缩放和平移方式。\n",
    "- 此外，在计算方差的平方根时，我们会添加一个较小的值（`eps`），以避免方差为 0 时出现除以 0 的错误。\n",
    "\n",
    "## 偏差方差\n",
    "\n",
    "- 在方差计算中，设置 `unbiased=False` 使用公式 $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$，其中 $n$ 是样本大小（即特征或列的数量）。该公式未使用贝塞尔校正（分母为 `n` 而非 `n-1`），因此提供的是方差的偏差估计。\n",
    "- 对于嵌入维度 $n$ 较大的 LLM 来说，使用 `n` 和 `n-1` 的差异可以忽略不计。\n",
    "- 然而，由于 GPT-2 在归一化层的训练中使用了偏差方差，为了与预训练权重兼容，我们也采用了这种设置。\n",
    "\n",
    "## 实践 LayerNorm\n",
    "\n",
    "- 现在让我们通过实际代码尝试 `LayerNorm` 的应用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b1000a-e613-4b43-bd90-e54deed8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)#归一化一个五维度\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c12de2-1cab-46e0-a099-e2e470353bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"../image/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11190e7d-8c29-4115-824a-e03702f9dd54",
   "metadata": {},
   "source": [
    "## 4.3 GELU作为激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585dfb-f21e-40e5-973f-2f63ad5cb169",
   "metadata": {},
   "source": [
    "- 在本节中，我们将实现一个小型神经网络子模块，该模块是 LLM 中 Transformer 块的核心组成部分。\n",
    "- 首先，我们从激活函数开始。\n",
    "- 在深度学习中，ReLU（线性整流单元）激活函数因其简单性和在各种神经网络架构中的高效性而被广泛使用。\n",
    "- 在 LLM 中，除了传统的 ReLU，还使用了其他类型的激活函数。其中两个典型的例子是 GELU（高斯误差线性单元）和 SwiGLU（Swish 门控线性单元）。\n",
    "- GELU 和 SwiGLU 是更复杂的平滑激活函数，分别结合了高斯函数和 sigmoid 门控线性单元，提供了比 ReLU 这种简单分段线性函数更好的性能，尤其适用于深度学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482ce7-e493-4bfc-a820-3ea99f564ebc",
   "metadata": {},
   "source": [
    "- **GELU**（[Hendrycks 和 Gimpel, 2016](https://arxiv.org/abs/1606.08415)）可以通过多种方式实现；其精确定义为 $\\text{GELU}(x) = x \\cdot \\Phi(x)$，其中 $\\Phi(x)$ 是标准高斯分布的累积分布函数。\n",
    "- 在实际应用中，通常会使用一种计算成本更低的近似形式：  \n",
    "  $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)$  \n",
    "  （原始 GPT-2 模型也是使用该近似公式进行训练的）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84694b7-95f3-4323-b6d6-0a73df278e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            #这一步把它变得平滑了很多\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5487d2-2576-4118-80a7-56c4caac2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()#先把函数给个小名\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100) #初定义一个张量\n",
    "y_gelu, y_relu = gelu(x), relu(x) #两种激活函数\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#一个经典的作图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- 正如我们所见，ReLU 是一种分段线性函数：对于正值直接输出输入值；对于负值则输出零。\n",
    "- GELU 是一种平滑的非线性函数，它近似 ReLU，但在负值时具有非零梯度（除了大约在 -0.75 处）。\n",
    "\n",
    "- 接下来，我们将实现一个小型神经网络模块 `FeedForward`，该模块将用于 LLM 的 Transformer 块中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    #运行一次就线性两次激活一次\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"../image/09.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928e7f7c-d0b1-499f-8d07-4cadb428a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"../image/10.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da2a50-04f4-4388-af23-ad32e405a972",
   "metadata": {},
   "source": [
    "<img src=\"../image/11.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4.4 类似于ResNet的shortcut传递"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae416c-821e-4bfa-a741-8af4ba5db00e",
   "metadata": {},
   "source": [
    "- 接下来，我们来讨论**快捷连接**（shortcut connections）的概念，也称为**跳跃连接**（skip connections）或**残差连接**（residual connections）。\n",
    "- 残差连接最初是在深度网络中提出的，主要应用于计算机视觉中的残差网络（ResNet），以缓解梯度消失问题。\n",
    "- 残差连接通过为梯度提供一条更短的替代路径，使其能够更顺畅地通过网络流动。\n",
    "- 具体实现是将某一层的输出与后续某一层的输出相加，通常会跳过中间的一层或多层。\n",
    "- 以下是一个小型网络的示例来说明这一概念：\n",
    "\n",
    "![残差连接示例](../image/12.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- 代码形式长成这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        # 定义多层网络，包含 5 层线性层和激活函数 GELU\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "        # 定义一个五层的神经网络块，其中每层包含一个线性变换和一个激活函数 GELU，\n",
    "        # 类似 ResNet 的结构，支持添加残差连接。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 遍历每一层\n",
    "        for layer in self.layers:\n",
    "            # 当前层的输出\n",
    "            layer_output = layer(x)\n",
    "            # 检查是否可以应用残差连接\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output  # 如果输入和输出维度匹配，添加残差连接\n",
    "            else:\n",
    "                x = layer_output  # 否则直接输出当前层结果\n",
    "        return x  # 返回最终结果\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])  # 定义目标值\n",
    "    # 计算损失，使用均方误差损失函数\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # 反向传播，计算梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 打印每层权重的梯度均值\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- 在没有残差链接的时候,梯度是这样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "#一次一次输出梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- **有** 残差的链接,梯度是这样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)\n",
    "#引入了残差链接,发现梯度消失的缺点明显改善了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff783a-46f0-49c5-a7a9-26a525764b6e",
   "metadata": {},
   "source": [
    "- 正如我们从上述输出中看到的，残差连接有效地防止了梯度在早期层（靠近 `layer.0`）中消失。\n",
    "- 接下来，在实现 Transformer 块时，我们将使用这一残差连接的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae578ca-e564-42cf-8635-a2267047cdff",
   "metadata": {},
   "source": [
    "## 4.5 在 Transformer 块中连接attention层与线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daac6f-6545-4258-8f2d-f45a7394f429",
   "metadata": {},
   "source": [
    "- 在本节中，我们将把前面介绍的概念整合成一个所谓的 Transformer 块。\n",
    "- Transformer 块将上一章中的因果多头注意力模块与线性层以及我们之前实现的前馈神经网络相结合。\n",
    "- 此外，Transformer 块还包含 Dropout 和残差连接的机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],               # 输入特征维度\n",
    "            d_out=cfg[\"emb_dim\"],              # 输出特征维度\n",
    "            context_length=cfg[\"context_length\"],  # 上下文长度\n",
    "            num_heads=cfg[\"n_heads\"],          # 注意力头的数量\n",
    "            dropout=cfg[\"drop_rate\"],          # Dropout 比例\n",
    "            qkv_bias=cfg[\"qkv_bias\"]           # 查询、键和值的偏置\n",
    "        )  # 多头注意力模块，结合各种参数\n",
    "        self.ff = FeedForward(cfg)  # 前馈神经网络模块\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])  # 第一归一化层\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])  # 第二归一化层\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])  # 残差连接的 Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对注意力模块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)  # 应用第一归一化层\n",
    "        x = self.att(x)  # 通过多头注意力模块，形状为 [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)  # 应用 Dropout\n",
    "        x = x + shortcut  # 将原始输入加回，实现残差连接\n",
    "\n",
    "        # 对前馈网络模块的残差连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)  # 应用第二归一化层\n",
    "        x = self.ff(x)  # 通过前馈神经网络模块\n",
    "        x = self.drop_shortcut(x)  # 应用 Dropout\n",
    "        x = x + shortcut  # 将原始输入加回，实现残差连接\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"../image/13.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d375-87bd-4153-9040-63a1e6a2b7cb",
   "metadata": {},
   "source": [
    "- 假设我们有 2 个输入样本，每个样本包含 6 个标记，每个标记是一个 768 维的嵌入向量；然后，这个 Transformer 块会先应用自注意力机制，再通过线性层处理，生成一个相同尺寸的输出。\n",
    "- 你可以将输出视为我们在上一章讨论的上下文向量的增强版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "#经典的一系列操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4ee4-cf23-4583-b1fd-317abb4fcd13",
   "metadata": {},
   "source": [
    "<img src=\"../image/14.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 4.6 编码GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
   "metadata": {},
   "source": [
    "- 终于要结束了：现在让我们将 Transformer 块插入到本章开头编写的架构中，这样就可以得到一个可用的 GPT 架构。\n",
    "- 请注意，Transformer 块会重复多次；对于最小的 124M GPT-2 模型来说，我们会重复 12 次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"../image/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- 对应的代码实现，其中 `cfg[\"n_layers\"] = 12`："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4e972-a13b-4733-9cf4-f54c882870cf",
   "metadata": {},
   "source": [
    "- (译者)一点语法小知识\n",
    "在 Python 中，* 是 解包操作符，用于将一个可迭代对象（如列表、元组）中的元素逐一解包成单独的参数。\n",
    "在 PyTorch 中，nn.Sequential 接受一组模块作为输入，而不是一个列表或其他容器。\n",
    "这里使用 * 将生成的 TransformerBlock 列表解包为独立的参数传递给 nn.Sequential。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):#召唤GPT!\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        #新建字典、位置信息、还有dropout的比率设置\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        #解包操作\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        TransformerBlock(cfg),\n",
    "        TransformerBlock(cfg),\n",
    "        TransformerBlock(cfg)\n",
    "                )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        #归一化\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        #输出头保证维度\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
   "metadata": {},
   "source": [
    "- 用了124M模型的原始参数,我们接下啦要初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "#经典操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616e7a-568b-4921-af29-bd3f4683cd2e",
   "metadata": {},
   "source": [
    "- 我们将在下一章训练这个模型。\n",
    "- 但关于其规模需要补充一点：我们之前提到它是一个拥有 1.24 亿参数的模型；我们可以通过以下方式再次确认这个数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "#模型的总参数数量\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660",
   "metadata": {},
   "source": [
    "- 正如我们上面看到的，这个模型实际上有 1.63 亿参数，而不是 1.24 亿；这是为什么呢？\n",
    "- 在原始 GPT-2 论文中，研究人员采用了**权重共享**（weight tying）技术，即将标记嵌入层（`tok_emb`）作为输出层复用，具体表现为设置 `self.out_head.weight = self.tok_emb.weight`。\n",
    "- 标记嵌入层将 50,257 维的独热编码输入标记映射到 768 维的嵌入表示。\n",
    "- 输出层则将 768 维的嵌入表示映射回 50,257 维的表示，从而可以将其还原为单词（关于这一点，我们将在下一节详细讨论）。\n",
    "- 因此，嵌入层和输出层的权重参数数量相同，从它们权重矩阵的形状可以看出这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "#输出格式让我们更好地理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- 在原始 GPT-2 论文中，研究人员将标记嵌入矩阵复用为输出矩阵。\n",
    "- 相应地，如果我们减去输出层的参数数量，就会得到一个拥有 1.24 亿参数的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95a22e02-50d3-48b3-a4e0-d9863343c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "#Parameter- sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b03f80-b94c-46e7-9d42-d0df399ff3db",
   "metadata": {},
   "source": [
    "- 在实际应用中，因为不使用权重共享更容易训练模型，所以这里没有进行权重共享。\n",
    "- 不过，在第 5 章加载预训练权重时，我们将重新讨论并应用权重共享的概念。\n",
    "- 最后，我们可以通过以下方式计算模型的内存需求，这可能是一个有用的参考点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n",
    "#计算总的容量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- **练习**：你可以尝试以下其他配置，这些配置参考自 [GPT-2 论文](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C)。\n",
    "\n",
    "    - **GPT2-small**（我们已经实现的 124M 配置）：\n",
    "        - `\"emb_dim\" = 768`\n",
    "        - `\"n_layers\" = 12`\n",
    "        - `\"n_heads\" = 12`\n",
    "\n",
    "    - **GPT2-medium**：\n",
    "        - `\"emb_dim\" = 1024`\n",
    "        - `\"n_layers\" = 24`\n",
    "        - `\"n_heads\" = 16`\n",
    "\n",
    "    - **GPT2-large**：\n",
    "        - `\"emb_dim\" = 1280`\n",
    "        - `\"n_layers\" = 36`\n",
    "        - `\"n_heads\" = 20`\n",
    "\n",
    "    - **GPT2-XL**：\n",
    "        - `\"emb_dim\" = 1600`\n",
    "        - `\"n_layers\" = 48`\n",
    "        - `\"n_heads\" = 25`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 4.7 文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- GPT架构的LLM一次只能生成一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"../image/16.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
   "metadata": {},
   "source": [
    "- 以下的 `generate_text_simple` 函数实现了贪心解码（greedy decoding），这是一种简单且快速的文本生成方法。\n",
    "- 在贪心解码中，模型在每一步选择具有最高概率的词（或标记）作为下一个输出（由于最高的 logit 值对应最高的概率，实际上我们不需要显式地计算 softmax 函数）。\n",
    "- 在下一章，我们将实现一个更为复杂的 `generate_text` 函数。\n",
    "- 下图展示了给定输入上下文时，GPT 模型是如何生成下一个词标记的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"../image/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # 预测单词的模块\n",
    "    # idx 是当前上下文中的（batch, n_tokens）索引数组\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 每次生成一个单词后，重新将其加入序列中\n",
    "        # 如果当前上下文长度超过模型支持的最大上下文长度，则截取\n",
    "        # 例如，如果LLM只支持5个token，而上下文长度为10\n",
    "        # 那么只使用最后5个token作为上下文\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # 如果idx的长度超过模型支持的上下文长度size，只保留最后size个token\n",
    "        # 避免溢出\n",
    "        # 获取预测结果\n",
    "        with torch.no_grad():  # 在推理阶段，不需要计算梯度，因为没有反向传播\n",
    "            # 这样可以减少存储开销\n",
    "            logits = model(idx_cond)\n",
    "            # 模型输出结果\n",
    "        # 只关注最后一个时间步的输出\n",
    "        # (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        # 关注最后一个时间步\n",
    "        # 使用softmax函数计算概率\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        # 归一化\n",
    "        # 获取具有最高概率值的词汇索引\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "        # 获取概率最高的词汇索引\n",
    "        # 将采样的索引添加到序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- 上面的 `generate_text_simple` 实现了一个迭代过程，其中它一次生成一个token。\n",
    "\n",
    "<img src=\"../image/18.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- 举个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "#模拟\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "#进行语义理解\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "#最终输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "#在检验的时候不需要正则化了\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    #左边的参数名字,右边是函数传入的实际模型\n",
    "    idx=encoded_tensor, #上下文的索引\n",
    "    max_new_tokens=6, #最多运行六次,然后取结果概率最高的\n",
    "    #初始文本➕6\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "#输出长度还有每个单词的id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- 去除批次维度并将其转换回文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894003-51f6-4ccc-996f-3b9c7d5a1d70",
   "metadata": {},
   "source": [
    "- 请注意，目前模型尚未经过训练，因此上述输出文本是随机的。\n",
    "- 我们将在下一章中训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35278b6-9e5c-480f-83e5-011a1173648f",
   "metadata": {},
   "source": [
    "## 总结与收获\n",
    "\n",
    "- 请查看 [./gpt.py](./gpt.py) 脚本，这是一个独立的脚本，包含了我们在此 Jupyter Notebook 中实现的 GPT 模型。\n",
    "- 练习题的解答可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch04/01_main-chapter-code/gpt.py">
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


def main():
    GPT_CONFIG_124M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "emb_dim": 768,          # Embedding dimension
        "n_heads": 12,           # Number of attention heads
        "n_layers": 12,          # Number of layers
        "drop_rate": 0.1,        # Dropout rate
        "qkv_bias": False        # Query-Key-Value bias
    }

    torch.manual_seed(123)
    model = GPTModel(GPT_CONFIG_124M)
    model.eval()  # disable dropout

    start_context = "Hello, I am"

    tokenizer = tiktoken.get_encoding("gpt2")
    encoded = tokenizer.encode(start_context)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)

    print(f"\n{50*'='}\n{22*' '}IN\n{50*'='}")
    print("\nInput text:", start_context)
    print("Encoded input text:", encoded)
    print("encoded_tensor.shape:", encoded_tensor.shape)

    out = generate_text_simple(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=10,
        context_size=GPT_CONFIG_124M["context_length"]
    )
    decoded_text = tokenizer.decode(out.squeeze(0).tolist())

    print(f"\n\n{50*'='}\n{22*' '}OUT\n{50*'='}")
    print("\nOutput:", out)
    print("Output length:", len(out[0]))
    print("Output text:", decoded_text)


if __name__ == "__main__":
    main()
</file>

<file path="ch04/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec
</file>

<file path="ch04/01_main-chapter-code/README.md">
# 第四章：从零实现 GPT 模型以生成文本

### 章节主要代码

- [ch04.ipynb](ch04.ipynb) 包含了章节中出现的所有代码。
- [previous_chapters.py](previous_chapters.py) 是一个 Python 模块，包含了上一章的 `MultiHeadAttention` 模块，我们在 [ch04.ipynb](ch04.ipynb) 中导入该模块以创建 GPT 模型。

### 可选代码

- [gpt.py](gpt.py) 是一个独立的 Python 脚本文件，包含了我们迄今为止实现的代码，包括本章编写的 GPT 模型。
</file>

<file path="ch04/01_main-chapter-code/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)

from gpt import main

expected = """
==================================================
                      IN
==================================================

Input text: Hello, I am
Encoded input text: [15496, 11, 314, 716]
encoded_tensor.shape: torch.Size([1, 4])


==================================================
                      OUT
==================================================

Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,
         49706, 43231, 47062, 34657]])
Output length: 14
Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous
"""


def test_main(capsys):
    main()
    captured = capsys.readouterr()

    # Normalize line endings and strip trailing whitespace from each line
    normalized_expected = '\n'.join(line.rstrip() for line in expected.splitlines())
    normalized_output = '\n'.join(line.rstrip() for line in captured.out.splitlines())

    # Compare normalized strings
    assert normalized_output == normalized_expected
</file>

<file path="ch04/02_performance-analysis/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.
import torch
import torch.nn as nn


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx
</file>

<file path="ch04/02_performance-analysis/README.md">
# Chapter 4: Implementing a GPT Model from Scratch To Generate Text

- [flops-analysis.ipynb](flops-analysis.ipynb) analyses the floating point operations per second (FLOPS) of the GPT model(s) implemented in the main chapter. 
- [previous_chapters.py](previous_chapters.py) is a Python module containing the `GPTModel` code we implemented in chapter 4 and other code implemented in previous chapters, which we import in the analysis notebook.
- `requirements-extra.txt` includes additional Python libraries that need to be installed (via `pip install -r requirements-extra.txt`.
</file>

<file path="ch04/02_performance-analysis/requirements-extra.txt">
thop
</file>

<file path="ch04/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# 第四章 课后练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2fac7a-fdcd-437c-b1c4-0b35a31cd489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import torch\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# 练习 4.1：前馈模块与注意力模块中的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2751b0e5-ffd3-4be2-8db3-e20dd4d61d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from gpt import TransformerBlock\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcaffd1-0cf6-4f8f-bd53-ab88a37f443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in feed forward module: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in block.ff.parameters())\n",
    "print(f\"Total number of parameters in feed forward module: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dd06c1-ab6c-4df7-ba73-f9cd54b31138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in attention module: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Total number of parameters in attention module: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15463dec-520a-47b4-b3ad-e180394fd076",
   "metadata": {},
   "source": [
    "- 上述结果是针对单个transformer块的。\n",
    "- 可选地，将结果乘以 12，以计算 124M GPT 模型中所有transformer的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e9251-e0a9-4972-8df6-f280f35939f9",
   "metadata": {},
   "source": [
    "**额外内容：数据分析**\n",
    "\n",
    "- 对于那些有兴趣了解这些参数数量如何从数据上计算的人，下面是详细的分析（假设 `emb_dim=768`）：\n",
    "\n",
    "前馈模块：\n",
    "\n",
    "- 第一个 `Linear` 层：768 输入 × 4×768 输出 + 4×768 偏置单元 = 2,362,368\n",
    "- 第二个 `Linear` 层：4×768 输入 × 768 输出 + 768 偏置单元 = 2,360,064\n",
    "- 总计：第一个 `Linear` 层 + 第二个 `Linear` 层 = 2,362,368 + 2,360,064 = 4,722,432\n",
    "\n",
    "注意力模块：\n",
    "\n",
    "- `W_query`：768 输入 × 768 输出 = 589,824 \n",
    "- `W_key`：768 输入 × 768 输出 = 589,824\n",
    "- `W_value`：768 输入 × 768 输出 = 589,824 \n",
    "- `out_proj`：768 输入 × 768 输出 + 768 偏置单元 = 590,592\n",
    "- 总计：`W_query` + `W_key` + `W_value` + `out_proj` = 3×589,824 + 590,592 = 2,360,064"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7c7f-0fa1-4d30-ab44-e499edd55b6d",
   "metadata": {},
   "source": [
    "# 练习4.2 初始化GPT模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b2e05-3ec8-47fc-afd9-83bf03d4aad8",
   "metadata": {},
   "source": [
    "- **GPT2-small**（已实现的 124M 配置）：\n",
    "    - \"emb_dim\" = 768\n",
    "    - \"n_layers\" = 12\n",
    "    - \"n_heads\" = 12\n",
    "\n",
    "- **GPT2-medium**：\n",
    "    - \"emb_dim\" = 1024\n",
    "    - \"n_layers\" = 24\n",
    "    - \"n_heads\" = 16\n",
    "\n",
    "- **GPT2-large**：\n",
    "    - \"emb_dim\" = 1280\n",
    "    - \"n_layers\" = 36\n",
    "    - \"n_heads\" = 20\n",
    "\n",
    "- **GPT2-XL**：\n",
    "    - \"emb_dim\" = 1600\n",
    "    - \"n_layers\" = 48\n",
    "    - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90185dea-81ca-4cdc-aef7-4aaf95cba946",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "\n",
    "def get_config(base_config, model_name=\"gpt2-small\"):\n",
    "    GPT_CONFIG = base_config.copy()\n",
    "\n",
    "    if model_name == \"gpt2-small\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 768\n",
    "        GPT_CONFIG[\"n_layers\"] = 12\n",
    "        GPT_CONFIG[\"n_heads\"] = 12\n",
    "\n",
    "    elif model_name == \"gpt2-medium\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1024\n",
    "        GPT_CONFIG[\"n_layers\"] = 24\n",
    "        GPT_CONFIG[\"n_heads\"] = 16\n",
    "\n",
    "    elif model_name == \"gpt2-large\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1280\n",
    "        GPT_CONFIG[\"n_layers\"] = 36\n",
    "        GPT_CONFIG[\"n_heads\"] = 20\n",
    "\n",
    "    elif model_name == \"gpt2-xl\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1600\n",
    "        GPT_CONFIG[\"n_layers\"] = 48\n",
    "        GPT_CONFIG[\"n_heads\"] = 25\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect model name {model_name}\")\n",
    "\n",
    "    return GPT_CONFIG\n",
    "\n",
    "\n",
    "def calculate_size(model): # based on chapter code\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "    \n",
    "    # Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "    total_size_bytes = total_params * 4\n",
    "    \n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2587e011-78a4-479c-a8fd-961cc40a5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gpt2-small:\n",
      "Total number of parameters: 163,009,536\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n",
      "\n",
      "\n",
      "gpt2-medium:\n",
      "Total number of parameters: 406,212,608\n",
      "Number of trainable parameters considering weight tying: 354,749,440\n",
      "Total size of the model: 1549.58 MB\n",
      "\n",
      "\n",
      "gpt2-large:\n",
      "Total number of parameters: 838,220,800\n",
      "Number of trainable parameters considering weight tying: 773,891,840\n",
      "Total size of the model: 3197.56 MB\n",
      "\n",
      "\n",
      "gpt2-xl:\n",
      "Total number of parameters: 1,637,792,000\n",
      "Number of trainable parameters considering weight tying: 1,557,380,800\n",
      "Total size of the model: 6247.68 MB\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPTModel\n",
    "\n",
    "\n",
    "for model_abbrev in (\"small\", \"medium\", \"large\", \"xl\"):\n",
    "    model_name = f\"gpt2-{model_abbrev}\"\n",
    "    CONFIG = get_config(GPT_CONFIG_124M, model_name=model_name)\n",
    "    model = GPTModel(CONFIG)\n",
    "    print(f\"\\n\\n{model_name}:\")\n",
    "    calculate_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2306e-5dc8-498e-92ee-70ae7ec37ac1",
   "metadata": {},
   "source": [
    "# 练习4.3 Droppout模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fee2cf5-61c3-4167-81b5-44ea155bbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate_emb\": 0.1,        # NEW: dropout for embedding layers\n",
    "    \"drop_rate_attn\": 0.1,       # NEW: dropout for multi-head attention  \n",
    "    \"drop_rate_shortcut\": 0.1,   # NEW: dropout for shortcut connections  \n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa1b0c1-d78a-48fc-ad08-4802458b43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from gpt import MultiHeadAttention, LayerNorm, FeedForward\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate_attn\"], # 新增：多头注意力的 dropout\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate_shortcut\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # 形状 [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # 将原始输入加回来\n",
    "\n",
    "        # 前馈块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # 将原始输入加回来\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"]) # 新增：embedding 层的 dropout\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # 形状 [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d013d32-c275-4f42-be21-9010f1537227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch04/README.md">
# 第四章：从零实现 GPT 模型以生成文本

&nbsp;
## 章节主要代码

- [01_main-chapter-code](01_main-chapter-code) 包含了章节的主要代码。

&nbsp;
## 额外材料

- [02_performance-analysis](02_performance-analysis) 包含了可选代码，用于分析章节中实现的 GPT 模型的性能。

- [ch05/07_gpt_to_llama](../ch05/07_gpt_to_llama) 包含了将 GPT 架构实现转换为 Llama 3.2 的逐步指南，并从 Meta AI 加载预训练权重（完成第四章后可以查看不同的架构，但你也可以把它留到第五章后再看）。
</file>

<file path="ch05/01_main-chapter-code/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch05/01_main-chapter-code/gpt_generate.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import json
import numpy as np
import os
import urllib.request

# import requests
import tensorflow as tf
import tiktoken
import torch
from tqdm import tqdm

# Import from local files
from previous_chapters import GPTModel


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def download_file(url, destination):
    # Send a GET request to download the file
    with urllib.request.urlopen(url) as response:
        # Get the total file size from headers, defaulting to 0 if not present
        file_size = int(response.headers.get("Content-Length", 0))

        # Check if file exists and has the same size
        if os.path.exists(destination):
            file_size_local = os.path.getsize(destination)
            if file_size == file_size_local:
                print(f"File already exists and is up-to-date: {destination}")
                return

        # Define the block size for reading the file
        block_size = 1024  # 1 Kilobyte

        # Initialize the progress bar with total file size
        progress_bar_description = os.path.basename(url)  # Extract filename from URL
        with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
            # Open the destination file in binary write mode
            with open(destination, "wb") as file:
                # Read the file in chunks and write to destination
                while True:
                    chunk = response.read(block_size)
                    if not chunk:
                        break
                    file.write(chunk)
                    progress_bar.update(len(chunk))  # Update progress bar


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx


def main(gpt_config, input_prompt, model_size):

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

    gpt = GPTModel(gpt_config)
    load_weights_into_gpt(gpt, params)
    gpt.to(device)
    gpt.eval()

    tokenizer = tiktoken.get_encoding("gpt2")
    torch.manual_seed(123)

    token_ids = generate(
        model=gpt,
        idx=text_to_token_ids(input_prompt, tokenizer).to(device),
        max_new_tokens=25,
        context_size=gpt_config["context_length"],
        top_k=50,
        temperature=1.0
    )

    print("Output text:\n", token_ids_to_text(token_ids, tokenizer))


if __name__ == "__main__":

    torch.manual_seed(123)

    CHOOSE_MODEL = "gpt2-small (124M)"
    INPUT_PROMPT = "Every effort moves you"

    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")

    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

    main(BASE_CONFIG, INPUT_PROMPT, model_size)
</file>

<file path="ch05/01_main-chapter-code/gpt_train.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import matplotlib.pyplot as plt
import os
import torch
import urllib.request
import tiktoken


# Import from local files
from previous_chapters import GPTModel, create_dataloader_v1, generate_text_simple


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()


def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen = 0
    global_step = -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            tokens_seen += input_batch.numel()
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Print a sample text after each epoch
        generate_and_print_sample(
            model, tokenizer, device, start_context
        )

    return train_losses, val_losses, track_tokens_seen


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots()

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    # plt.show()


def main(gpt_config, settings):

    torch.manual_seed(123)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ##############################
    # Download data if necessary
    ##############################

    file_path = "the-verdict.txt"
    url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode('utf-8')
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()

    ##############################
    # Initialize model
    ##############################

    model = GPTModel(gpt_config)
    model.to(device)  # no assignment model = model.to(device) necessary for nn.Module classes
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=settings["learning_rate"], weight_decay=settings["weight_decay"]
    )

    ##############################
    # Set up dataloaders
    ##############################

    # Train/validation ratio
    train_ratio = 0.90
    split_idx = int(train_ratio * len(text_data))

    train_loader = create_dataloader_v1(
        text_data[:split_idx],
        batch_size=settings["batch_size"],
        max_length=gpt_config["context_length"],
        stride=gpt_config["context_length"],
        drop_last=True,
        shuffle=True,
        num_workers=0
    )

    val_loader = create_dataloader_v1(
        text_data[split_idx:],
        batch_size=settings["batch_size"],
        max_length=gpt_config["context_length"],
        stride=gpt_config["context_length"],
        drop_last=False,
        shuffle=False,
        num_workers=0
    )

    ##############################
    # Train model
    ##############################

    tokenizer = tiktoken.get_encoding("gpt2")

    train_losses, val_losses, tokens_seen = train_model_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=settings["num_epochs"], eval_freq=5, eval_iter=1,
        start_context="Every effort moves you", tokenizer=tokenizer
    )

    return train_losses, val_losses, tokens_seen, model


if __name__ == "__main__":

    GPT_CONFIG_124M = {
        "vocab_size": 50257,    # Vocabulary size
        "context_length": 256,  # Shortened context length (orig: 1024)
        "emb_dim": 768,         # Embedding dimension
        "n_heads": 12,          # Number of attention heads
        "n_layers": 12,         # Number of layers
        "drop_rate": 0.1,       # Dropout rate
        "qkv_bias": False       # Query-key-value bias
    }

    OTHER_SETTINGS = {
        "learning_rate": 5e-4,
        "num_epochs": 10,
        "batch_size": 2,
        "weight_decay": 0.1
    }

    ###########################
    # Initiate training
    ###########################

    train_losses, val_losses, tokens_seen, model = main(GPT_CONFIG_124M, OTHER_SETTINGS)

    ###########################
    # After training
    ###########################

    # Plot results
    epochs_tensor = torch.linspace(0, OTHER_SETTINGS["num_epochs"], len(train_losses))
    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
    plt.savefig("loss.pdf")

    # Save and load model
    torch.save(model.state_dict(), "model.pth")
    model = GPTModel(GPT_CONFIG_124M)
    model.load_state_dict(torch.load("model.pth", weights_only=True))
</file>

<file path="ch05/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


if __name__ == "__main__":

    GPT_CONFIG_124M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "emb_dim": 768,          # Embedding dimension
        "n_heads": 12,           # Number of attention heads
        "n_layers": 12,          # Number of layers
        "drop_rate": 0.1,        # Dropout rate
        "qkv_bias": False        # Query-Key-Value bias
    }

    torch.manual_seed(123)
    model = GPTModel(GPT_CONFIG_124M)
    model.eval()  # disable dropout

    start_context = "Hello, I am"

    tokenizer = tiktoken.get_encoding("gpt2")
    encoded = tokenizer.encode(start_context)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)

    print(f"\n{50*'='}\n{22*' '}IN\n{50*'='}")
    print("\nInput text:", start_context)
    print("Encoded input text:", encoded)
    print("encoded_tensor.shape:", encoded_tensor.shape)

    out = generate_text_simple(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=10,
        context_size=GPT_CONFIG_124M["context_length"]
    )
    decoded_text = tokenizer.decode(out.squeeze(0).tolist())

    print(f"\n\n{50*'='}\n{22*' '}OUT\n{50*'='}")
    print("\nOutput:", out)
    print("Output length:", len(out[0]))
    print("Output text:", decoded_text)
</file>

<file path="ch05/01_main-chapter-code/README.md">
# 第五章：在无标签数据上进行预训练

### 章节主要代码

- [ch05.ipynb](ch05.ipynb) 包含了章节中出现的所有代码。
- [previous_chapters.py](previous_chapters.py) 是一个 Python 模块，包含了上一章的 `MultiHeadAttention` 模块和 `GPTModel` 类，我们在 [ch05.ipynb](ch05.ipynb) 中导入该模块以进行 GPT 模型的预训练。
- [gpt_download.py](gpt_download.py) 包含了用于下载预训练 GPT 模型权重的工具函数。
- [exercise-solutions.ipynb](exercise-solutions.ipynb) 包含了本章习题的解答。

### 可选代码

- [gpt_train.py](gpt_train.py) 是一个独立的 Python 脚本文件，包含了我们在 [ch05.ipynb](ch05.ipynb) 中实现的训练 GPT 模型的代码（你可以将其看作是本章代码的总结）。
- [gpt_generate.py](gpt_generate.py) 是一个独立的 Python 脚本文件，包含了我们在 [ch05.ipynb](ch05.ipynb) 中实现的加载并使用 OpenAI 提供的预训练模型权重的代码。
</file>

<file path="ch05/01_main-chapter-code/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)

import pytest
from gpt_train import main
import http.client
from urllib.parse import urlparse


@pytest.fixture
def gpt_config():
    return {
        "vocab_size": 50257,
        "context_length": 12,  # small for testing efficiency
        "emb_dim": 32,         # small for testing efficiency
        "n_heads": 4,          # small for testing efficiency
        "n_layers": 2,         # small for testing efficiency
        "drop_rate": 0.1,
        "qkv_bias": False
    }


@pytest.fixture
def other_settings():
    return {
        "learning_rate": 5e-4,
        "num_epochs": 1,    # small for testing efficiency
        "batch_size": 2,
        "weight_decay": 0.1
    }


def test_main(gpt_config, other_settings):
    train_losses, val_losses, tokens_seen, model = main(gpt_config, other_settings)

    assert len(train_losses) == 39, "Unexpected number of training losses"
    assert len(val_losses) == 39, "Unexpected number of validation losses"
    assert len(tokens_seen) == 39, "Unexpected number of tokens seen"


def check_file_size(url, expected_size):
    parsed_url = urlparse(url)
    if parsed_url.scheme == "https":
        conn = http.client.HTTPSConnection(parsed_url.netloc)
    else:
        conn = http.client.HTTPConnection(parsed_url.netloc)

    conn.request("HEAD", parsed_url.path)
    response = conn.getresponse()
    if response.status != 200:
        return False, f"{url} not accessible"
    size = response.getheader("Content-Length")
    if size is None:
        return False, "Content-Length header is missing"
    size = int(size)
    if size != expected_size:
        return False, f"{url} file has expected size {expected_size}, but got {size}"
    return True, f"{url} file size is correct"


def test_model_files():
    def check_model_files(base_url):

        model_size = "124M"
        files = {
            "checkpoint": 77,
            "encoder.json": 1042301,
            "hparams.json": 90,
            "model.ckpt.data-00000-of-00001": 497759232,
            "model.ckpt.index": 5215,
            "model.ckpt.meta": 471155,
            "vocab.bpe": 456318
        }

        for file_name, expected_size in files.items():
            url = f"{base_url}/{model_size}/{file_name}"
            valid, message = check_file_size(url, expected_size)
            assert valid, message

        model_size = "355M"
        files = {
            "checkpoint": 77,
            "encoder.json": 1042301,
            "hparams.json": 91,
            "model.ckpt.data-00000-of-00001": 1419292672,
            "model.ckpt.index": 10399,
            "model.ckpt.meta": 926519,
            "vocab.bpe": 456318
        }

        for file_name, expected_size in files.items():
            url = f"{base_url}/{model_size}/{file_name}"
            valid, message = check_file_size(url, expected_size)
            assert valid, message

    check_model_files(base_url="https://openaipublic.blob.core.windows.net/gpt-2/models")
    check_model_files(base_url="https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2")
</file>

<file path="ch05/02_alternative_weight_loading/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
</file>

<file path="ch05/03_bonus_pretraining_on_gutenberg/prepare_dataset.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

"""
Script that processes the Project Gutenberg files into fewer larger files.
"""

import argparse
import os
import re
from tqdm import tqdm
from gutenberg.src.cleanup import strip_headers


def is_english(text, threshold=0.9):
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    return ascii_chars / len(text) > threshold


def combine_files(file_paths, target_dir, max_size_mb=500, separator="<|endoftext|>", fallback_encoding="latin1"):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    current_content = []
    current_size = 0
    file_counter = 1

    for file_path in tqdm(file_paths):
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read()
        except UnicodeDecodeError:
            # Attempt to read the file with a fallback encoding
            tqdm.write(f"Warning: UnicodeDecodeError encountered. Trying fallback encoding for {file_path}")
            with open(file_path, "r", encoding=fallback_encoding) as file:
                content = file.read()

        if not is_english(content):
            tqdm.write(f"Skipping {file_path} as it does not contain primarily English text.")
            continue
        content = strip_headers(content)

        # Regular expression to replace multiple blank lines with a single blank line
        content = re.sub(r'\n\s*\n', '\n\n', content)
        estimated_size = len(content.encode("utf-8"))

        if current_size + estimated_size > max_size_mb * 1024 * 1024:
            target_file_path = os.path.join(target_dir, f"combined_{file_counter}.txt")
            with open(target_file_path, "w", encoding="utf-8") as target_file:
                target_file.write(separator.join(current_content))
            file_counter += 1
            current_content = [content]
            current_size = estimated_size
        else:
            current_content.append(content)
            current_size += estimated_size

    if current_content:
        target_file_path = os.path.join(target_dir, f"combined_{file_counter}.txt")
        with open(target_file_path, "w", encoding="utf-8") as target_file:
            target_file.write(separator.join(current_content))
    return file_counter


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Preprocess and combine text files for pretraining")

    parser.add_argument("--data_dir", type=str, default="gutenberg/data/raw",
                        help="Directory containing the downloaded raw training data")
    parser.add_argument("--max_size_mb", type=int, default=500,
                        help="The maximum file size for each concatenated file in megabytes")
    parser.add_argument("--output_dir", type=str, default="gutenberg_preprocessed",
                        help="Directory where the preprocessed data will be saved")

    args = parser.parse_args()

    all_files = [os.path.join(path, name) for path, subdirs, files in os.walk(args.data_dir)
                 for name in files if name.endswith((".txt", ".txt.utf8"))]

    print(f"{len(all_files)} file(s) to process.")
    file_counter = combine_files(all_files, args.output_dir, max_size_mb=args.max_size_mb)
    print(f"{file_counter} file(s) saved in {os.path.abspath(args.output_dir)}")
</file>

<file path="ch05/03_bonus_pretraining_on_gutenberg/pretraining_simple.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

"""
Script for pretraining a small GPT-2 124M parameter model
on books from Project Gutenberg.

Before running this script, make sure you downloaded and
processed the dataset as described in the README.md.
"""

import argparse
import os
from pathlib import Path
import time
import tiktoken
import torch
from previous_chapters import (
    create_dataloader_v1,
    GPTModel,
    generate_and_print_sample,
    calc_loss_batch,
    evaluate_model,
    plot_losses
)


def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        text_data = file.read()
    return text_data


def create_dataloaders(text_data, train_ratio, batch_size, max_length, stride, num_workers=0):
    split_idx = int(train_ratio * len(text_data))
    train_loader = create_dataloader_v1(
        text_data[:split_idx],
        batch_size=batch_size,
        max_length=max_length,
        stride=stride,
        drop_last=True,
        shuffle=True,
        num_workers=num_workers
    )
    val_loader = create_dataloader_v1(
        text_data[split_idx:],
        batch_size=batch_size,
        max_length=max_length,
        stride=stride,
        drop_last=False,
        shuffle=False,
        num_workers=num_workers
    )
    return train_loader, val_loader


def convert_time(seconds):
    hours, rem = divmod(seconds, 3600)
    minutes, seconds = divmod(rem, 60)
    return int(hours), int(minutes), int(seconds)


def print_eta(start_time, book_start_time, index, total_files):
    book_end_time = time.time()  # End time of processing this book
    elapsed_time = book_end_time - book_start_time
    total_elapsed_time = book_end_time - start_time
    books_remaining = total_files - index
    average_time_per_book = total_elapsed_time / index
    eta = average_time_per_book * books_remaining

    book_h, book_m, book_s = convert_time(elapsed_time)
    total_h, total_m, total_s = convert_time(total_elapsed_time)
    eta_h, eta_m, eta_s = convert_time(eta)

    print(f"Book processed {book_h}h {book_m}m {book_s}s"
          f"\nTotal time elapsed {total_h}h {total_m}m {total_s}s"
          f"\nETA for remaining books: {eta_h}h {eta_m}m {eta_s}s")


def train_model_simple(model, optimizer, device, n_epochs,
                       eval_freq, eval_iter, print_sample_iter, start_context,
                       output_dir, save_ckpt_freq, tokenizer,
                       batch_size=1024, train_ratio=0.90):

    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen = 0
    global_step = -1
    start_time = time.time()

    try:
        for epoch in range(n_epochs):

            # Iterate over the books in the training corpus
            for index, file_path in enumerate(all_files, 1):
                book_start_time = time.time()
                text_data = read_text_file(file_path) + " <|endoftext|> "
                print(f"Tokenizing file {index} of {total_files}: {file_path}")

                # Initialize new data loaders for each book
                train_loader, val_loader = create_dataloaders(
                    text_data,
                    train_ratio=train_ratio,
                    batch_size=batch_size,
                    max_length=GPT_CONFIG_124M["context_length"],
                    stride=GPT_CONFIG_124M["context_length"],
                    num_workers=0
                )
                print("Training ...")
                model.train()
                for input_batch, target_batch in train_loader:
                    optimizer.zero_grad()
                    loss = calc_loss_batch(input_batch, target_batch, model, device)
                    loss.backward()
                    optimizer.step()
                    tokens_seen += input_batch.numel()
                    global_step += 1

                    # Optional evaluation step
                    if global_step % eval_freq == 0:
                        train_loss, val_loss = evaluate_model(
                            model, train_loader, val_loader, device, eval_iter)
                        train_losses.append(train_loss)
                        val_losses.append(val_loss)
                        track_tokens_seen.append(tokens_seen)
                        print(f"Ep {epoch+1} (Step {global_step}): "
                              f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

                    # Generate text passage
                    if global_step % print_sample_iter == 0:
                        generate_and_print_sample(
                            model, tokenizer, device, start_context
                        )

                if global_step % save_ckpt_freq:
                    file_name = output_dir / f"model_pg_{global_step}.pth"
                    torch.save(model.state_dict(), file_name)
                    print(f"Saved {file_name}")

                print_eta(start_time, book_start_time, index, total_files)

    except KeyboardInterrupt:
        file_name = output_dir / f"model_pg_{global_step}_interrupted.pth"
        torch.save(model.state_dict(), file_name)
        print(f"Saved {file_name}")

    return train_losses, val_losses, track_tokens_seen


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='GPT Model Training Configuration')

    parser.add_argument('--data_dir', type=str, default='gutenberg/data',
                        help='Directory containing the training data')
    parser.add_argument('--output_dir', type=str, default='model_checkpoints',
                        help='Directory where the model checkpoints will be saved')
    parser.add_argument('--n_epochs', type=int, default=1,
                        help='Number of epochs to train the model')
    parser.add_argument('--print_sample_iter', type=int, default=1000,
                        help='Iterations between printing sample outputs')
    parser.add_argument('--eval_freq', type=int, default=100,
                        help='Frequency of evaluations during training')
    parser.add_argument('--save_ckpt_freq', type=int, default=100_000,
                        help='Frequency of saving model checkpoints during training')
    parser.add_argument('--lr', type=float, default=5e-4,
                        help='Learning rate for the optimizer')
    parser.add_argument('--batch_size', type=int, default=4,
                        help='Batch size for training')
    parser.add_argument('--debug', type=bool, default=False,
                        help='Uses a very small model for debugging purposes')

    args = parser.parse_args()

    if args.debug:
        GPT_CONFIG_124M = {
            "vocab_size": 50257,     # Vocabulary size
            "context_length": 10,    # Context length
            "emb_dim": 12,           # Embedding dimension
            "n_heads": 2,            # Number of attention heads
            "n_layers": 2,           # Number of layers
            "drop_rate": 0.0,        # Dropout rate, deactivated via 0.0 as dropout in LLMs is not recommended anymore
            "qkv_bias": False        # Query-key-value bias
        }

    else:
        GPT_CONFIG_124M = {
            "vocab_size": 50257,     # Vocabulary size
            "context_length": 1024,  # Context length
            "emb_dim": 768,          # Embedding dimension
            "n_heads": 12,           # Number of attention heads
            "n_layers": 12,          # Number of layers
            "drop_rate": 0.1,        # Dropout rate
            "qkv_bias": False        # Query-key-value bias
        }

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.manual_seed(123)
    model = GPTModel(GPT_CONFIG_124M)
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.1)
    tokenizer = tiktoken.get_encoding("gpt2")

    data_dir = args.data_dir
    all_files = [os.path.join(path, name) for path, subdirs, files
                 in os.walk(data_dir) for name in files if name.endswith((".txt"))]
    total_files = len(all_files)

    if total_files == 0:
        print("No training text files found. Make sure you "
              "selected the correct input directory")
        quit()
    print("Total files:", total_files)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    train_losses, val_losses, tokens_seen = train_model_simple(
        model, optimizer, device,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        eval_freq=args.eval_freq,
        eval_iter=1,
        print_sample_iter=args.print_sample_iter,
        output_dir=output_dir,
        save_ckpt_freq=args.save_ckpt_freq,
        start_context="Every effort moves you",
        tokenizer=tokenizer
    )

    epochs_tensor = torch.linspace(0, args.n_epochs, len(train_losses))
    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, output_dir)

    torch.save(model.state_dict(), output_dir / "model_pg_final.pth")
    print(f"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
</file>

<file path="ch05/03_bonus_pretraining_on_gutenberg/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})

        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################

class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################

class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
####################################


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size)
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses, output_dir):
    fig, ax1 = plt.subplots()

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig(output_dir / "losses.pdf")


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # Remove batch dimension
    return tokenizer.decode(flat.tolist())
</file>

<file path="ch05/03_bonus_pretraining_on_gutenberg/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)

from pathlib import Path
import os
import subprocess


def test_pretraining():

    sequence = "a b c d"
    repetitions = 1000
    content = sequence * repetitions

    folder_path = Path("gutenberg") / "data"
    file_name = "repeated_sequence.txt"

    os.makedirs(folder_path, exist_ok=True)

    with open(folder_path/file_name, "w") as file:
        file.write(content)

    result = subprocess.run(
        ["python", "pretraining_simple.py", "--debug", "true"],
        capture_output=True, text=True
    )
    print(result.stdout)
    assert "Maximum GPU memory allocated" in result.stdout
</file>

<file path="ch05/05_bonus_hparam_tuning/hparam_search.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import itertools
import math
import os
import tiktoken
import torch
from previous_chapters import GPTModel, create_dataloader_v1


# Define a grid of hyperparameters to search over
HPARAM_GRID = {
    "batch_size": [2, 4, 8, 16],
    "drop_rate": [0.0, 0.1, 0.2],
    "warmup_iters": [10, 20, 30],
    "weight_decay": [0.1, 0.01, 0.0],
    "peak_lr": [0.0001, 0.0005, 0.001, 0.005],
    "initial_lr": [0.00005, 0.0001],
    "min_lr": [0.00005, 0.00001, 0.0001],
    "n_epochs": [5, 10, 15, 20, 25],
}


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)

    logits = model(input_batch)
    logits = logits.view(-1, logits.size(-1))
    loss = torch.nn.functional.cross_entropy(logits, target_batch.view(-1))
    return loss


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def train_model(model, train_loader, val_loader, optimizer, device,
                n_epochs, eval_freq, eval_iter,
                encoded_start_context, tokenizer, warmup_iters=10,
                initial_lr=3e-05, min_lr=1e-6):
    global_step = 0

    max_lr = optimizer.param_groups[0]["lr"]

    # Calculate total number of iterations
    total_training_iters = len(train_loader) * n_epochs

    # Calculate the learning rate increment at each step during warmup
    lr_increment = (optimizer.param_groups[0]["lr"] - initial_lr) / warmup_iters

    for epoch in range(n_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()

            # Increment the global step at the beginning of the iteration
            global_step += 1

            # Warmup: adjust learning rate linearly
            if global_step <= warmup_iters:
                lr = initial_lr + global_step * lr_increment
            # Cosine annealing phase
            else:
                progress = (global_step - warmup_iters) / (total_training_iters - warmup_iters)
                lr = min_lr + (max_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))

            # Apply the calculated learning rate
            for param_group in optimizer.param_groups:
                param_group["lr"] = lr

            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()

            # Apply gradient clipping
            if global_step >= warmup_iters:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)

    return train_loss, val_loss


if __name__ == "__main__":

    # Generate all combinations of hyperparameters
    hyperparameter_combinations = list(itertools.product(*HPARAM_GRID.values()))
    total_combinations = len(hyperparameter_combinations)
    print(f"Total hyperparameter configurations: {total_combinations}")

    # Placeholder for the best loss and best hyperparameters
    best_val_loss = float('inf')
    best_hparams = {}

    script_path = os.path.abspath(__file__)
    script_dir = os.path.dirname(script_path)
    with open(os.path.join(script_dir, "the-verdict.txt"), "r", encoding="utf-8") as file:
        text_data = file.read()

    tokenizer = tiktoken.get_encoding("gpt2")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_ratio = 0.95
    split_idx = int(train_ratio * len(text_data))

    torch.manual_seed(123)

    interrupted = False
    current_config = 0
    for combination in hyperparameter_combinations:

        try:
            current_config += 1
            print(f"Evaluating configuration {current_config} of {total_combinations}")

            # Unpack the current combination of hyperparameters
            HPARAM_CONFIG = dict(zip(HPARAM_GRID.keys(), combination))

            GPT_CONFIG_124M = {
                "vocab_size": 50257,    # Vocabulary size
                "context_length": 256,  # Context length -- shortened from original 1024 tokens
                "emb_dim": 768,         # Embedding dimension
                "n_heads": 12,          # Number of attention heads
                "n_layers": 12,         # Number of layers
                "drop_rate": HPARAM_CONFIG["drop_rate"],
                "qkv_bias": False,     # Query-Key-Value bias
            }

            torch.manual_seed(123)
            train_loader = create_dataloader_v1(
                text_data[:split_idx],
                batch_size=HPARAM_CONFIG["batch_size"],
                max_length=GPT_CONFIG_124M["context_length"],
                stride=GPT_CONFIG_124M["context_length"],
                drop_last=True,
                shuffle=True,
                num_workers=0
            )

            val_loader = create_dataloader_v1(
                text_data[split_idx:],
                batch_size=HPARAM_CONFIG["batch_size"],
                max_length=GPT_CONFIG_124M["context_length"],
                stride=GPT_CONFIG_124M["context_length"],
                drop_last=False,
                shuffle=False,
                num_workers=0
            )

            model = GPTModel(GPT_CONFIG_124M)
            model.to(device)

            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=HPARAM_CONFIG["peak_lr"],
                weight_decay=HPARAM_CONFIG["weight_decay"]
            )

            encoded_start_context = tokenizer.encode("Nevertheless")
            encoded_tensor = torch.tensor(encoded_start_context).unsqueeze(0)

            train_loss, val_loss = train_model(
                model, train_loader, val_loader, optimizer, device,
                n_epochs=HPARAM_CONFIG["n_epochs"],
                eval_freq=5, eval_iter=1,
                encoded_start_context=encoded_tensor,
                tokenizer=tokenizer,
                warmup_iters=HPARAM_CONFIG["warmup_iters"],
                initial_lr=HPARAM_CONFIG["initial_lr"],
                min_lr=HPARAM_CONFIG["min_lr"]
            )

            # Log the best hyperparameters based on validation loss
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_train_loss = train_loss
                best_hparams = HPARAM_CONFIG

        except KeyboardInterrupt:
            print("Hyperparameter search completed.")
            print(f"Best hyperparameters: {best_hparams}")
            print(f"Best Val loss: {best_val_loss} | Training loss {train_loss}")
            interrupted = True
            break

    if not interrupted:
        print("Hyperparameter search completed.")
        print(f"Best hyperparameters: {best_hparams}")
        print(f"Best Val loss: {best_val_loss} | Training loss {train_loss}")
</file>

<file path="ch05/05_bonus_hparam_tuning/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


if __name__ == "__main__":

    GPT_CONFIG_124M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "emb_dim": 768,          # Embedding dimension
        "n_heads": 12,           # Number of attention heads
        "n_layers": 12,          # Number of layers
        "drop_rate": 0.1,        # Dropout rate
        "qkv_bias": False        # Query-Key-Value bias
    }

    torch.manual_seed(123)
    model = GPTModel(GPT_CONFIG_124M)
    model.eval()  # disable dropout

    start_context = "Hello, I am"

    tokenizer = tiktoken.get_encoding("gpt2")
    encoded = tokenizer.encode(start_context)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)

    print(f"\n{50*'='}\n{22*' '}IN\n{50*'='}")
    print("\nInput text:", start_context)
    print("Encoded input text:", encoded)
    print("encoded_tensor.shape:", encoded_tensor.shape)

    out = generate_text_simple(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=10,
        context_size=GPT_CONFIG_124M["context_length"]
    )
    decoded_text = tokenizer.decode(out.squeeze(0).tolist())

    print(f"\n\n{50*'='}\n{22*' '}OUT\n{50*'='}")
    print("\nOutput:", out)
    print("Output length:", len(out[0]))
    print("Output text:", decoded_text)
</file>

<file path="ch05/05_bonus_hparam_tuning/the-verdict.txt">
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. "Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of." The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's "Moon-dancers" to say, with tears in her eyes: "We shall not look upon its like again"?

Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome "obituary" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of "Gisburns" went up.

It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had "dragged him down." For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.

Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to "lift him up"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.

The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.

I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was "interesting": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the "deadening atmosphere of mediocrity" (I quote Miss Croft) was having on him.

I have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.

"Money's only excuse is to put beauty into circulation," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: "Jack is so morbidly sensitive to every form of beauty."

Poor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.

"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle," was his only protest, as he rose from the table and strolled out onto the sunlit terrace.

I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend's feet, and I wondered if a tinge of jealousy underlay the latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their "Grindles."

I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.

"Why _has_ he chucked painting?" I asked abruptly.

She raised her eyebrows with a hint of good-humoured surprise.

"Oh, he doesn't _have_ to now, you know; and I want him to enjoy himself," she said quite simply.

I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.

"Has he chucked his pictures too? I haven't seen a single one in the house."

A slight shade of constraint crossed Mrs. Gisburn's open countenance. "It's his ridiculous modesty, you know. He says they're not fit to have about; he's sent them all away except one--my portrait--and that I have to keep upstairs."

His ridiculous modesty--Jack's modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: "I must really see your portrait, you know."

She glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound's head between his knees.

"Well, come while he's not looking," she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.

In the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!

Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: "If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay."

Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's "strongest," as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown's ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted "strongly" because she was tired of being painted "sweetly"--and yet not to lose an atom of the sweetness.

"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride. "The last but one," she corrected herself--"but the other doesn't count, because he destroyed it."

"Destroyed it?" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.

As he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.

His wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.

"Mr. Rickham wanted to see it," she began, as if excusing herself. He shrugged his shoulders, still smiling.

"Oh, Rickham found me out long ago," he said lightly; then, passing his arm through mine: "Come and see the rest of the house."

He showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire's domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: "Yes, I really don't see how people manage to live without that."

Well--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: "Be dissatisfied with your leisure!" as once one had longed to say: "Be dissatisfied with your work!"

But, with the cry on my lips, my diagnosis suffered an unexpected check.

"This is my own lair," he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no "effects"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.

The fact brought home to me the absolute finality of Jack's break with his old life.

"Don't you ever dabble with paint any more?" I asked, still looking about for a trace of such activity.

"Never," he said briefly.

"Or water-colour--or etching?"

His confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.

"Never think of it, my dear fellow--any more than if I'd never touched a brush."

And his tone told me in a flash that he never thought of anything else.

I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.

"Oh, by Jove!" I said.

It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.

"By Jove--a Stroud!" I cried.

He was silent; but I felt him close behind me, breathing a little quickly.

"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?"

He answered slowly: "Mrs. Stroud gave it to me."

"Ah--I didn't know you even knew the Strouds. He was such an inflexible hermit."

"I didn't--till after. . . . She sent for me to paint him when he was dead."

"When he was dead? You?"

I must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: "Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter."

"Ah, poor Stroud--as you say. Was _that_ his history?"

"That was his history. She believed in him, gloried in him--or thought she did. But she couldn't bear not to have all the drawing-rooms with her. She couldn't bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She's just a fragment groping for other fragments. Stroud is the only whole I ever knew."

"You ever knew? But you just said--"

Gisburn had a curious smile in his eyes.

"Oh, I knew him, and he knew me--only it happened after he was dead."

I dropped my voice instinctively. "When she sent for you?"

"Yes--quite insensible to the irony. She wanted him vindicated--and by me!"

He laughed again, and threw back his head to look up at the sketch of the donkey. "There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. That's the reason why I don't dabble any more, my dear Rickham; or rather Stroud himself is the reason."

For the first time my idle curiosity about my companion turned into a serious desire to understand him better.

"I wish you'd tell me how it happened," I said.

He stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.

"I'd rather like to tell you--because I've always suspected you of loathing my work."

I made a deprecating gesture, which he negatived with a good-humoured shrug.

"Oh, I didn't care a straw when I believed in myself--and now it's an added tie between us!"

He laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. "There: make yourself comfortable--and here are the cigars you like."

He placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.

"How it happened? I can tell you in five minutes--and it didn't take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud's note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.

"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud's career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.

"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.

"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a 'subject.' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.

"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn't watching the showy bits--I couldn't distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!

"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .

"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn't been born of me--I had just adopted them. . . .

"Hang it, Rickham, with that face watching me I couldn't do another stroke. The plain truth was, I didn't know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don't you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my 'technique' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'

"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?

"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . ."

He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.

"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day."

And, in answer to a question I put half-mechanically--"Begin again?" he flashed out. "When the one thing that brings me anywhere near him is that I knew enough to leave off?"

He stood up and laid his hand on my shoulder with a laugh. "Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art."
</file>

<file path="ch05/06_user_interface/app_orig.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import tiktoken
import torch
import chainlit

from previous_chapters import (
    download_and_load_gpt2,
    generate,
    GPTModel,
    load_weights_into_gpt,
    text_to_token_ids,
    token_ids_to_text,
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def get_model_and_tokenizer():
    """
    Code to load a GPT-2 model with pretrained weights from OpenAI.
    The code is similar to chapter 5.
    The model will be downloaded automatically if it doesn't exist in the current folder, yet.
    """

    CHOOSE_MODEL = "gpt2-small (124M)"  # Optionally replace with another model from the model_configs dir below

    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")

    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

    settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

    gpt = GPTModel(BASE_CONFIG)
    load_weights_into_gpt(gpt, params)
    gpt.to(device)
    gpt.eval()

    tokenizer = tiktoken.get_encoding("gpt2")

    return tokenizer, gpt, BASE_CONFIG


# Obtain the necessary tokenizer and model files for the chainlit function below
tokenizer, model, model_config = get_model_and_tokenizer()


@chainlit.on_message
async def main(message: chainlit.Message):
    """
    The main Chainlit function.
    """
    token_ids = generate(  # function uses `with torch.no_grad()` internally already
        model=model,
        idx=text_to_token_ids(message.content, tokenizer).to(device),  # The user text is provided via as `message.content`
        max_new_tokens=50,
        context_size=model_config["context_length"],
        top_k=1,
        temperature=0.0
    )

    text = token_ids_to_text(token_ids, tokenizer)

    await chainlit.Message(
        content=f"{text}",  # This returns the model response to the interface
    ).send()
</file>

<file path="ch05/06_user_interface/app_own.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

from pathlib import Path
import sys

import tiktoken
import torch
import chainlit

from previous_chapters import (
    generate,
    GPTModel,
    text_to_token_ids,
    token_ids_to_text,
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def get_model_and_tokenizer():
    """
    Code to load a GPT-2 model with pretrained weights generated in chapter 5.
    This requires that you run the code in chapter 5 first, which generates the necessary model.pth file.
    """

    GPT_CONFIG_124M = {
        "vocab_size": 50257,    # Vocabulary size
        "context_length": 256,  # Shortened context length (orig: 1024)
        "emb_dim": 768,         # Embedding dimension
        "n_heads": 12,          # Number of attention heads
        "n_layers": 12,         # Number of layers
        "drop_rate": 0.1,       # Dropout rate
        "qkv_bias": False       # Query-key-value bias
    }

    tokenizer = tiktoken.get_encoding("gpt2")

    model_path = Path("..") / "01_main-chapter-code" / "model.pth"
    if not model_path.exists():
        print(f"Could not find the {model_path} file. Please run the chapter 5 code (ch05.ipynb) to generate the model.pth file.")
        sys.exit()

    checkpoint = torch.load(model_path, weights_only=True)
    model = GPTModel(GPT_CONFIG_124M)
    model.load_state_dict(checkpoint)
    model.to(device)

    return tokenizer, model, GPT_CONFIG_124M


# Obtain the necessary tokenizer and model files for the chainlit function below
tokenizer, model, model_config = get_model_and_tokenizer()


@chainlit.on_message
async def main(message: chainlit.Message):
    """
    The main Chainlit function.
    """
    token_ids = generate(  # function uses `with torch.no_grad()` internally already
        model=model,
        idx=text_to_token_ids(message.content, tokenizer).to(device),  # The user text is provided via as `message.content`
        max_new_tokens=50,
        context_size=model_config["context_length"],
        top_k=1,
        temperature=0.0
    )

    text = token_ids_to_text(token_ids, tokenizer)

    await chainlit.Message(
        content=f"{text}",  # This returns the model response to the interface
    ).send()
</file>

<file path="ch05/06_user_interface/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.

import json
import os
import urllib

import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn
from tqdm import tqdm


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


#####################################
# Chapter 5
#####################################
def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination):
    # Send a GET request to download the file
    with urllib.request.urlopen(url) as response:
        # Get the total file size from headers, defaulting to 0 if not present
        file_size = int(response.headers.get("Content-Length", 0))

        # Check if file exists and has the same size
        if os.path.exists(destination):
            file_size_local = os.path.getsize(destination)
            if file_size == file_size_local:
                print(f"File already exists and is up-to-date: {destination}")
                return

        # Define the block size for reading the file
        block_size = 1024  # 1 Kilobyte

        # Initialize the progress bar with total file size
        progress_bar_description = os.path.basename(url)  # Extract filename from URL
        with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
            # Open the destination file in binary write mode
            with open(destination, "wb") as file:
                # Read the file in chunks and write to destination
                while True:
                    chunk = response.read(block_size)
                    if not chunk:
                        break
                    file.write(chunk)
                    progress_bar.update(len(chunk))  # Update progress bar


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
</file>

<file path="ch05/06_user_interface/requirements-extra.txt">
chainlit>=1.2.0
</file>

<file path="ch05/07_gpt_to_llama/tests/test-requirements-extra.txt">
pytest>=8.1.1
transformers>=4.44.2
</file>

<file path="ch05/07_gpt_to_llama/tests/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)

import io
import os
import sys
import types
import nbformat
from packaging import version
from typing import Optional, Tuple
import torch
import pytest
import transformers
from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb


transformers_version = transformers.__version__

# LitGPT code function `litgpt_build_rope_cache` from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py
# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE


def litgpt_build_rope_cache(
    seq_len: int,
    n_elem: int,
    device: Optional[torch.device] = None,
    base: int = 10000,
    condense_ratio: int = 1,
    extra_config: Optional[dict] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Enhanced Transformer with Rotary Position Embedding.

    Args:
        seq_len (int): Sequence length.
        n_elem (int): Number of elements (head dimension).
        device (torch.device, optional): Device for tensor allocations.
        base (int, optional): Base for computing inverse frequencies.
        condense_ratio (int, optional): Ratio to condense the position indices.
        extra_config (dict, optional): Configuration parameters for frequency adjustments (used by Llama 3.1 and 3.2)

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: Cosine and sine caches for RoPE.
    """

    # Compute the inverse frequencies theta
    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device).float() / n_elem))

    if extra_config is not None:
        orig_context_len = extra_config["original_max_seq_len"]
        factor = extra_config["factor"]
        low_freq_factor = extra_config["low_freq_factor"]
        high_freq_factor = extra_config["high_freq_factor"]

        wavelen = 2 * torch.pi / theta
        ratio = orig_context_len / wavelen
        smooth_factor = (ratio - low_freq_factor) / (high_freq_factor - low_freq_factor)
        smooth_factor = torch.clamp(smooth_factor, min=0.0, max=1.0)

        # Compute adjusted_theta without masked indexing
        adjusted_theta = (1 - smooth_factor) * (theta / factor) + smooth_factor * theta
        theta = adjusted_theta

    # Create position indices `[0, 1, ..., seq_len - 1]`
    seq_idx = torch.arange(seq_len, device=device) / condense_ratio

    # Calculate the product of position index and $\theta_i$
    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)

    return torch.cos(idx_theta), torch.sin(idx_theta)


# LitGPT code from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py
# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE
def litgpt_apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:
    head_size = x.size(-1)
    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
    x2 = x[..., head_size // 2:]  # (B, nh, T, hs/2)
    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
    if cos.dim() > 1:
        # batch dimensions must align
        # sin/cos are (B, T, hs) so we unsqeeze -3 for nh
        # we count from back because all of apply_rope does
        cos = cos.unsqueeze(-3)
        sin = sin.unsqueeze(-3)

    roped = (x * cos) + (rotated * sin)
    return roped.to(dtype=x.dtype)


@pytest.fixture(scope="module")
def notebook():
    def import_definitions_from_notebook(notebooks):
        imported_modules = {}

        for fullname, names in notebooks.items():
            # Get the directory of the current test file
            current_dir = os.path.dirname(__file__)
            path = os.path.join(current_dir, "..", fullname + ".ipynb")
            path = os.path.normpath(path)

            # Load the notebook
            if not os.path.exists(path):
                raise FileNotFoundError(f"Notebook file not found at: {path}")

            with io.open(path, "r", encoding="utf-8") as f:
                nb = nbformat.read(f, as_version=4)

            # Create a module to store the imported functions and classes
            mod = types.ModuleType(fullname)
            sys.modules[fullname] = mod

            # Go through the notebook cells and only execute function or class definitions
            for cell in nb.cells:
                if cell.cell_type == "code":
                    cell_code = cell.source
                    for name in names:
                        # Check for function or class definitions
                        if f"def {name}" in cell_code or f"class {name}" in cell_code:
                            exec(cell_code, mod.__dict__)

            imported_modules[fullname] = mod

        return imported_modules

    notebooks = {
        "converting-gpt-to-llama2": ["SiLU", "RMSNorm", "precompute_rope_params", "compute_rope"],
        "converting-llama2-to-llama3": ["precompute_rope_params"]
    }

    return import_definitions_from_notebook(notebooks)


@pytest.fixture(autouse=True)
def set_seed():
    torch.manual_seed(123)


def test_rope_llama2(notebook):

    this_nb = notebook["converting-gpt-to-llama2"]

    # Settings
    batch_size = 1
    context_len = 4096
    num_heads = 4
    head_dim = 16
    theta_base = 10_000

    # Instantiate RoPE parameters
    cos, sin = this_nb.precompute_rope_params(head_dim=head_dim, context_length=context_len)

    # Dummy query and key tensors
    queries = torch.randn(batch_size, num_heads, context_len, head_dim)
    keys = torch.randn(batch_size, num_heads, context_len, head_dim)

    # Apply rotary position embeddings
    queries_rot = this_nb.compute_rope(queries, cos, sin)
    keys_rot = this_nb.compute_rope(keys, cos, sin)

    # Generate reference RoPE via HF

    if version.parse(transformers_version) < version.parse("4.48"):
        rot_emb = LlamaRotaryEmbedding(
            dim=head_dim,
            max_position_embeddings=context_len,
            base=theta_base
        )
    else:
        class RoPEConfig:
            dim: int = head_dim
            rope_theta = theta_base
            max_position_embeddings: int = 8192
            hidden_size = head_dim * num_heads
            num_attention_heads = num_heads

        config = RoPEConfig()
        rot_emb = LlamaRotaryEmbedding(config=config)

    position_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)
    ref_cos, ref_sin = rot_emb(queries, position_ids)
    ref_queries_rot, ref_keys_rot = apply_rotary_pos_emb(queries, keys, ref_cos, ref_sin)
    torch.testing.assert_close(sin, ref_sin.squeeze(0))
    torch.testing.assert_close(cos, ref_cos.squeeze(0))
    torch.testing.assert_close(keys_rot, ref_keys_rot)
    torch.testing.assert_close(queries_rot, ref_queries_rot)

    # Generate reference RoPE via LitGPT
    litgpt_cos, litgpt_sin = litgpt_build_rope_cache(context_len, n_elem=head_dim, base=10_000)
    litgpt_queries_rot = litgpt_apply_rope(queries, litgpt_cos, litgpt_sin)
    litgpt_keys_rot = litgpt_apply_rope(keys, litgpt_cos, litgpt_sin)

    torch.testing.assert_close(sin, litgpt_sin)
    torch.testing.assert_close(cos, litgpt_cos)
    torch.testing.assert_close(keys_rot, litgpt_keys_rot)
    torch.testing.assert_close(queries_rot, litgpt_queries_rot)


def test_rope_llama3(notebook):

    nb1 = notebook["converting-gpt-to-llama2"]
    nb2 = notebook["converting-llama2-to-llama3"]

    # Settings
    batch_size = 1
    context_len = 8192
    num_heads = 4
    head_dim = 16
    theta_base = 500_000

    # Instantiate RoPE parameters
    cos, sin = nb2.precompute_rope_params(
        head_dim=head_dim,
        context_length=context_len,
        theta_base=theta_base
    )

    # Dummy query and key tensors
    torch.manual_seed(123)
    queries = torch.randn(batch_size, num_heads, context_len, head_dim)
    keys = torch.randn(batch_size, num_heads, context_len, head_dim)

    # Apply rotary position embeddings
    queries_rot = nb1.compute_rope(queries, cos, sin)
    keys_rot = nb1.compute_rope(keys, cos, sin)

    # Generate reference RoPE via HF
    if version.parse(transformers_version) < version.parse("4.48"):
        rot_emb = LlamaRotaryEmbedding(
            dim=head_dim,
            max_position_embeddings=context_len,
            base=theta_base
        )
    else:
        class RoPEConfig:
            dim: int = head_dim
            rope_theta = theta_base
            max_position_embeddings: int = 8192
            hidden_size = head_dim * num_heads
            num_attention_heads = num_heads

        config = RoPEConfig()
        rot_emb = LlamaRotaryEmbedding(config=config)

    position_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)
    ref_cos, ref_sin = rot_emb(queries, position_ids)
    ref_queries_rot, ref_keys_rot = apply_rotary_pos_emb(queries, keys, ref_cos, ref_sin)

    torch.testing.assert_close(sin, ref_sin.squeeze(0))
    torch.testing.assert_close(cos, ref_cos.squeeze(0))
    torch.testing.assert_close(keys_rot, ref_keys_rot)
    torch.testing.assert_close(queries_rot, ref_queries_rot)

    # Generate reference RoPE via LitGPT
    litgpt_cos, litgpt_sin = litgpt_build_rope_cache(context_len, n_elem=head_dim, base=theta_base)
    litgpt_queries_rot = litgpt_apply_rope(queries, litgpt_cos, litgpt_sin)
    litgpt_keys_rot = litgpt_apply_rope(keys, litgpt_cos, litgpt_sin)

    torch.testing.assert_close(sin, litgpt_sin)
    torch.testing.assert_close(cos, litgpt_cos)
    torch.testing.assert_close(keys_rot, litgpt_keys_rot)
    torch.testing.assert_close(queries_rot, litgpt_queries_rot)


def test_rope_llama3_12(notebook):

    nb1 = notebook["converting-gpt-to-llama2"]
    nb2 = notebook["converting-llama2-to-llama3"]

    # Settings
    batch_size = 1
    context_len = 8192
    num_heads = 4
    head_dim = 16
    rope_theta = 500_000

    rope_config = {
        "factor": 8.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_context_length": 8192,
    }

    # Instantiate RoPE parameters
    cos, sin = nb2.precompute_rope_params(
        head_dim=head_dim,
        theta_base=rope_theta,
        context_length=context_len,
        freq_config=rope_config,
    )

    # Dummy query and key tensors
    torch.manual_seed(123)
    queries = torch.randn(batch_size, num_heads, context_len, head_dim)
    keys = torch.randn(batch_size, num_heads, context_len, head_dim)

    # Apply rotary position embeddings
    queries_rot = nb1.compute_rope(queries, cos, sin)
    keys_rot = nb1.compute_rope(keys, cos, sin)

    # Generate reference RoPE via HF
    hf_rope_params = {
        "factor": 8.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
    }

    class RoPEConfig:
        rope_type = "llama3"
        rope_scaling = hf_rope_params
        factor = 1.0
        dim: int = head_dim
        rope_theta = 500_000
        max_position_embeddings: int = 8192
        hidden_size = head_dim * num_heads
        num_attention_heads = num_heads

    config = RoPEConfig()

    rot_emb = LlamaRotaryEmbedding(config=config)
    position_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)
    ref_cos, ref_sin = rot_emb(queries, position_ids)
    ref_queries_rot, ref_keys_rot = apply_rotary_pos_emb(queries, keys, ref_cos, ref_sin)

    torch.testing.assert_close(sin, ref_sin.squeeze(0))
    torch.testing.assert_close(cos, ref_cos.squeeze(0))
    torch.testing.assert_close(keys_rot, ref_keys_rot)
    torch.testing.assert_close(queries_rot, ref_queries_rot)

    # Generate reference RoPE via LitGPT
    litgpt_rope_config = {
        "factor": 8.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_max_seq_len": 8192
    }

    litgpt_cos, litgpt_sin = litgpt_build_rope_cache(
        context_len,
        n_elem=head_dim,
        base=rope_theta,
        extra_config=litgpt_rope_config
    )
    litgpt_queries_rot = litgpt_apply_rope(queries, litgpt_cos, litgpt_sin)
    litgpt_keys_rot = litgpt_apply_rope(keys, litgpt_cos, litgpt_sin)

    torch.testing.assert_close(sin, litgpt_sin)
    torch.testing.assert_close(cos, litgpt_cos)
    torch.testing.assert_close(keys_rot, litgpt_keys_rot)
    torch.testing.assert_close(queries_rot, litgpt_queries_rot)


def test_silu(notebook):
    example_batch = torch.randn(2, 3, 4)
    silu = notebook["converting-gpt-to-llama2"].SiLU()
    assert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))


@pytest.mark.skipif(torch.__version__ < "2.4", reason="Requires PyTorch 2.4 or newer")
def test_rmsnorm(notebook):
    example_batch = torch.randn(2, 3, 4)
    rms_norm = notebook["converting-gpt-to-llama2"].RMSNorm(emb_dim=example_batch.shape[-1], eps=1e-5)
    rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5)

    assert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch))
</file>

<file path="ch05/07_gpt_to_llama/config.json">
{
    "HF_ACCESS_TOKEN": "hf-...",
    "_comment": "Enter your access token from https://huggingface.co/settings/tokens"
}
</file>

<file path="ch05/07_gpt_to_llama/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import torch


#####################################
# Chapter 5
#####################################
def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
</file>

<file path="ch05/07_gpt_to_llama/requirements-extra.txt">
blobfile>=3.0.0
huggingface_hub>=0.24.7
ipywidgets>=8.1.2
safetensors>=0.4.4
sentencepiece>=0.1.99
</file>

<file path="ch05/08_memory_efficient_weight_loading/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.


import torch
import torch.nn as nn

#####################################
# Chapter 3
#####################################


class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
</file>

<file path="ch05/09_extending-tokenizers/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination):
    # Send a GET request to download the file

    try:
        with urllib.request.urlopen(url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return

            # Define the block size for reading the file
            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(url)  # Extract filename from URL
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                # Open the destination file in binary write mode
                with open(destination, "wb") as file:
                    # Read the file in chunks and write to destination
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))  # Update progress bar
    except urllib.error.HTTPError:
        s = (
            f"The specified URL ({url}) is incorrect, the internet connection cannot be established,"
            "\nor the requested file is temporarily unavailable.\nPlease visit the following website"
            " for help: https://github.com/rasbt/LLMs-from-scratch/discussions/273")
        print(s)


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch05/09_extending-tokenizers/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-4.
# This file can be run as a standalone script.

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


if __name__ == "__main__":

    GPT_CONFIG_124M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "emb_dim": 768,          # Embedding dimension
        "n_heads": 12,           # Number of attention heads
        "n_layers": 12,          # Number of layers
        "drop_rate": 0.1,        # Dropout rate
        "qkv_bias": False        # Query-Key-Value bias
    }

    torch.manual_seed(123)
    model = GPTModel(GPT_CONFIG_124M)
    model.eval()  # disable dropout

    start_context = "Hello, I am"

    tokenizer = tiktoken.get_encoding("gpt2")
    encoded = tokenizer.encode(start_context)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)

    print(f"\n{50*'='}\n{22*' '}IN\n{50*'='}")
    print("\nInput text:", start_context)
    print("Encoded input text:", encoded)
    print("encoded_tensor.shape:", encoded_tensor.shape)

    out = generate_text_simple(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=10,
        context_size=GPT_CONFIG_124M["context_length"]
    )
    decoded_text = tokenizer.decode(out.squeeze(0).tolist())

    print(f"\n\n{50*'='}\n{22*' '}OUT\n{50*'='}")
    print("\nOutput:", out)
    print("Output length:", len(out[0]))
    print("Output text:", decoded_text)
</file>

<file path="ch06/01_main-chapter-code/gpt_class_finetune.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# This is a summary file containing the main takeaways from chapter 6.

import urllib.request
import zipfile
import os
from pathlib import Path
import time

import matplotlib.pyplot as plt
import pandas as pd
import tiktoken
import torch
from torch.utils.data import Dataset, DataLoader

from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt


def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path, test_mode=False):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download and extraction.")
        return

    if test_mode:  # Try multiple times since CI sometimes has connectivity issues
        max_retries = 5
        delay = 5  # delay between retries in seconds
        for attempt in range(max_retries):
            try:
                # Downloading the file
                with urllib.request.urlopen(url, timeout=10) as response:
                    with open(zip_path, "wb") as out_file:
                        out_file.write(response.read())
                break  # if download is successful, break out of the loop
            except urllib.error.URLError as e:
                print(f"Attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay)  # wait before retrying
                else:
                    print("Failed to download file after several attempts.")
                    return  # exit if all retries fail

    else:  # Code as it appears in the chapter
        # Downloading the file
        with urllib.request.urlopen(url) as response:
            with open(zip_path, "wb") as out_file:
                out_file.write(response.read())

    # Unzipping the file
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extracted_path)

    # Add .tsv file extension
    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)
    print(f"File downloaded and saved as {data_file_path}")


def create_balanced_dataset(df):
    # Count the instances of "spam"
    num_spam = df[df["Label"] == "spam"].shape[0]

    # Randomly sample "ham" instances to match the number of "spam" instances
    ham_subset = df[df["Label"] == "ham"].sample(num_spam, random_state=123)

    # Combine ham "subset" with "spam"
    balanced_df = pd.concat([ham_subset, df[df["Label"] == "spam"]])

    return balanced_df


def random_split(df, train_frac, validation_frac):
    # Shuffle the entire DataFrame
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Calculate split indices
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # Split the DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df


class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            # Truncate sequences if they are longer than max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]

        # Pad sequences to the longest sequence
        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = 0
        for encoded_text in self.encoded_texts:
            encoded_length = len(encoded_text)
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            with torch.no_grad():
                logits = model(input_batch)[:, -1, :]  # Logits of last output token
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # Logits of last output token
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen


def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_values, label=f"Training {label}")
    ax1.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Examples seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig(f"{label}-plot.pdf")
    # plt.show()


if __name__ == "__main__":

    import argparse

    parser = argparse.ArgumentParser(
        description="Finetune a GPT model for classification"
    )
    parser.add_argument(
        "--test_mode",
        default=False,
        action="store_true",
        help=("This flag runs the model in test mode for internal testing purposes. "
              "Otherwise, it runs the model as it is used in the chapter (recommended).")
    )
    args = parser.parse_args()

    ########################################
    # Download and prepare dataset
    ########################################

    url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
    zip_path = "sms_spam_collection.zip"
    extracted_path = "sms_spam_collection"
    data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path, test_mode=args.test_mode)
    df = pd.read_csv(data_file_path, sep="\t", header=None, names=["Label", "Text"])
    balanced_df = create_balanced_dataset(df)
    balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)
    train_df.to_csv("train.csv", index=None)
    validation_df.to_csv("validation.csv", index=None)
    test_df.to_csv("test.csv", index=None)

    ########################################
    # Create data loaders
    ########################################
    tokenizer = tiktoken.get_encoding("gpt2")

    train_dataset = SpamDataset(
        csv_file="train.csv",
        max_length=None,
        tokenizer=tokenizer
    )

    val_dataset = SpamDataset(
        csv_file="validation.csv",
        max_length=train_dataset.max_length,
        tokenizer=tokenizer
    )

    test_dataset = SpamDataset(
        csv_file="test.csv",
        max_length=train_dataset.max_length,
        tokenizer=tokenizer
    )

    num_workers = 0
    batch_size = 8

    torch.manual_seed(123)

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    ########################################
    # Load pretrained model
    ########################################

    # Small GPT model for testing purposes
    if args.test_mode:
        BASE_CONFIG = {
            "vocab_size": 50257,
            "context_length": 120,
            "drop_rate": 0.0,
            "qkv_bias": False,
            "emb_dim": 12,
            "n_layers": 1,
            "n_heads": 2
        }
        model = GPTModel(BASE_CONFIG)
        model.eval()
        device = "cpu"

    # Code as it is used in the main chapter
    else:
        CHOOSE_MODEL = "gpt2-small (124M)"
        INPUT_PROMPT = "Every effort moves"

        BASE_CONFIG = {
            "vocab_size": 50257,     # Vocabulary size
            "context_length": 1024,  # Context length
            "drop_rate": 0.0,        # Dropout rate
            "qkv_bias": True         # Query-key-value bias
        }

        model_configs = {
            "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
            "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
            "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
            "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
        }

        BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

        assert train_dataset.max_length <= BASE_CONFIG["context_length"], (
            f"Dataset length {train_dataset.max_length} exceeds model's context "
            f"length {BASE_CONFIG['context_length']}. Reinitialize data sets with "
            f"`max_length={BASE_CONFIG['context_length']}`"
        )

        model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
        settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

        model = GPTModel(BASE_CONFIG)
        load_weights_into_gpt(model, params)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ########################################
    # Modify and pretrained model
    ########################################

    for param in model.parameters():
        param.requires_grad = False

    torch.manual_seed(123)

    num_classes = 2
    model.out_head = torch.nn.Linear(in_features=BASE_CONFIG["emb_dim"], out_features=num_classes)
    model.to(device)

    for param in model.trf_blocks[-1].parameters():
        param.requires_grad = True

    for param in model.final_norm.parameters():
        param.requires_grad = True

    ########################################
    # Finetune modified model
    ########################################

    start_time = time.time()
    torch.manual_seed(123)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

    num_epochs = 5
    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=50, eval_iter=5,
        tokenizer=tokenizer
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    ########################################
    # Plot results
    ########################################

    # loss plot
    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))
    plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)

    # accuracy plot
    epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))
    plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label="accuracy")
</file>

<file path="ch06/01_main-chapter-code/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch06/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.
# This file can be run as a standalone script.

import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())
</file>

<file path="ch06/01_main-chapter-code/README.md">
# 第6章：分类任务微调

### 主章节代码

- [ch06.ipynb](ch06.ipynb) 包含了章节中出现的所有代码
- [previous_chapters.py](previous_chapters.py) 是一个Python模块，包含了我们在前几章中编码和训练的GPT模型，以及许多工具函数，我们将在本章中重复使用这些函数
- [gpt_download.py](gpt_download.py) 包含了用于下载预训练GPT模型权重的工具函数
- [exercise-solutions.ipynb](exercise-solutions.ipynb) 包含了本章的习题解答

### 可选代码

- [load-finetuned-model.ipynb](load-finetuned-model.ipynb) 是一个独立的Jupyter笔记本，用于加载我们在本章中创建的微调模型
- [gpt_class_finetune.py](gpt_class_finetune.py) 是一个独立的Python脚本文件，包含了我们在[ch06.ipynb](ch06.ipynb)中实现的微调GPT模型的代码（可以将其视为章节总结）
</file>

<file path="ch06/01_main-chapter-code/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)


import subprocess


def test_gpt_class_finetune():
    command = ["python", "ch06/01_main-chapter-code/gpt_class_finetune.py", "--test_mode"]

    result = subprocess.run(command, capture_output=True, text=True)
    assert result.returncode == 0, f"Script exited with errors: {result.stderr}"
</file>

<file path="ch06/02_bonus_additional-experiments/additional_experiments.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
import math
import os
from pathlib import Path
import time
import urllib.request
import zipfile

import pandas as pd
import tiktoken
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt


class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x


class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )

    def forward(self, x):
        return self.linear(x) + self.lora(x)


# This LoRA code is equivalent to LinearWithLoRA
class LinearWithLoRAMerged(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )

    def forward(self, x):
        lora = self.lora.A @ self.lora.B
        combined_weight = self.linear.weight + self.lora.alpha*lora.T
        return torch.nn.functional.linear(x, combined_weight, self.linear.bias)


class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256, no_padding=False):
        self.data = pd.read_csv(csv_file)
        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text)[:self.max_length]
            for text in self.data["Text"]
        ]

        if not no_padding:
            # Pad sequences to the longest sequence
            self.encoded_texts = [
                et + [pad_token_id] * (self.max_length - len(et))
                for et in self.encoded_texts
            ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self, tokenizer):
        max_length = 0
        for text in self.data["Text"]:
            encoded_length = len(tokenizer.encode(text))
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def download_and_unzip(url, zip_path, extract_to, new_file_path):
    if new_file_path.exists():
        print(f"{new_file_path} already exists. Skipping download and extraction.")
        return

    # Downloading the file
    with urllib.request.urlopen(url) as response:
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())

    # Unzipping the file
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_to)

    # Renaming the file to indicate its format
    original_file = Path(extract_to) / "SMSSpamCollection"
    os.rename(original_file, new_file_path)
    print(f"File downloaded and saved as {new_file_path}")


def random_split(df, train_frac, validation_frac):
    # Shuffle the entire DataFrame
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Calculate split indices
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # Split the DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df


def create_dataset_csvs(new_file_path):
    df = pd.read_csv(new_file_path, sep="\t", header=None, names=["Label", "Text"])

    # Create balanced dataset
    n_spam = df[df["Label"] == "spam"].shape[0]
    ham_sampled = df[df["Label"] == "ham"].sample(n_spam, random_state=123)
    balanced_df = pd.concat([ham_sampled, df[df["Label"] == "spam"]])
    balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)
    balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

    # Sample and save csv files
    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)
    train_df.to_csv("train.csv", index=None)
    validation_df.to_csv("validation.csv", index=None)
    test_df.to_csv("test.csv", index=None)


def instantiate_model(choose_model, load_weights):

    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    BASE_CONFIG.update(model_configs[choose_model])

    if not load_weights:
        torch.manual_seed(123)
    model = GPTModel(BASE_CONFIG, disable_causal_mask=args.disable_causal_mask)

    if load_weights:
        model_size = choose_model.split(" ")[-1].lstrip("(").rstrip(")")
        settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")
        load_weights_into_gpt(model, params)

    model.eval()
    return model


def calc_loss_batch(input_batch, target_batch, model, device,
                    trainable_token_pos=-1, ignore_index=-100, average_embeddings=False):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)

    if trainable_token_pos == "flexible":  # Selects the last tokens before the padding tokens
        # From https://github.com/rasbt/LLMs-from-scratch/discussions/434
        # Find the last non-padding token for each sequence in the batch
        pad_token_id = 50256  # <|endoftext|> token used for padding
        mask = input_batch != pad_token_id
        last_token_pos = mask.sum(dim=1) - 1  # Get position of last real token

        # Get model outputs
        logits = model(input_batch)  # shape: [batch_size, seq_len, num_classes]

        # Select the logits corresponding to the last real token of each sequence
        batch_size = logits.size(0)
        selected_logits = logits[torch.arange(batch_size), last_token_pos]

        loss = torch.nn.functional.cross_entropy(selected_logits, target_batch)
        return loss

    else:
        model_output = model(input_batch)
        if average_embeddings:
            # Average over the sequence dimension (dim=1)
            logits = model_output.mean(dim=1)
        else:
            # Select embeddings at the specified token position
            logits = model_output[:, trainable_token_pos, :]

        loss = torch.nn.functional.cross_entropy(logits, target_batch, ignore_index=ignore_index)
        return loss


def calc_loss_loader(data_loader, model, device,
                     num_batches=None, trainable_token_pos=-1,
                     ignore_index=-100, average_embeddings=False):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(
                input_batch, target_batch, model, device,
                trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,
                average_embeddings=average_embeddings
            )
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


@torch.no_grad()  # Disable gradient tracking for efficiency
def calc_accuracy_loader(data_loader, model, device, num_batches=None,
                         trainable_token_pos=-1, average_embeddings=False):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    if trainable_token_pos == "flexible":
        for i, (input_batch, target_batch) in enumerate(data_loader):
            if i < num_batches:
                input_batch, target_batch = input_batch.to(device), target_batch.to(device)

                # Find the last non-padding token for each sequence in the batch
                pad_token_id = 50256  # <|endoftext|> token used for padding
                mask = input_batch != pad_token_id
                last_token_pos = mask.sum(dim=1) - 1  # Get position of last real token

                with torch.no_grad():
                    logits = model(input_batch)  # Logits of last output token
                    # Select the logits corresponding to the last real token of each sequence
                    batch_size = logits.size(0)
                    selected_logits = logits[torch.arange(batch_size), last_token_pos]
                    predicted_labels = torch.argmax(selected_logits, dim=-1)

                num_examples += predicted_labels.shape[0]
                correct_predictions += (predicted_labels == target_batch).sum().item()
            else:
                break

    else:
        for i, (input_batch, target_batch) in enumerate(data_loader):
            if i < num_batches:
                input_batch, target_batch = input_batch.to(device), target_batch.to(device)

                model_output = model(input_batch)
                if average_embeddings:
                    # Average over the sequence dimension (dim=1)
                    logits = model_output.mean(dim=1)
                else:
                    # Select embeddings at the specified token position
                    logits = model_output[:, trainable_token_pos, :]

                predicted_labels = torch.argmax(logits, dim=-1)

                num_examples += predicted_labels.shape[0]
                correct_predictions += (predicted_labels == target_batch).sum().item()
            else:
                break
    return correct_predictions / num_examples


def evaluate_model(model, train_loader, val_loader, device,
                   eval_iter, trainable_token_pos=-1,
                   ignore_index=-100, average_embeddings=False):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,
            average_embeddings=average_embeddings
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,
            average_embeddings=average_embeddings
        )
    model.train()
    return train_loss, val_loss


def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter, max_steps=None, trainable_token_pos=-1,
                            accumulation_steps=1, ignore_index=-100, average_embeddings=False):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for batch_idx, (input_batch, target_batch) in enumerate(train_loader):
            loss = calc_loss_batch(
                input_batch, target_batch, model, device,
                trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,
                average_embeddings=average_embeddings
            )

            # Use gradient accumulation if accumulation_steps > 1
            # See https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html
            # for an explanation
            loss /= accumulation_steps

            loss.backward()  # Calculate loss gradients

            # Use gradient accumulation if accumulation_steps > 1
            is_update_step = ((batch_idx + 1) % accumulation_steps == 0) or ((batch_idx + 1) == len(train_loader))
            if is_update_step:
                optimizer.step()  # Update model weights using loss gradients
                optimizer.zero_grad()  # Reset loss gradients from previous batch iteration

            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter,
                    trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,
                    average_embeddings=average_embeddings
                )
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

            if max_steps is not None and global_step > max_steps:
                break

        # New: Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(
            train_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
        val_accuracy = calc_accuracy_loader(
            val_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

        if max_steps is not None and global_step > max_steps:
            break

    return train_losses, val_losses, train_accs, val_accs, examples_seen


def replace_linear_with_lora(model, rank, alpha, alternative=False):
    for name, module in model.named_children():
        if isinstance(module, torch.nn.Linear):
            # Replace the Linear layer with LinearWithLoRA
            if alternative:
                setattr(model, name, LinearWithLoRAMerged(module, rank, alpha))
            else:
                setattr(model, name, LinearWithLoRA(module, rank, alpha))
        else:
            # Recursively apply the same function to child modules
            replace_linear_with_lora(module, rank, alpha)


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_size",
        type=str,
        default="gpt2-small (124M)",
        help=(
            "Which GPT model to use. Options: 'gpt2-small (124M)', 'gpt2-medium (355M)',"
            " 'gpt2-large (774M)', 'gpt2-xl (1558M)'."
        )
    )
    parser.add_argument(
        "--weights",
        type=str,
        default="pretrained",
        help=(
            "Whether to use 'pretrained' or 'random' weights."
        )
    )
    parser.add_argument(
        "--trainable_layers",
        type=str,
        default="last_block",
        help=(
            "Which layers to train. Options: 'all', 'last_block', 'last_two_blocks', 'last_layer', 'lora', 'lora_alternative'."
        )
    )
    parser.add_argument(
        "--trainable_token_pos",
        type=str,
        default="last",
        help=(
            "Which token position to train. Options: 'first', 'last', 'flexible'."
        )
    )
    parser.add_argument(
        "--average_embeddings",
        action='store_true',
        default=False,
        help=(
            "Average the output embeddings from all tokens instead of using"
            " only the embedding at the token position specified by `--trainable_token_pos`."
        )
    )
    parser.add_argument(
        "--context_length",
        type=str,
        default="longest_training_example",
        help=(
            "The context length of the data inputs."
            " Options: 'longest_training_example', 'model_context_length' or integer value."
        )
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=8,
        help=(
            "The LoRA rank when choosing `--trainable_layers lora`"
        )
    )
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=8,
        help=(
            "The LoRA alpha value when choosing `--trainable_layers lora`"
        )
    )
    parser.add_argument(
        "--no_padding",
        action='store_true',
        default=False,
        help=(
            "Disable padding, which means each example may have a different length."
            " This requires setting `--batch_size 1`."
        )
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=5,
        help=(
            "Number of training epochs."
        )
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=8,
        help=(
            "The batch size used for training."
        )
    )
    parser.add_argument(
        "--accumulation_steps",
        type=int,
        default=1,
        help=(
            "Accumulation steps to allow for gradient accumulation."
            " See https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html for explanation."
            " For example, setting `batch_size=8` and `accumulation_steps=1` compute the exact same"
            " loss and weight updates as setting `batch_size=1` and `accumulation_steps=8`, however,"
            " the latter setting uses more iterations."
        )
    )
    parser.add_argument(
        "--disable_causal_mask",
        action='store_true',
        default=False,
        help=(
            "Disables the causal attention mask."
        )
    )
    parser.add_argument(
        "--ignore_index",
        type=int,
        default=-100,
        help=(
            "Sets the `ignore_index` in the cross-entropy loss."
        )
    )

    args = parser.parse_args()

    if args.trainable_token_pos == "first":
        args.trainable_token_pos = 0
    elif args.trainable_token_pos == "last":
        args.trainable_token_pos = -1
    # The "flexible" setting selects the last tokens before the padding tokens
    # See https://github.com/rasbt/LLMs-from-scratch/discussions/434
    elif args.trainable_token_pos == "flexible":
        args.trainable_token_pos = "flexible"
    else:
        raise ValueError("Invalid --trainable_token_pos argument")

    ###############################
    # Load model
    ###############################

    if args.weights == "pretrained":
        load_weights = True
    elif args.weights == "random":
        load_weights = False
    else:
        raise ValueError("Invalid --weights argument.")

    model = instantiate_model(args.model_size, load_weights)
    for param in model.parameters():
        param.requires_grad = False

    if args.model_size == "gpt2-small (124M)":
        in_features = 768
    elif args.model_size == "gpt2-medium (355M)":
        in_features = 1024
    elif args.model_size == "gpt2-large (774M)":
        in_features = 1280
    elif args.model_size == "gpt2-xl (1558M)":
        in_features = 1600
    else:
        raise ValueError("Invalid --model_size argument")

    torch.manual_seed(123)
    model.out_head = torch.nn.Linear(in_features=in_features, out_features=2)

    if args.trainable_layers == "last_layer":
        pass
    elif args.trainable_layers == "last_block" or args.trainable_layers == "last_two_blocks":
        for param in model.trf_blocks[-1].parameters():
            param.requires_grad = True
        for param in model.final_norm.parameters():
            param.requires_grad = True
        if args.trainable_layers == "last_two_blocks":
            for param in model.trf_blocks[-2].parameters():
                param.requires_grad = True
    elif args.trainable_layers == "all":
        for param in model.parameters():
            param.requires_grad = True
    elif args.trainable_layers in ("lora", "lora_alternative"):
        if args.trainable_layers == "lora_alternative":
            alternative = True
        else:
            alternative = False
        replace_linear_with_lora(model, rank=args.lora_rank, alpha=args.lora_alpha, alternative=alternative)
    else:
        raise ValueError("Invalid --trainable_layers argument.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    ###############################
    # Instantiate dataloaders
    ###############################

    url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
    zip_path = "sms_spam_collection.zip"
    extract_to = "sms_spam_collection"
    new_file_path = Path(extract_to) / "SMSSpamCollection.tsv"

    base_path = Path(".")
    file_names = ["train.csv", "validation.csv", "test.csv"]
    all_exist = all((base_path / file_name).exists() for file_name in file_names)

    if not all_exist:
        download_and_unzip(url, zip_path, extract_to, new_file_path)
        create_dataset_csvs(new_file_path)

    tokenizer = tiktoken.get_encoding("gpt2")

    train_dataset = None

    if args.no_padding:
        max_length = None

    else:
        if args.context_length == "model_context_length":
            max_length = model.pos_emb.weight.shape[0]
        elif args.context_length == "longest_training_example":
            train_dataset = SpamDataset(base_path / "train.csv", max_length=None, tokenizer=tokenizer, no_padding=args.no_padding)
            max_length = train_dataset.max_length
        else:
            try:
                max_length = int(args.context_length)
            except ValueError:
                raise ValueError("Invalid --context_length argument")

    if train_dataset is None:
        train_dataset = SpamDataset(base_path / "train.csv", max_length=max_length, tokenizer=tokenizer, no_padding=args.no_padding)
    val_dataset = SpamDataset(base_path / "validation.csv", max_length=max_length, tokenizer=tokenizer, no_padding=args.no_padding)
    test_dataset = SpamDataset(base_path / "test.csv", max_length=max_length, tokenizer=tokenizer, no_padding=args.no_padding)

    tokenizer = tiktoken.get_encoding("gpt2")

    num_workers = 0

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=args.batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=args.batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    assert train_dataset.max_length <= model.pos_emb.weight.shape[0], (
        f"Dataset length {train_dataset.max_length} exceeds model's context "
        f"length {model.pos_emb.weight.shape[0]}. Reinitialize data sets with "
        f"`max_length={model.pos_emb.weight.shape[0]}`"
    )

    ###############################
    # Train model
    ###############################

    start_time = time.time()
    torch.manual_seed(123)
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=args.num_epochs, eval_freq=50, eval_iter=5,
        max_steps=None, trainable_token_pos=args.trainable_token_pos,
        accumulation_steps=args.accumulation_steps, average_embeddings=args.average_embeddings
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    ###############################
    # Evaluate model
    ###############################

    train_accuracy = calc_accuracy_loader(
        train_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )
    val_accuracy = calc_accuracy_loader(
        val_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )
    test_accuracy = calc_accuracy_loader(
        test_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )

    print(f"Training accuracy: {train_accuracy*100:.2f}%")
    print(f"Validation accuracy: {val_accuracy*100:.2f}%")
    print(f"Test accuracy: {test_accuracy*100:.2f}%")
</file>

<file path="ch06/02_bonus_additional-experiments/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch06/02_bonus_additional-experiments/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.
# This file can be run as a standalone script.

import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, disable_causal_mask=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)

        if not disable_causal_mask:
            self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))
        self.disable_causal_mask = disable_causal_mask

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        if not self.disable_causal_mask:
            # Original mask truncated to the number of tokens and converted to boolean
            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

            # Use the mask to fill attention scores
            attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg, disable_causal_mask=False):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"],
            disable_causal_mask=disable_causal_mask
        )
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg, disable_causal_mask=False):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg, disable_causal_mask) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):
    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
</file>

<file path="ch06/02_bonus_additional-experiments/README.md">
# 附加分类微调实验

下表添加了实验以回答有关各种设计选择的其他问题。第一行使用与主要章节相同的设置，并用作参考。
例如，

- 比较第 1 行和第 2 行回答了以下问题：“当我们训练最后一个或第一个 token 时，性能有什么不同？”；
- 比较第 1 行和第 3 行回答了以下问题：“当我们只训练最后一层而不是最后一个块时，性能有什么不同？”；
- 等等。

&nbsp;

| | 模型 | 权重 | 可训练 token 位置 | 可训练层 | 上下文长度 | 训练 acc | 验证 acc | 测试 acc | 训练时间 | CPU/GPU |
| ---- | ------------------ | ---------- | ------------------------ | ---------------- | ------------------------------------------------------ | ------------ | -------------- | -------- | ------------- | ------- |
| 1 | gpt2-small (124M) | pretrained | last | last_block | longest train ex. （120）| 96.63% | 99.33% | 95.00% | 0.28 分钟 | A100 |
| 2 | gpt2-small（124M）| 预训练 | first | last_block | 最长的训练示例。（120）| 78.46% | 80.54% | 75.00% | 0.28 分钟 | A100 |
| 3 | gpt2-small（124M）| 预训练 | last | last_layer | 最长的训练示例。（120）| 78.65% | 79.87% | 72.00% | 0.25 分钟 | A100 |
| 4 | gpt2-small（124M）| 预训练 | last | last_two_blocks | 最长的训练示例。（120）| 98.85% | 98.66% 98.33% | 0.33 分钟 | A100 |
| 5 | gpt2-small (124M) | 预训练 | 最后 | 全部 | 最长训练示例 (120) | 99.62% | 96.64% | 96.67% | 0.69 分钟 | A100 |
| 6 | gpt2-medium (355M) | 预训练 | 最后 | last_block | 最长训练示例 (120) | 87.50% | 91.28% | 84.67% | 0.75 分钟 | A100 |
| 7 | gpt2-large (774M) | 预训练 | 最后 | last_block | 最长训练示例 (120) | 99.52% | 98.66% | 96.67% | 1.50 分钟 | A100 |
| 8 | gpt2-xl (1558M) | 预训练 | 最后 | last_block | 最长的训练示例 (120) | 99.81% | 99.81% | 98.33% | 2.83 分钟 | A100 |
| 9 | gpt2-xl (1558M) | 预训练 | 最后 | 全部 | 最长的训练示例 (120) | 100.00% | 98.66% | 98.67% | 8.12 分钟 | A100 |
| 10 | gpt2-small (124M) | 随机 | 最后 | 全部 | 最长的训练示例 (120) | 100.00% | 96.64% | 93.67% | 0.69 分钟 | A100 |
| 11 | gpt2-small (124M) | 预训练 | 最后 | LoRA | 最长的训练示例（120）| 100.00% | 97.32% | 96.67% | 0.75 分钟 | A100 |
| 12 | gpt2-xl（1558M）| 预训练 | 最后 | LoRA | 最长训练示例。（120）| 100.00% | 98.66% | 98.33% | 5.79 分钟 | A100 |
| 13 | gpt2-small（124M）| 预训练 | 最后 | last_block | 上下文长度（1024）| 83.08% | 87.92% | 78.33% | 2.46 分钟 | A100 |
| 14 | gpt2-small（124M）| 预训练 | 最后 | last_block | 变量：无填充（批量大小 1）| 100.00% | 98.66% | 98.00% | 1.75 分钟 | A100 |
| 15 | gpt2-small (124M) | 预训练 | last | last_block | 变量：无填充（批量大小 8）| 99.33% | 98.66% | 98.33% | 1.70 分钟 | A100 |
| 16 | gpt2-small (124M) | 预训练 | last | last_block | 灵活（最后一个非填充位置）| 99.42% | 98.66% | 98.33% | 0.30 分钟 | A100 |
| 17 | gpt2-small (124M) | 预训练 | last | last_block | 最长的训练示例（120）；但没有因果掩码 | 99.23% | 98.66% | 95.33% | 0.29 分钟 | A100 |
| 18 | gpt2-small (124M) | 预训练 | last | last_block | 最长的训练示例。 (120) 和 `ignore_index` 用于填充 | 96.63% | 99.33% | 95.00% | 0.28 分钟 | A100 |
| 19 | gpt2-small (124M) | 预训练 | last + 池化嵌入 | last_block | 最长的训练示例。 (120) | 97.79% |99.33% | 96.33% | 0.32 分钟 | A100 |

&nbsp;

### 用法

您可以使用以下代码重现实验：

- 第 1 行：`python additional_experiments.py`
- 第 2 行：`python additional_experiments.py --trainable_token_pos first`
- 第 3 行：`python additional_experiments.py --trainable_layers last_layer`
- 第 4 行：`python additional_experiments.py --trainable_layers last_two_blocks`
- 第 5 行：`python additional_experiments.py --trainable_layers all`
- 第 6 行：`python additional_experiments.py --model_size "gpt2-medium (355M)"`
- 第 7 行：`python additional_experiments.py --model_size "gpt2-large (774M)"`
- 第 8 行：`python additional_experiments.py --model_size "gpt2-xl (1558M)"`
- 第 9 行：`python additional_experiments.py --model_size "gpt2-xl (1558M)"--trainable_layers all`
- 第 10 行：`python additional_experiments.py --weights random --trainable_layers all`
- 第 11 行：`python additional_experiments.py --trainable_layers lora --lora_rank 16 --lora_alpha 16`
- 第 12 行：`python additional_experiments.py --trainable_layers lora --lora_rank 16 --lora_alpha 8 --model_size "gpt2-xl (1558M)"`
- 第 13 行：`python additional_experiments.py --context_length "model_context_length"`
- 第 14 行：`python additional_experiments.py --no_padding --batch_size 1`
- 第 15 行：`python additional_experiments.py --no_padding --batch_size 1 --accumulation_steps 8`
- 第 16 行：`python additional_experiments.py --trainable_token_pos "flexible"`
- 第 17 行：`python additional_experiments.py --disable_causal_mask`
- 第 18 行：`python additional_experiments.py --ignore_index 50256`
- 第 19 行：`python additional_experiments.py --average embeddings`

我故意将 LLM 和数据集保持在较小规模，这样如果您无法使用 GPU，您可以在 MacBook Air M3 等普通笔记本电脑上大约 15 分钟（默认设置）运行训练。

&nbsp;

### 解释

1. **训练最后一个与第一个输出标记位置（第 1 行与第 2 行）**：训练最后一个输出标记位置与第一个相比，性能明显更好。由于因果自注意力掩码，这种改进是可以预期的。
2. **训练最后一个 Transformer 块与最后一层（第 1 行与第 3 行）**：训练整个最后一个 Transformer 块也比仅训练最后一层产生更好的结果。
3. **训练最后一个 Transformer 块与最后两个 Transformer 块（第 1 行与第 4 行）**：训练最后两个 Transformer 块而不是仅训练最后一个 Transformer 块可使准确率明显提高 3.33%。
4. **训练最后一个 Transformer 块与所有层（第 1 行与第 5 行）**：训练所有层比仅训练最后一个 Transformer 块显示出约 2% 的适度改进，但训练时间几乎是后者的三倍。此外，它的表现不如仅训练 12 个 Transformer 块中的最后两个。
5. **使用更大的预训练模型（第 1 行与第 6 行，以及第 1 行与第 7 行和第 8 行）**：使用 3 倍大的预训练模型会导致更糟糕的结果。但是，与预期相比，使用 5 倍大的模型可以提高与初始模型相比的性能。同样，12 倍大的模型可以进一步提高预测性能。（中等模型可能未经过很好的预训练，或者特定的微调配置对该模型不太适用。）
6. **使用具有随机权重的模型与预训练权重的模型（第 1 行和第 5 行与第 10 行）**：与使用预训练权重相比，使用具有随机权重的模型产生的结果仅略差（分别差 3% 和 1.3%）。
7. **使用 LoRA（低秩自适应）与训练所有层（第 11 行与第 5 行，第 12 行与第 9 行）**：保持模型冻结并添加可训练的 LoRA 层（有关详细信息，请参阅 [附录 E](../../appendix-E/01_main-chapter-code/appendix-E.ipynb)）是训练所有模型参数的可行替代方案，甚至可以将性能提高 1%（第 11 行与第 5 行）。从使用 LoRA 时训练和验证准确率之间的差距缩小约 1% 可以看出，这可能是由于过度拟合较少。此外，使用 LoRA 也更节省内存，因为需要更新的参数更少。在训练较大的模型（第 12 行与第 9 行）时，我们还可以看到 LoRA 训练速度要快得多（5.79 分钟而不是 8.12 分钟）。
8. **将输入填充到完整上下文长度与最长训练示例（第 1 行与第 13 行）**：将输入填充到完整支持的上下文长度结果明显更差。
9. **填充与无填充（第 1 行与第 14 行、第 15 行和第 16 行）**：`--no_padding` 选项禁用数据集中的填充，这需要使用 1 的批处理大小训练模型，因为输入的长度可变。这可以提高测试准确率，但需要更长的时间来训练。在第 15 行中，我们另外启用了 8 步梯度累积，以实现与其他实验相同的批处理大小，这有助于减少过度拟合并略微提高测试集准确率。在第 16 行中，应用了填充，但根据最后一个标记位置选择标记位置非填充标记。第 16 行在数学上应与使用梯度累积的第 15 行相似。但是，由于在标记计数不相等的情况下梯度累积存在一些挑战，因此可能会存在微小差异（这在 [这篇](https://unsloth.ai/blog/gradient) 博客文章中进行了讨论）。
10. **禁用因果注意掩码（第 1 行与第 17 行）**：禁用多头注意模块中使用的因果注意掩码。这意味着所有标记都可以关注所有其他标记。与使用因果掩码的 GPT 模型相比，模型准确性略有提高。
11. **忽略损失和反向传播中的填充索引（第 1 行与第 18 行）**：设置 `--ignore_index 50256` 会排除 PyTorch 中 `cross_entropy` 损失函数中的 `|endoftext|` 填充标记。在这种情况下，它没有任何效果，因为我们替换了输出层，以便二进制分类示例的标记 ID 为 0 或 1。但是，此设置在第 7 章中指导微调模型时很有用。
13. **对所有标记的嵌入取平均值（第 1 行与第 19 行）**：设置 `--average_embeddings` 将对所有标记的嵌入取平均值。如果不使用此选项（默认），则仅考虑所选标记位置（由 `--trainable_token_pos` 指定）的输出嵌入；例如，最后一个标记的嵌入。启用 `--average_embeddings` 将把所有标记的嵌入均值池化到 `--trainable_token_pos` 选择的位置（默认为最后一个标记）。我们可以看到，这将性能从 95.00％ 提高到了 96.33％，而运行时间仅略微增加（0.28 分钟到 0.32 分钟），在实践中值得考虑。
</file>

<file path="ch06/03_bonus_imdb-classification/download_prepare_dataset.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import os
import sys
import tarfile
import time
import urllib.request
import pandas as pd


def reporthook(count, block_size, total_size):
    global start_time
    if count == 0:
        start_time = time.time()
    else:
        duration = time.time() - start_time
        progress_size = int(count * block_size)
        percent = count * block_size * 100 / total_size

        speed = int(progress_size / (1024 * duration)) if duration else 0
        sys.stdout.write(
            f"\r{int(percent)}% | {progress_size / (1024**2):.2f} MB "
            f"| {speed:.2f} MB/s | {duration:.2f} sec elapsed"
        )
        sys.stdout.flush()


def download_and_extract_dataset(dataset_url, target_file, directory):
    if not os.path.exists(directory):
        if os.path.exists(target_file):
            os.remove(target_file)
        urllib.request.urlretrieve(dataset_url, target_file, reporthook)
        print("\nExtracting dataset ...")
        with tarfile.open(target_file, "r:gz") as tar:
            tar.extractall()
    else:
        print(f"Directory `{directory}` already exists. Skipping download.")


def load_dataset_to_dataframe(basepath="aclImdb", labels={"pos": 1, "neg": 0}):
    data_frames = []  # List to store each chunk of DataFrame
    for subset in ("test", "train"):
        for label in ("pos", "neg"):
            path = os.path.join(basepath, subset, label)
            for file in sorted(os.listdir(path)):
                with open(os.path.join(path, file), "r", encoding="utf-8") as infile:
                    # Create a DataFrame for each file and add it to the list
                    data_frames.append(pd.DataFrame({"text": [infile.read()], "label": [labels[label]]}))
    # Concatenate all DataFrame chunks together
    df = pd.concat(data_frames, ignore_index=True)
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)  # Shuffle the DataFrame
    return df


def partition_and_save(df, sizes=(35000, 5000, 10000)):
    # Shuffle the DataFrame
    df_shuffled = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Get indices for where to split the data
    train_end = sizes[0]
    val_end = sizes[0] + sizes[1]

    # Split the DataFrame
    train = df_shuffled.iloc[:train_end]
    val = df_shuffled.iloc[train_end:val_end]
    test = df_shuffled.iloc[val_end:]

    # Save to CSV files
    train.to_csv("train.csv", index=False)
    val.to_csv("validation.csv", index=False)
    test.to_csv("test.csv", index=False)


if __name__ == "__main__":
    dataset_url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    print("Downloading dataset ...")
    download_and_extract_dataset(dataset_url, "aclImdb_v1.tar.gz", "aclImdb")
    print("Creating data frames ...")
    df = load_dataset_to_dataframe()
    print("Partitioning and saving data frames ...")
    partition_and_save(df)
</file>

<file path="ch06/03_bonus_imdb-classification/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch06/03_bonus_imdb-classification/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.
# This file can be run as a standalone script.

import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())
</file>

<file path="ch06/03_bonus_imdb-classification/README.md">
# 对 50k IMDB 电影评论的情绪进行分类的附加实验

&nbsp;
## 步骤 1：安装依赖项

通过以下方式安装额外的依赖项

```bash
pip install -r requirements-extra.txt
```

&nbsp;
## 步骤 2：下载数据集

代码使用来自 IMDb 的 50k 电影评论（[数据集来源](https://ai.stanford.edu/~amaas/data/sentiment/)) 来预测电影评论是正面的还是负面的。

运行以下代码以创建 `train.csv`、`validation.csv` 和 `test.csv` 数据集：

```bash
python download_prepare_dataset.py
```

&nbsp;
## 步骤 3：运行模型

主要章节中使用的 124M GPT-2 模型，从预训练权重开始，并对所有权重进行微调：

```bash
python train_gpt.py --trainable_layers "all" --num_epochs 1
```

```
Ep 1（步骤 000000）：训练损失 3.706，Val 损失 3.853
Ep 1（步骤 000050）：训练损失 0.682，Val 损失 0.706
...
Ep 1（步骤 004300）：训练损失 0.199，Val 损失 0.285
Ep 1（步骤 004350）：训练损失 0.188，Val 损失 0.208
训练准确率：95.62% |验证准确率：95.00%
训练用时 9.48 分钟。

在完整数据集上进行评估...

训练准确率：95.64%
验证准确率：92.32%
测试准确率：91.88%
```

<br>

---

<br>

340M 参数编码器样式 [BERT](https://arxiv.org/abs/1810.04805) 模型：

```bash
python train_bert_hf.py --trainable_layers "all" --num_epochs 1 --model "bert"
```

```
Ep 1 (步骤 000000)：训练损失 0.848，验证损失 0.775
Ep 1 (步骤 000050)：训练损失 0.655，验证损失 0.682
...
Ep 1 (步骤 004300)：训练损失 0.146，验证损失0.318
Ep 1（步骤 004350）：训练损失 0.204，验证损失 0.217
训练准确率：92.50% | 验证准确率：88.75%
训练在 7.65 分钟内完成。

在完整数据集上进行评估...

训练准确率：94.35%
验证准确率：90.74%
测试准确率：90.89%
```

<br>

---

<br>

66M 参数编码器样式 [DistilBERT](https://arxiv.org/abs/1910.01108) 模型（从 340M 参数 BERT 模型中提炼而来），从预训练权重开始，仅训练最后一个转换器块和输出层：

```bash
python train_bert_hf.py --trainable_layers "all" --num_epochs 1 --model "distilbert"
```

```
Ep 1（步骤 000000）：训练损失 0.693，验证损失 0.688
Ep 1（步骤 000050）：训练损失0.452，Val 损失 0.460
...
Ep 1（步骤 004300）：训练损失 0.179，Val 损失 0.272
Ep 1（步骤 004350）：训练损失 0.199，Val 损失 0.182
训练准确率：95.62% | 验证准确率：91.25%
训练在 4.26 分钟内完成。

在完整数据集上进行评估...

训练准确率：95.30%
验证准确率：91.12%
测试准确率：91.40%
```
<br>

---

<br>

355M 参数编码器样式 [RoBERTa](https://arxiv.org/abs/1907.11692) 模型，从预训练权重开始，仅训练最后一个转换器块和输出层：

```bash
python train_bert_hf.py --trainable_layers "last_block" --num_epochs 1 --model "roberta"
```

```
Ep 1 (步骤 000000)：训练损失 0.695，验证损失 0.698
Ep 1 (步骤 000050)：训练损失 0.670，验证损失 0.690
...
Ep 1 （步骤 004300）：训练损失 0.126，验证损失 0.149
Ep 1（步骤 004350）：训练损失 0.211，验证损失 0.138
训练准确率：92.50% | 验证准确率：94.38%
训练在 7.20 分钟内完成。

对完整数据集进行评估...

训练准确率：93.44%
验证准确率：93.02%
测试准确率：92.95%
```

<br>

---

<br>

以 scikit-learn 逻辑回归分类器作为基线：

```bash
python train_sklearn_logreg.py
```

```
虚拟分类器：
训练准确率：50.01%
验证准确率：50.14%
测试准确率：49.91%

逻辑回归分类器：
训练准确率：99.80%
验证准确率：88.62%
测试准确率：88.85%
```
</file>

<file path="ch06/03_bonus_imdb-classification/requirements-extra.txt">
transformers>=4.33.2
scikit-learn>=1.3.0
</file>

<file path="ch06/03_bonus_imdb-classification/train_bert_hf_spam.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
import os
from pathlib import Path
import time
import urllib
import zipfile

import pandas as pd
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

from transformers import AutoTokenizer, AutoModelForSequenceClassification


class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256, no_padding=False):
        self.data = pd.read_csv(csv_file)
        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text)[:self.max_length]
            for text in self.data["Text"]
        ]

        if not no_padding:
            # Pad sequences to the longest sequence
            self.encoded_texts = [
                et + [pad_token_id] * (self.max_length - len(et))
                for et in self.encoded_texts
            ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self, tokenizer):
        max_length = 0
        for text in self.data["Text"]:
            encoded_length = len(tokenizer.encode(text))
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def download_and_unzip(url, zip_path, extract_to, new_file_path):
    if new_file_path.exists():
        print(f"{new_file_path} already exists. Skipping download and extraction.")
        return

    # Downloading the file
    with urllib.request.urlopen(url) as response:
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())

    # Unzipping the file
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_to)

    # Renaming the file to indicate its format
    original_file = Path(extract_to) / "SMSSpamCollection"
    os.rename(original_file, new_file_path)
    print(f"File downloaded and saved as {new_file_path}")


def random_split(df, train_frac, validation_frac):
    # Shuffle the entire DataFrame
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Calculate split indices
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # Split the DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df


def create_dataset_csvs(new_file_path):
    df = pd.read_csv(new_file_path, sep="\t", header=None, names=["Label", "Text"])

    # Create balanced dataset
    n_spam = df[df["Label"] == "spam"].shape[0]
    ham_sampled = df[df["Label"] == "ham"].sample(n_spam, random_state=123)
    balanced_df = pd.concat([ham_sampled, df[df["Label"] == "spam"]])
    balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)
    balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

    # Sample and save csv files
    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)
    train_df.to_csv("train.csv", index=None)
    validation_df.to_csv("validation.csv", index=None)
    test_df.to_csv("test.csv", index=None)


class SPAMDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256, use_attention_mask=False):
        self.data = pd.read_csv(csv_file)
        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)
        self.pad_token_id = pad_token_id
        self.use_attention_mask = use_attention_mask

        # Pre-tokenize texts and create attention masks if required
        self.encoded_texts = [
            tokenizer.encode(text, truncation=True, max_length=self.max_length)
            for text in self.data["Text"]
        ]
        self.encoded_texts = [
            et + [pad_token_id] * (self.max_length - len(et))
            for et in self.encoded_texts
        ]

        if self.use_attention_mask:
            self.attention_masks = [
                self._create_attention_mask(et)
                for et in self.encoded_texts
            ]
        else:
            self.attention_masks = None

    def _create_attention_mask(self, encoded_text):
        return [1 if token_id != self.pad_token_id else 0 for token_id in encoded_text]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]

        if self.use_attention_mask:
            attention_mask = self.attention_masks[index]
        else:
            attention_mask = torch.ones(self.max_length, dtype=torch.long)

        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(attention_mask, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self, tokenizer):
        max_length = 0
        for text in self.data["Text"]:
            encoded_length = len(tokenizer.encode(text))
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device):
    attention_mask_batch = attention_mask_batch.to(device)
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    # logits = model(input_batch)[:, -1, :]  # Logits of last output token
    logits = model(input_batch, attention_mask=attention_mask_batch).logits
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss


# Same as in chapter 5
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


@torch.no_grad()  # Disable gradient tracking for efficiency
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            attention_mask_batch = attention_mask_batch.to(device)
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)
            # logits = model(input_batch)[:, -1, :]  # Logits of last output token
            logits = model(input_batch, attention_mask=attention_mask_batch).logits
            predicted_labels = torch.argmax(logits, dim=1)
            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter, max_steps=None):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, attention_mask_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

            if max_steps is not None and global_step > max_steps:
                break

        # New: Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

        if max_steps is not None and global_step > max_steps:
            break

    return train_losses, val_losses, train_accs, val_accs, examples_seen


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--trainable_layers",
        type=str,
        default="all",
        help=(
            "Which layers to train. Options: 'all', 'last_block', 'last_layer'."
        )
    )
    parser.add_argument(
        "--use_attention_mask",
        type=str,
        default="true",
        help=(
            "Whether to use a attention mask for padding tokens. Options: 'true', 'false'."
        )
    )
    parser.add_argument(
        "--model",
        type=str,
        default="distilbert",
        help=(
            "Which model to train. Options: 'distilbert', 'bert', 'roberta'."
        )
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=1,
        help=(
            "Number of epochs."
        )
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-6,
        help=(
            "Learning rate."
        )
    )
    args = parser.parse_args()

    ###############################
    # Load model
    ###############################

    torch.manual_seed(123)
    if args.model == "distilbert":

        model = AutoModelForSequenceClassification.from_pretrained(
            "distilbert-base-uncased", num_labels=2
        )
        model.out_head = torch.nn.Linear(in_features=768, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.out_head.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.pre_classifier.parameters():
                param.requires_grad = True
            for param in model.distilbert.transformer.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

    elif args.model == "bert":

        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased", num_labels=2
        )
        model.classifier = torch.nn.Linear(in_features=768, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.classifier.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.classifier.parameters():
                param.requires_grad = True
            for param in model.bert.pooler.dense.parameters():
                param.requires_grad = True
            for param in model.bert.encoder.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    elif args.model == "roberta":

        model = AutoModelForSequenceClassification.from_pretrained(
            "FacebookAI/roberta-large", num_labels=2
        )
        model.classifier.out_proj = torch.nn.Linear(in_features=1024, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.classifier.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.classifier.parameters():
                param.requires_grad = True
            for param in model.roberta.encoder.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-large")
    else:
        raise ValueError("Selected --model {args.model} not supported.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    ###############################
    # Instantiate dataloaders
    ###############################

    url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
    zip_path = "sms_spam_collection.zip"
    extract_to = "sms_spam_collection"
    new_file_path = Path(extract_to) / "SMSSpamCollection.tsv"

    base_path = Path(".")
    file_names = ["train.csv", "validation.csv", "test.csv"]
    all_exist = all((base_path / file_name).exists() for file_name in file_names)

    if not all_exist:
        download_and_unzip(url, zip_path, extract_to, new_file_path)
        create_dataset_csvs(new_file_path)

    if args.use_attention_mask.lower() == "true":
        use_attention_mask = True
    elif args.use_attention_mask.lower() == "false":
        use_attention_mask = False
    else:
        raise ValueError("Invalid argument for `use_attention_mask`.")

    train_dataset = SPAMDataset(
        base_path / "train.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )
    val_dataset = SPAMDataset(
        base_path / "validation.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )
    test_dataset = SPAMDataset(
        base_path / "test.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )

    num_workers = 0
    batch_size = 8

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    ###############################
    # Train model
    ###############################

    start_time = time.time()
    torch.manual_seed(123)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=0.1)

    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=args.num_epochs, eval_freq=50, eval_iter=20,
        max_steps=None
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    ###############################
    # Evaluate model
    ###############################

    print("\nEvaluating on the full datasets ...\n")

    train_accuracy = calc_accuracy_loader(train_loader, model, device)
    val_accuracy = calc_accuracy_loader(val_loader, model, device)
    test_accuracy = calc_accuracy_loader(test_loader, model, device)

    print(f"Training accuracy: {train_accuracy*100:.2f}%")
    print(f"Validation accuracy: {val_accuracy*100:.2f}%")
    print(f"Test accuracy: {test_accuracy*100:.2f}%")
</file>

<file path="ch06/03_bonus_imdb-classification/train_bert_hf.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
from pathlib import Path
import time

import pandas as pd
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

from transformers import AutoTokenizer, AutoModelForSequenceClassification


class IMDBDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256, use_attention_mask=False):
        self.data = pd.read_csv(csv_file)
        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)
        self.pad_token_id = pad_token_id
        self.use_attention_mask = use_attention_mask

        # Pre-tokenize texts and create attention masks if required
        self.encoded_texts = [
            tokenizer.encode(text, truncation=True, max_length=self.max_length)
            for text in self.data["text"]
        ]
        self.encoded_texts = [
            et + [pad_token_id] * (self.max_length - len(et))
            for et in self.encoded_texts
        ]

        if self.use_attention_mask:
            self.attention_masks = [
                self._create_attention_mask(et)
                for et in self.encoded_texts
            ]
        else:
            self.attention_masks = None

    def _create_attention_mask(self, encoded_text):
        return [1 if token_id != self.pad_token_id else 0 for token_id in encoded_text]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["label"]

        if self.use_attention_mask:
            attention_mask = self.attention_masks[index]
        else:
            attention_mask = torch.ones(self.max_length, dtype=torch.long)

        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(attention_mask, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self, tokenizer):
        max_length = 0
        for text in self.data["text"]:
            encoded_length = len(tokenizer.encode(text))
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device):
    attention_mask_batch = attention_mask_batch.to(device)
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    # logits = model(input_batch)[:, -1, :]  # Logits of last output token
    logits = model(input_batch, attention_mask=attention_mask_batch).logits
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss


# Same as in chapter 5
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


@torch.no_grad()  # Disable gradient tracking for efficiency
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            attention_mask_batch = attention_mask_batch.to(device)
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)
            # logits = model(input_batch)[:, -1, :]  # Logits of last output token
            logits = model(input_batch, attention_mask=attention_mask_batch).logits
            predicted_labels = torch.argmax(logits, dim=1)
            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter, max_steps=None):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, attention_mask_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

            if max_steps is not None and global_step > max_steps:
                break

        # New: Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

        if max_steps is not None and global_step > max_steps:
            break

    return train_losses, val_losses, train_accs, val_accs, examples_seen


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--trainable_layers",
        type=str,
        default="all",
        help=(
            "Which layers to train. Options: 'all', 'last_block', 'last_layer'."
        )
    )
    parser.add_argument(
        "--use_attention_mask",
        type=str,
        default="true",
        help=(
            "Whether to use a attention mask for padding tokens. Options: 'true', 'false'."
        )
    )
    parser.add_argument(
        "--model",
        type=str,
        default="distilbert",
        help=(
            "Which model to train. Options: 'distilbert', 'bert', 'roberta'."
        )
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=1,
        help=(
            "Number of epochs."
        )
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-6,
        help=(
            "Learning rate."
        )
    )
    args = parser.parse_args()

    ###############################
    # Load model
    ###############################

    torch.manual_seed(123)
    if args.model == "distilbert":

        model = AutoModelForSequenceClassification.from_pretrained(
            "distilbert-base-uncased", num_labels=2
        )
        model.out_head = torch.nn.Linear(in_features=768, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.out_head.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.pre_classifier.parameters():
                param.requires_grad = True
            for param in model.distilbert.transformer.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

    elif args.model == "bert":

        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased", num_labels=2
        )
        model.classifier = torch.nn.Linear(in_features=768, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.classifier.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.classifier.parameters():
                param.requires_grad = True
            for param in model.bert.pooler.dense.parameters():
                param.requires_grad = True
            for param in model.bert.encoder.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    elif args.model == "roberta":

        model = AutoModelForSequenceClassification.from_pretrained(
            "FacebookAI/roberta-large", num_labels=2
        )
        model.classifier.out_proj = torch.nn.Linear(in_features=1024, out_features=2)
        for param in model.parameters():
            param.requires_grad = False
        if args.trainable_layers == "last_layer":
            for param in model.classifier.parameters():
                param.requires_grad = True
        elif args.trainable_layers == "last_block":
            for param in model.classifier.parameters():
                param.requires_grad = True
            for param in model.roberta.encoder.layer[-1].parameters():
                param.requires_grad = True
        elif args.trainable_layers == "all":
            for param in model.parameters():
                param.requires_grad = True
        else:
            raise ValueError("Invalid --trainable_layers argument.")

        tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-large")
    else:
        raise ValueError("Selected --model {args.model} not supported.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    ###############################
    # Instantiate dataloaders
    ###############################

    base_path = Path(".")

    if args.use_attention_mask.lower() == "true":
        use_attention_mask = True
    elif args.use_attention_mask.lower() == "false":
        use_attention_mask = False
    else:
        raise ValueError("Invalid argument for `use_attention_mask`.")

    train_dataset = IMDBDataset(
        base_path / "train.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )
    val_dataset = IMDBDataset(
        base_path / "validation.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )
    test_dataset = IMDBDataset(
        base_path / "test.csv",
        max_length=256,
        tokenizer=tokenizer,
        pad_token_id=tokenizer.pad_token_id,
        use_attention_mask=use_attention_mask
    )

    num_workers = 0
    batch_size = 8

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    ###############################
    # Train model
    ###############################

    start_time = time.time()
    torch.manual_seed(123)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=0.1)

    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=args.num_epochs, eval_freq=50, eval_iter=20,
        max_steps=None
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    ###############################
    # Evaluate model
    ###############################

    print("\nEvaluating on the full datasets ...\n")

    train_accuracy = calc_accuracy_loader(train_loader, model, device)
    val_accuracy = calc_accuracy_loader(val_loader, model, device)
    test_accuracy = calc_accuracy_loader(test_loader, model, device)

    print(f"Training accuracy: {train_accuracy*100:.2f}%")
    print(f"Validation accuracy: {val_accuracy*100:.2f}%")
    print(f"Test accuracy: {test_accuracy*100:.2f}%")
</file>

<file path="ch06/03_bonus_imdb-classification/train_gpt.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
from pathlib import Path
import time

import pandas as pd
import tiktoken
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt


class IMDBDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)
        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text)[:self.max_length]
            for text in self.data["text"]
        ]
        # Pad sequences to the longest sequence
        self.encoded_texts = [
            et + [pad_token_id] * (self.max_length - len(et))
            for et in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["label"]
        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self, tokenizer):
        max_length = 0
        for text in self.data["text"]:
            encoded_length = len(tokenizer.encode(text))
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length


def instantiate_model(choose_model, load_weights):

    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    BASE_CONFIG.update(model_configs[choose_model])

    if not load_weights:
        torch.manual_seed(123)
    model = GPTModel(BASE_CONFIG)

    if load_weights:
        model_size = choose_model.split(" ")[-1].lstrip("(").rstrip(")")
        settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")
        load_weights_into_gpt(model, params)

    model.eval()
    return model


def calc_loss_batch(input_batch, target_batch, model, device,
                    trainable_token_pos=-1, average_embeddings=False):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)

    model_output = model(input_batch)
    if average_embeddings:
        # Average over the sequence dimension (dim=1)
        logits = model_output.mean(dim=1)
    else:
        # Select embeddings at the specified token position
        logits = model_output[:, trainable_token_pos, :]

    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss


def calc_loss_loader(data_loader, model, device,
                     num_batches=None, trainable_token_pos=-1,
                     average_embeddings=False):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(
                input_batch, target_batch, model, device,
                trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
            )
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


@torch.no_grad()  # Disable gradient tracking for efficiency
def calc_accuracy_loader(data_loader, model, device,
                         num_batches=None, trainable_token_pos=-1,
                         average_embeddings=False):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            model_output = model(input_batch)
            if average_embeddings:
                # Average over the sequence dimension (dim=1)
                logits = model_output.mean(dim=1)
            else:
                # Select embeddings at the specified token position
                logits = model_output[:, trainable_token_pos, :]

            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples


def evaluate_model(model, train_loader, val_loader, device, eval_iter,
                   trainable_token_pos=-1, average_embeddings=False):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
    model.train()
    return train_loss, val_loss


def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter, max_steps=None, trainable_token_pos=-1,
                            average_embeddings=False):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device,
                                   trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter,
                    trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
                )
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

            if max_steps is not None and global_step > max_steps:
                break

        # New: Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(
            train_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
        val_accuracy = calc_accuracy_loader(
            val_loader, model, device, num_batches=eval_iter,
            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings
        )
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

        if max_steps is not None and global_step > max_steps:
            break

    return train_losses, val_losses, train_accs, val_accs, examples_seen


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_size",
        type=str,
        default="gpt2-small (124M)",
        help=(
            "Which GPT model to use. Options: 'gpt2-small (124M)', 'gpt2-medium (355M)',"
            " 'gpt2-large (774M)', 'gpt2-xl (1558M)'."
        )
    )
    parser.add_argument(
        "--weights",
        type=str,
        default="pretrained",
        help=(
            "Whether to use 'pretrained' or 'random' weights."
        )
    )
    parser.add_argument(
        "--trainable_layers",
        type=str,
        default="last_block",
        help=(
            "Which layers to train. Options: 'all', 'last_block', 'last_layer'."
        )
    )
    parser.add_argument(
        "--trainable_token_pos",
        type=str,
        default="last",
        help=(
            "Which token to train. Options: 'first', 'last'."
        )
    )
    parser.add_argument(
        "--average_embeddings",
        action='store_true',
        default=False,
        help=(
            "Average the output embeddings from all tokens instead of using"
            " only the embedding at the token position specified by `--trainable_token_pos`."
        )
    )
    parser.add_argument(
        "--context_length",
        type=str,
        default="256",
        help=(
            "The context length of the data inputs."
            "Options: 'longest_training_example', 'model_context_length' or integer value."
        )
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=1,
        help=(
            "Number of epochs."
        )
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help=(
            "Learning rate."
        )
    )
    args = parser.parse_args()

    if args.trainable_token_pos == "first":
        args.trainable_token_pos = 0
    elif args.trainable_token_pos == "last":
        args.trainable_token_pos = -1
    else:
        raise ValueError("Invalid --trainable_token_pos argument")

    ###############################
    # Load model
    ###############################

    if args.weights == "pretrained":
        load_weights = True
    elif args.weights == "random":
        load_weights = False
    else:
        raise ValueError("Invalid --weights argument.")

    model = instantiate_model(args.model_size, load_weights)
    for param in model.parameters():
        param.requires_grad = False

    if args.model_size == "gpt2-small (124M)":
        in_features = 768
    elif args.model_size == "gpt2-medium (355M)":
        in_features = 1024
    elif args.model_size == "gpt2-large (774M)":
        in_features = 1280
    elif args.model_size == "gpt2-xl (1558M)":
        in_features = 1600
    else:
        raise ValueError("Invalid --model_size argument")

    torch.manual_seed(123)
    model.out_head = torch.nn.Linear(in_features=in_features, out_features=2)

    if args.trainable_layers == "last_layer":
        pass
    elif args.trainable_layers == "last_block":
        for param in model.trf_blocks[-1].parameters():
            param.requires_grad = True
        for param in model.final_norm.parameters():
            param.requires_grad = True
    elif args.trainable_layers == "all":
        for param in model.parameters():
            param.requires_grad = True
    else:
        raise ValueError("Invalid --trainable_layers argument.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    ###############################
    # Instantiate dataloaders
    ###############################

    base_path = Path(".")

    tokenizer = tiktoken.get_encoding("gpt2")

    train_dataset = None
    if args.context_length == "model_context_length":
        max_length = model.pos_emb.weight.shape[0]
    elif args.context_length == "longest_training_example":
        train_dataset = IMDBDataset(base_path / "train.csv", max_length=None, tokenizer=tokenizer)
        max_length = train_dataset.max_length
    else:
        try:
            max_length = int(args.context_length)
        except ValueError:
            raise ValueError("Invalid --context_length argument")

    if train_dataset is None:
        train_dataset = IMDBDataset(base_path / "train.csv", max_length=max_length, tokenizer=tokenizer)
    val_dataset = IMDBDataset(base_path / "validation.csv", max_length=max_length, tokenizer=tokenizer)
    test_dataset = IMDBDataset(base_path / "test.csv", max_length=max_length, tokenizer=tokenizer)

    num_workers = 0
    batch_size = 8

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=False,
    )

    ###############################
    # Train model
    ###############################

    start_time = time.time()
    torch.manual_seed(123)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=0.1)

    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=args.num_epochs, eval_freq=50, eval_iter=20,
        max_steps=None, trainable_token_pos=args.trainable_token_pos,
        average_embeddings=args.average_embeddings
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    ###############################
    # Evaluate model
    ###############################

    print("\nEvaluating on the full datasets ...\n")

    train_accuracy = calc_accuracy_loader(
        train_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )
    val_accuracy = calc_accuracy_loader(
        val_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )
    test_accuracy = calc_accuracy_loader(
        test_loader, model, device,
        trainable_token_pos=args.trainable_token_pos, average_embeddings=args.average_embeddings
    )

    print(f"Training accuracy: {train_accuracy*100:.2f}%")
    print(f"Validation accuracy: {val_accuracy*100:.2f}%")
    print(f"Test accuracy: {test_accuracy*100:.2f}%")
</file>

<file path="ch06/03_bonus_imdb-classification/train_sklearn_logreg.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# from sklearn.metrics import balanced_accuracy_score
from sklearn.dummy import DummyClassifier


def load_dataframes():
    df_train = pd.read_csv("train.csv")
    df_val = pd.read_csv("validation.csv")
    df_test = pd.read_csv("test.csv")

    return df_train, df_val, df_test


def eval(model, X_train, y_train, X_val, y_val, X_test, y_test):
    # Making predictions
    y_pred_train = model.predict(X_train)
    y_pred_val = model.predict(X_val)
    y_pred_test = model.predict(X_test)

    # Calculating accuracy and balanced accuracy
    accuracy_train = accuracy_score(y_train, y_pred_train)
    # balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)

    accuracy_val = accuracy_score(y_val, y_pred_val)
    # balanced_accuracy_val = balanced_accuracy_score(y_val, y_pred_val)

    accuracy_test = accuracy_score(y_test, y_pred_test)
    # balanced_accuracy_test = balanced_accuracy_score(y_test, y_pred_test)

    # Printing the results
    print(f"Training Accuracy: {accuracy_train*100:.2f}%")
    print(f"Validation Accuracy: {accuracy_val*100:.2f}%")
    print(f"Test Accuracy: {accuracy_test*100:.2f}%")

    # print(f"\nTraining Balanced Accuracy: {balanced_accuracy_train*100:.2f}%")
    # print(f"Validation Balanced Accuracy: {balanced_accuracy_val*100:.2f}%")
    # print(f"Test Balanced Accuracy: {balanced_accuracy_test*100:.2f}%")


if __name__ == "__main__":
    df_train, df_val, df_test = load_dataframes()

    #########################################
    # Convert text into bag-of-words model
    vectorizer = CountVectorizer()
    #########################################

    X_train = vectorizer.fit_transform(df_train["text"])
    X_val = vectorizer.transform(df_val["text"])
    X_test = vectorizer.transform(df_test["text"])
    y_train, y_val, y_test = df_train["label"], df_val["label"], df_test["label"]

    #####################################
    # Model training and evaluation
    #####################################

    # Create a dummy classifier with the strategy to predict the most frequent class
    dummy_clf = DummyClassifier(strategy="most_frequent")
    dummy_clf.fit(X_train, y_train)

    print("Dummy classifier:")
    eval(dummy_clf, X_train, y_train, X_val, y_val, X_test, y_test)

    print("\n\nLogistic regression classifier:")
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    eval(model, X_train, y_train, X_val, y_val, X_test, y_test)
</file>

<file path="ch06/04_user_interface/app.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

from pathlib import Path
import sys

import tiktoken
import torch
import chainlit

from previous_chapters import (
    classify_review,
    GPTModel
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def get_model_and_tokenizer():
    """
    Code to load finetuned GPT-2 model generated in chapter 6.
    This requires that you run the code in chapter 6 first, which generates the necessary model.pth file.
    """

    GPT_CONFIG_124M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "emb_dim": 768,          # Embedding dimension
        "n_heads": 12,           # Number of attention heads
        "n_layers": 12,          # Number of layers
        "drop_rate": 0.1,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    tokenizer = tiktoken.get_encoding("gpt2")

    model_path = Path("..") / "01_main-chapter-code" / "review_classifier.pth"
    if not model_path.exists():
        print(
            f"Could not find the {model_path} file. Please run the chapter 6 code"
            " (ch06.ipynb) to generate the review_classifier.pth file."
        )
        sys.exit()

    # Instantiate model
    model = GPTModel(GPT_CONFIG_124M)

    # Convert model to classifier as in section 6.5 in ch06.ipynb
    num_classes = 2
    model.out_head = torch.nn.Linear(in_features=GPT_CONFIG_124M["emb_dim"], out_features=num_classes)

    # Then load model weights
    checkpoint = torch.load(model_path, map_location=device, weights_only=True)
    model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()

    return tokenizer, model


# Obtain the necessary tokenizer and model files for the chainlit function below
tokenizer, model = get_model_and_tokenizer()


@chainlit.on_message
async def main(message: chainlit.Message):
    """
    The main Chainlit function.
    """
    user_input = message.content

    label = classify_review(user_input, model, tokenizer, device, max_length=120)

    await chainlit.Message(
        content=f"{label}",  # This returns the model response to the interface
    ).send()
</file>

<file path="ch06/04_user_interface/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.

import json
import os
import urllib

import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn
from tqdm import tqdm


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


#####################################
# Chapter 5
#####################################
def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination):
    # Send a GET request to download the file
    with urllib.request.urlopen(url) as response:
        # Get the total file size from headers, defaulting to 0 if not present
        file_size = int(response.headers.get("Content-Length", 0))

        # Check if file exists and has the same size
        if os.path.exists(destination):
            file_size_local = os.path.getsize(destination)
            if file_size == file_size_local:
                print(f"File already exists and is up-to-date: {destination}")
                return

        # Define the block size for reading the file
        block_size = 1024  # 1 Kilobyte

        # Initialize the progress bar with total file size
        progress_bar_description = os.path.basename(url)  # Extract filename from URL
        with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
            # Open the destination file in binary write mode
            with open(destination, "wb") as file:
                # Read the file in chunks and write to destination
                while True:
                    chunk = response.read(block_size)
                    if not chunk:
                        break
                    file.write(chunk)
                    progress_bar.update(len(chunk))  # Update progress bar


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


#####################################
# Chapter 6
#####################################
def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):
    model.eval()

    # Prepare inputs to the model
    input_ids = tokenizer.encode(text)
    supported_context_length = model.pos_emb.weight.shape[0]

    # Truncate sequences if they too long
    input_ids = input_ids[:min(max_length, supported_context_length)]

    # Pad sequences to the longest sequence
    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)  # add batch dimension

    # Model inference
    with torch.no_grad():
        logits = model(input_tensor.to(device))[:, -1, :]  # Logits of the last output token
    predicted_label = torch.argmax(logits, dim=-1).item()

    # Return the classified result
    return "spam" if predicted_label == 1 else "not spam"
</file>

<file path="ch06/04_user_interface/README.md">
# 构建用户界面与基于 GPT 的垃圾邮件分类器进行交互

该附加文件夹包含用于运行类似 ChatGPT 的用户界面，以便与第六章中微调的基于 GPT 的垃圾邮件分类器进行交互的代码，如下所示。

![Chainlit UI example](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/chainlit/chainlit-spam.webp)

为了实现这个用户界面，我们使用了开源的 [Chainlit Python 包](https://github.com/Chainlit/chainlit)。

&nbsp;
## 步骤 1：安装依赖

首先，通过以下命令安装 `chainlit` 包：

```bash
pip install chainlit
```
</file>

<file path="ch06/04_user_interface/requirements-extra.txt">
chainlit>=1.2.0
</file>

<file path="ch06/README.md">
# 第六章：分类任务的微调

&nbsp;
## 主章节代码

- [01_main-chapter-code](01_main-chapter-code) 包含了主章节的代码

&nbsp;
## 附加材料

- [02_bonus_additional-experiments](02_bonus_additional-experiments) 包含了额外的实验（例如，训练最后一个与第一个标记、扩展输入长度等）
- [03_bonus_imdb-classification](03_bonus_imdb-classification) 比较了第六章中的 LLM 与其他模型在 50k IMDB 电影评论情感分类数据集上的表现
- [04_user_interface](04_user_interface) 实现了一个互动用户界面，用于与预训练的 LLM 进行交互
</file>

<file path="ch07/01_main-chapter-code/exercise_experiments.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# Code to run the exercises; see exercise-solutions.ipynb for more information

from functools import partial
from importlib.metadata import version
import json
import math
import os
import re
import time
import urllib

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import tiktoken
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# Import from local files in this folder
from gpt_download import download_and_load_gpt2
from previous_chapters import (
    calc_loss_loader,
    generate,
    GPTModel,
    load_weights_into_gpt,
    text_to_token_ids,
    train_model_simple,
    token_ids_to_text
)


class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)


class InstructionDatasetWithMasking(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # New: Separate list for instruction lengths
        self.instruction_lengths = []
        self.encoded_texts = []

        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text

            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

            # New: collect instruction lengths
            instruction_length = len(tokenizer.encode(instruction_plus_input))
            self.instruction_lengths.append(instruction_length)

    def __getitem__(self, index):
        # New: return both instruction lengths and texts separately
        return self.instruction_lengths[index], self.encoded_texts[index]

    def __len__(self):
        return len(self.data)


class InstructionDatasetPhi(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:

            ###################################################################
            # NEW: Use `format_input_phi` and adjust the response text template
            instruction_plus_input = format_input_phi(entry)
            response_text = f"\n<|assistant|>:\n{entry['output']}"
            ###################################################################
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)


class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )

    def forward(self, x):
        return self.linear(x) + self.lora(x)


class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x


def replace_linear_with_lora(model, rank, alpha):
    for name, module in model.named_children():
        if isinstance(module, torch.nn.Linear):
            # Replace the Linear layer with LinearWithLoRA
            setattr(model, name, LinearWithLoRA(module, rank, alpha))
        else:
            # Recursively apply the same function to child modules
            replace_linear_with_lora(module, rank, alpha)


def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # New: Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor


def custom_collate_with_masking_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # New: batch is now a tuple

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for instruction_length, item in batch:  # New: batch is now a tuple
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Mask all input and instruction tokens in the targets
        targets[:instruction_length-1] = -100

        # Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor


def download_and_load_file(file_path, url):

    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()

    with open(file_path, "r") as file:
        data = json.load(file)

    return data


def format_input_phi(entry):
    instruction_text = (
        f"<|user|>\n{entry['instruction']}"
    )

    input_text = f"\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text


def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses, plot_name):
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    print(f"Plot saved as {plot_name}")
    plt.savefig(plot_name)
    # plt.show()


def main(mask_instructions=False, alpaca52k=False, phi3_prompt=False, lora=False):
    #######################################
    # Print package versions
    #######################################
    print()
    pkgs = [
        "matplotlib",  # Plotting library
        "tiktoken",    # Tokenizer
        "torch",       # Deep learning library
        "tqdm",        # Progress bar
        "tensorflow",  # For OpenAI's pretrained weights
    ]
    for p in pkgs:
        print(f"{p} version: {version(p)}")
    print(50*"-")

    #######################################
    # Download and prepare dataset
    #######################################
    file_path = "instruction-data.json"

    if alpaca52k:
        url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
    else:
        url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json"
    data = download_and_load_file(file_path, url)

    train_portion = int(len(data) * 0.85)  # 85% for training
    test_portion = int(len(data) * 0.1)    # 10% for testing

    train_data = data[:train_portion]
    test_data = data[train_portion:train_portion + test_portion]
    val_data = data[train_portion + test_portion:]

    print("Training set length:", len(train_data))
    print("Validation set length:", len(val_data))
    print("Test set length:", len(test_data))
    print(50*"-")

    tokenizer = tiktoken.get_encoding("gpt2")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)
    print(50*"-")

    if alpaca52k:
        allowed_max_length = 512
    else:
        allowed_max_length = 1024

    if mask_instructions and phi3_prompt:
        raise ValueError("Simultaneous support for instruction masking and the Phi-3 prompt template has not been implemented, yet.")

    if mask_instructions:
        customized_collate_fn = partial(custom_collate_with_masking_fn, device=device, allowed_max_length=allowed_max_length)
        CustomDataset = InstructionDatasetWithMasking
    elif phi3_prompt:
        customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=allowed_max_length)
        CustomDataset = InstructionDatasetPhi
    else:
        customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=allowed_max_length)
        CustomDataset = InstructionDataset

    num_workers = 0

    if alpaca52k:
        batch_size = 4
    else:
        batch_size = 8

    torch.manual_seed(123)

    train_dataset = CustomDataset(train_data, tokenizer)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=customized_collate_fn,
        shuffle=True,
        drop_last=True,
        num_workers=num_workers
    )

    val_dataset = CustomDataset(val_data, tokenizer)
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=customized_collate_fn,
        shuffle=False,
        drop_last=False,
        num_workers=num_workers
    )

    #######################################
    # Load pretrained model
    #######################################
    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    CHOOSE_MODEL = "gpt2-medium (355M)"

    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

    model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
    settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

    model = GPTModel(BASE_CONFIG)
    load_weights_into_gpt(model, params)
    model.eval()
    model.to(device)

    print("Loaded model:", CHOOSE_MODEL)
    print(50*"-")

    if lora:
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Total trainable parameters before: {total_params:,}")

        for param in model.parameters():
            param.requires_grad = False

        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Total trainable parameters after: {total_params:,}")
        replace_linear_with_lora(model, rank=16, alpha=16)

        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Total trainable LoRA parameters: {total_params:,}")
        model.to(device)

    #######################################
    # Finetuning the model
    #######################################
    print("Initial losses")
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)

    print("   Training loss:", train_loss)
    print("   Validation loss:", val_loss)

    start_time = time.time()

    num_epochs = 2
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)

    torch.manual_seed(123)

    start_context = format_input_phi(val_data[0]) if phi3_prompt else format_input(val_data[0])

    train_losses, val_losses, tokens_seen = train_model_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=5, eval_iter=5,
        start_context=start_context, tokenizer=tokenizer
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))

    plot_name = "loss-plot.pdf"
    if mask_instructions:
        plot_name = plot_name.replace(".pdf", "-mask-instructions.pdf")
    if alpaca52k:
        plot_name = plot_name.replace(".pdf", "-alpaca52k.pdf")
    if phi3_prompt:
        plot_name = plot_name.replace(".pdf", "-phi3-prompt.pdf")
    if lora:
        plot_name = plot_name.replace(".pdf", "-lora.pdf")
    if not any([mask_instructions, alpaca52k, phi3_prompt, lora]):
        plot_name = plot_name.replace(".pdf", "-baseline.pdf")

    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, plot_name)
    print(50*"-")

    #######################################
    # Saving results
    #######################################
    print("Generating responses")
    for i, entry in tqdm(enumerate(test_data), total=len(test_data)):

        input_text = format_input_phi(entry) if phi3_prompt else format_input(entry)

        token_ids = generate(
            model=model,
            idx=text_to_token_ids(input_text, tokenizer).to(device),
            max_new_tokens=256,
            context_size=BASE_CONFIG["context_length"],
            eos_id=50256
        )
        generated_text = token_ids_to_text(token_ids, tokenizer)

        if phi3_prompt:
            response_text = generated_text[len(input_text):].replace("<|assistant|>:", "").strip()
        else:
            response_text = generated_text[len(input_text):].replace("### Response:", "").strip()

        test_data[i]["model_response"] = response_text

    test_data_path = "instruction-data-with-response.json"
    file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth"

    if mask_instructions:
        test_data_path = test_data_path.replace(".json", "-mask-instructions.json")
        file_name = file_name.replace(".pth", "-mask-instructions.pth")
    if alpaca52k:
        test_data_path = test_data_path.replace(".json", "-alpaca52k.json")
        file_name = file_name.replace(".pth", "-alpaca52k.pth")
    if phi3_prompt:
        test_data_path = test_data_path.replace(".json", "-phi3-prompt.json")
        file_name = file_name.replace(".pth", "-phi3-prompt.pth")
    if lora:
        test_data_path = test_data_path.replace(".json", "-lora.json")
        file_name = file_name.replace(".pth", "-lora.pth")
    if not any([mask_instructions, alpaca52k, phi3_prompt, lora]):
        test_data_path = test_data_path.replace(".json", "-baseline.json")
        file_name = file_name.replace(".pth", "-baseline.pth")

    with open(test_data_path, "w") as file:
        json.dump(test_data, file, indent=4)  # "indent" for pretty-printing
    print(f"Responses saved as {test_data_path}")

    torch.save(model.state_dict(), file_name)
    print(f"Model saved as {file_name}")


if __name__ == "__main__":

    import argparse

    parser = argparse.ArgumentParser(
        description="Instruction finetune a GPT model"
    )
    options = {"baseline", "mask_instructions", "alpaca_52k", "phi3_prompt", "lora"}
    parser.add_argument(
        "--exercise_solution",
        type=str,
        default="last_block",
        help=(
            f"Which experiment to run. Options: {options}."
        )
    )
    args = parser.parse_args()

    if args.exercise_solution == "baseline":
        main()
    elif args.exercise_solution == "mask_instructions":
        main(mask_instructions=True)
    elif args.exercise_solution == "alpaca_52k":
        main(alpaca52k=True)
    elif args.exercise_solution == "phi3_prompt":
        main(phi3_prompt=True)
    elif args.exercise_solution == "lora":
        main(lora=True)
    else:
        raise ValueError(f"{args.exercise_solution} is not a valid --args.exercise_solution option. Options: {options}")
</file>

<file path="ch07/01_main-chapter-code/gpt_download.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch


import os
import urllib.request

# import requests
import json
import numpy as np
import tensorflow as tf
from tqdm import tqdm


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    backup_base_url = "https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        backup_url = os.path.join(backup_base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path, backup_url)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination, backup_url=None):
    def _attempt_download(download_url):
        with urllib.request.urlopen(download_url) as response:
            # Get the total file size from headers, defaulting to 0 if not present
            file_size = int(response.headers.get("Content-Length", 0))

            # Check if file exists and has the same size
            if os.path.exists(destination):
                file_size_local = os.path.getsize(destination)
                if file_size == file_size_local:
                    print(f"File already exists and is up-to-date: {destination}")
                    return True  # Indicate success without re-downloading

            block_size = 1024  # 1 Kilobyte

            # Initialize the progress bar with total file size
            progress_bar_description = os.path.basename(download_url)
            with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
                with open(destination, "wb") as file:
                    while True:
                        chunk = response.read(block_size)
                        if not chunk:
                            break
                        file.write(chunk)
                        progress_bar.update(len(chunk))
            return True

    try:
        if _attempt_download(url):
            return
    except (urllib.error.HTTPError, urllib.error.URLError):
        if backup_url is not None:
            print(f"Primary URL ({url}) failed. Attempting backup URL: {backup_url}")
            try:
                if _attempt_download(backup_url):
                    return
            except urllib.error.HTTPError:
                pass

        # If we reach here, both attempts have failed
        error_message = (
            f"Failed to download from both primary URL ({url})"
            f"{' and backup URL (' + backup_url + ')' if backup_url else ''}."
            "\nCheck your internet connection or the file availability.\n"
            "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273"
        )
        print(error_message)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# Alternative way using `requests`
"""
def download_file(url, destination):
    # Send a GET request to download the file in streaming mode
    response = requests.get(url, stream=True)

    # Get the total file size from headers, defaulting to 0 if not present
    file_size = int(response.headers.get("content-length", 0))

    # Check if file exists and has the same size
    if os.path.exists(destination):
        file_size_local = os.path.getsize(destination)
        if file_size == file_size_local:
            print(f"File already exists and is up-to-date: {destination}")
            return

    # Define the block size for reading the file
    block_size = 1024  # 1 Kilobyte

    # Initialize the progress bar with total file size
    progress_bar_description = url.split("/")[-1]  # Extract filename from URL
    with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
        # Open the destination file in binary write mode
        with open(destination, "wb") as file:
            # Iterate over the file data in chunks
            for chunk in response.iter_content(block_size):
                progress_bar.update(len(chunk))  # Update progress bar
                file.write(chunk)  # Write the chunk to the file
"""


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params
</file>

<file path="ch07/01_main-chapter-code/gpt_instruction_finetuning.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# A minimal instruction finetuning file based on the code in chapter 7

from functools import partial
from importlib.metadata import version
import json
import os
import re
import time
import urllib

import matplotlib.pyplot as plt
import tiktoken
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# Import from local files in this folder
from gpt_download import download_and_load_gpt2
from previous_chapters import (
    calc_loss_loader,
    generate,
    GPTModel,
    load_weights_into_gpt,
    text_to_token_ids,
    train_model_simple,
    token_ids_to_text
)


class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)


def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # New: Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor


def download_and_load_file(file_path, url):

    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()

    with open(file_path, "r") as file:
        data = json.load(file)

    return data


def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    plot_name = "loss-plot-standalone.pdf"
    print(f"Plot saved as {plot_name}")
    plt.savefig(plot_name)
    # plt.show()


def main(test_mode=False):
    #######################################
    # Print package versions
    #######################################
    print()
    pkgs = [
        "matplotlib",  # Plotting library
        "tiktoken",    # Tokenizer
        "torch",       # Deep learning library
        "tqdm",        # Progress bar
        "tensorflow",  # For OpenAI's pretrained weights
    ]
    for p in pkgs:
        print(f"{p} version: {version(p)}")
    print(50*"-")

    #######################################
    # Download and prepare dataset
    #######################################
    file_path = "instruction-data.json"
    url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json"
    data = download_and_load_file(file_path, url)

    train_portion = int(len(data) * 0.85)  # 85% for training
    test_portion = int(len(data) * 0.1)    # 10% for testing

    train_data = data[:train_portion]
    test_data = data[train_portion:train_portion + test_portion]
    val_data = data[train_portion + test_portion:]

    # Use very small subset for testing purposes
    if args.test_mode:
        train_data = train_data[:10]
        val_data = val_data[:10]
        test_data = test_data[:10]

    print("Training set length:", len(train_data))
    print("Validation set length:", len(val_data))
    print("Test set length:", len(test_data))
    print(50*"-")

    tokenizer = tiktoken.get_encoding("gpt2")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)
    print(50*"-")

    customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)

    num_workers = 0
    batch_size = 8

    torch.manual_seed(123)

    train_dataset = InstructionDataset(train_data, tokenizer)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=customized_collate_fn,
        shuffle=True,
        drop_last=True,
        num_workers=num_workers
    )

    val_dataset = InstructionDataset(val_data, tokenizer)
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=customized_collate_fn,
        shuffle=False,
        drop_last=False,
        num_workers=num_workers
    )

    #######################################
    # Load pretrained model
    #######################################

    # Small GPT model for testing purposes
    if args.test_mode:
        BASE_CONFIG = {
            "vocab_size": 50257,
            "context_length": 120,
            "drop_rate": 0.0,
            "qkv_bias": False,
            "emb_dim": 12,
            "n_layers": 1,
            "n_heads": 2
        }
        model = GPTModel(BASE_CONFIG)
        model.eval()
        device = "cpu"
        CHOOSE_MODEL = "Small test model"

    # Code as it is used in the main chapter
    else:
        BASE_CONFIG = {
            "vocab_size": 50257,     # Vocabulary size
            "context_length": 1024,  # Context length
            "drop_rate": 0.0,        # Dropout rate
            "qkv_bias": True         # Query-key-value bias
        }

        model_configs = {
            "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
            "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
            "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
            "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
        }

        CHOOSE_MODEL = "gpt2-medium (355M)"

        BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

        model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
        settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

        model = GPTModel(BASE_CONFIG)
        load_weights_into_gpt(model, params)
        model.eval()
        model.to(device)

    print("Loaded model:", CHOOSE_MODEL)
    print(50*"-")

    #######################################
    # Finetuning the model
    #######################################
    print("Initial losses")
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)

    print("   Training loss:", train_loss)
    print("   Validation loss:", val_loss)

    start_time = time.time()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)

    num_epochs = 2

    torch.manual_seed(123)
    train_losses, val_losses, tokens_seen = train_model_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=5, eval_iter=5,
        start_context=format_input(val_data[0]), tokenizer=tokenizer
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
    print(50*"-")

    #######################################
    # Saving results
    #######################################
    print("Generating responses")
    for i, entry in tqdm(enumerate(test_data), total=len(test_data)):

        input_text = format_input(entry)

        token_ids = generate(
            model=model,
            idx=text_to_token_ids(input_text, tokenizer).to(device),
            max_new_tokens=256,
            context_size=BASE_CONFIG["context_length"],
            eos_id=50256
        )
        generated_text = token_ids_to_text(token_ids, tokenizer)
        response_text = generated_text[len(input_text):].replace("### Response:", "").strip()

        test_data[i]["model_response"] = response_text

    test_data_path = "instruction-data-with-response-standalone.json"
    with open(test_data_path, "w") as file:
        json.dump(test_data, file, indent=4)  # "indent" for pretty-printing
    print(f"Responses saved as {test_data_path}")

    file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft-standalone.pth"
    torch.save(model.state_dict(), file_name)
    print(f"Model saved as {file_name}")


if __name__ == "__main__":

    import argparse

    parser = argparse.ArgumentParser(
        description="Finetune a GPT model for classification"
    )
    parser.add_argument(
        "--test_mode",
        default=False,
        action="store_true",
        help=("This flag runs the model in test mode for internal testing purposes. "
              "Otherwise, it runs the model as it is used in the chapter (recommended).")
    )
    args = parser.parse_args()

    main(args.test_mode)
</file>

<file path="ch07/01_main-chapter-code/ollama_evaluate.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# A minimal instruction finetuning file based on the code in chapter 7

import json
import psutil
from tqdm import tqdm
import urllib.request


def query_model(prompt, model="llama3", url="http://localhost:11434/api/chat"):
    # Create the data payload as a dictionary
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "options": {     # Settings below are required for deterministic responses
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }

    # Convert the dictionary to a JSON formatted string and encode it to bytes
    payload = json.dumps(data).encode("utf-8")

    # Create a request object, setting the method to POST and adding necessary headers
    request = urllib.request.Request(url, data=payload, method="POST")
    request.add_header("Content-Type", "application/json")

    # Send the request and capture the response
    response_data = ""
    with urllib.request.urlopen(request) as response:
        # Read and decode the response
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]

    return response_data


def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running


def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text


def main(file_path):
    ollama_running = check_if_running("ollama")

    if not ollama_running:
        raise RuntimeError("Ollama not running. Launch ollama before proceeding.")
    print("Ollama running:", check_if_running("ollama"))

    with open(file_path, "r") as file:
        test_data = json.load(file)

    model = "llama3"
    scores = generate_model_scores(test_data, "model_response", model)
    print(f"Number of scores: {len(scores)} of {len(test_data)}")
    print(f"Average score: {sum(scores)/len(scores):.2f}\n")


def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        if entry[json_key] == "":
            scores.append(0)
        else:
            prompt = (
                f"Given the input `{format_input(entry)}` "
                f"and correct output `{entry['output']}`, "
                f"score the model response `{entry[json_key]}`"
                f" on a scale from 0 to 100, where 100 is the best score. "
                f"Respond with the integer number only."
            )
            score = query_model(prompt, model)
            try:
                scores.append(int(score))
            except ValueError:
                print(f"Could not convert score: {score}")
                continue

    return scores


if __name__ == "__main__":

    import argparse

    parser = argparse.ArgumentParser(
        description="Evaluate model responses with ollama"
    )
    parser.add_argument(
        "--file_path",
        required=True,
        help=(
            "The path to the test dataset `.json` file with the"
            " `'output'` and `'model_response'` keys"
        )
    )
    args = parser.parse_args()

    main(file_path=args.file_path)
</file>

<file path="ch07/01_main-chapter-code/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-6.
# This file can be run as a standalone script.


import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx


def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            tokens_seen += input_batch.numel()
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Print a sample text after each epoch
        generate_and_print_sample(
            model, tokenizer, device, start_context
        )

    return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig("loss-plot.pdf")
    plt.show()
</file>

<file path="ch07/01_main-chapter-code/README.md">
# 第7章：微调以跟随指令

### 主要章节代码

- [ch07.ipynb](ch07.ipynb) 包含本章中出现的所有代码
- [previous_chapters.py](previous_chapters.py) 是一个Python模块，包含我们在前几章中编码和训练的GPT模型以及许多实用功能，我们在本章中重用这些功能
- [gpt_download.py](gpt_download.py) 包含下载预训练GPT模型权重的实用函数
- [exercise-solutions.ipynb](exercise-solutions.ipynb) 包含本章的习题解答

### 可选代码

- [load-finetuned-model.ipynb](load-finetuned-model.ipynb) 是一个独立的Jupyter笔记本，用于加载我们在本章创建的指令微调模型
- [gpt_instruction_finetuning.py](gpt_instruction_finetuning.py) 是一个独立的Python脚本，用于按本章所述进行指令微调（可以视为本章微调部分的总结）

用法：
```bash
python gpt_instruction_finetuning.py
```bash
python gpt_instruction_finetuning.py
```

```
matplotlib version: 3.9.0
tiktoken version: 0.7.0
torch version: 2.3.1
tqdm version: 4.66.4
tensorflow version: 2.16.1
--------------------------------------------------
Training set length: 935
Validation set length: 55
Test set length: 110
--------------------------------------------------
Device: cpu
--------------------------------------------------
File already exists and is up-to-date: gpt2/355M/checkpoint
File already exists and is up-to-date: gpt2/355M/encoder.json
File already exists and is up-to-date: gpt2/355M/hparams.json
File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001
File already exists and is up-to-date: gpt2/355M/model.ckpt.index
File already exists and is up-to-date: gpt2/355M/model.ckpt.meta
File already exists and is up-to-date: gpt2/355M/vocab.bpe
Loaded model: gpt2-medium (355M)
--------------------------------------------------
Initial losses
   Training loss: 3.839039182662964
   Validation loss: 3.7619192123413088
Ep 1 (Step 000000): Train loss 2.611, Val loss 2.668
Ep 1 (Step 000005): Train loss 1.161, Val loss 1.131
Ep 1 (Step 000010): Train loss 0.939, Val loss 0.973
...
Training completed in 15.66 minutes.
Plot saved as loss-plot-standalone.pdf
--------------------------------------------------
Generating responses
100%|█████████████████████████████████████████████████████████| 110/110 [06:57<00:00,  3.80s/it]
Responses saved as instruction-data-with-response-standalone.json
Model saved as gpt2-medium355M-sft-standalone.pth
```

- [ollama_evaluate.py](ollama_evaluate.py) 是一个独立的Python脚本，用于评估微调模型的响应，如本章所述（可以视为本章评估部分的总结）

用法：
```bash
python ollama_evaluate.py --file_path instruction-data-with-response-standalone.json

```bash
python ollama_evaluate.py --file_path instruction-data-with-response-standalone.json
```

```
Ollama running: True
Scoring entries: 100%|███████████████████████████████████████| 110/110 [01:08<00:00,  1.62it/s]
Number of scores: 110 of 110
Average score: 51.75
```

- [exercise_experiments.py](exercise_experiments.py) 是一个可选的脚本，实现了习题解答；更多细节请见 [exercise-solutions.ipynb](exercise-solutions.ipynb)
</file>

<file path="ch07/01_main-chapter-code/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)


import subprocess


def test_gpt_class_finetune():
    command = ["python", "ch07/01_main-chapter-code/gpt_instruction_finetuning.py", "--test_mode"]

    result = subprocess.run(command, capture_output=True, text=True)
    assert result.returncode == 0, f"Script exited with errors: {result.stderr}"
</file>

<file path="ch07/02_dataset-utilities/config.json">
{
    "OPENAI_API_KEY": "sk-...",
    "_comment": "Enter your API key from https://platform.openai.com/api-keys"
}
</file>

<file path="ch07/02_dataset-utilities/find-near-duplicates.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
import json
import re
from sklearn import __version__ as sklearn_version
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# Sample JSON dataset
example_data = [
    {"instruction": "What is the capital of Italy?",
     "input": "", "output": "The capital of Italy is Rome."
     },
    {"instruction": "What's the capital city of Italy?",
     "input": "", "output": "The capital city is Rome."
     },
    {"instruction": "Identify the main verb in the sentence: 'The cat sleeps on the couch.'",
     "input": "", "output": "The verb is 'sleeps'."
     },
    {"instruction": "Identify the verb in the following sentence: The cat sleeps on the couch.",
     "input": "", "output": "The verb in the sentence is \"sleeps.\""
     },
    # ...
]


def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    return text


def find_near_duplicates(json_data, threshold=0.75, key="instruction"):
    """The higher the threshold, the more similar the texts have to be to match"""

    # Extract instructions
    text = [preprocess_text(item[key]) for item in json_data if item[key]]
    near_duplicates = []
    indices_to_remove = set()

    if not text:
        return {}, near_duplicates

    # Vectorize the text data
    vectorizer = TfidfVectorizer(stop_words=None, analyzer='char', ngram_range=(1, 3))
    tfidf_matrix = vectorizer.fit_transform(text)

    # Compute cosine similarity between each pair of entries
    cos_sim_matrix = cosine_similarity(tfidf_matrix)

    # Find pairs of near-duplicate instructions based on the threshold

    for i in range(len(cos_sim_matrix)):
        for j in range(i+1, len(cos_sim_matrix)):
            if cos_sim_matrix[i, j] > threshold:
                if len(json_data[i][key]) <= 1 or len(json_data[j][key]) <= 1:
                    continue
                near_duplicates.append((json_data[i], json_data[j], cos_sim_matrix[i, j]))
                if key in ("input", "output"):  # Don't remove duplicates based on the instruction
                    indices_to_remove.add(j)  # Mark the second entry for removal

    # Remove the near-duplicate entries
    filtered_json_data = [item for index, item in enumerate(json_data) if index not in indices_to_remove]

    return filtered_json_data, near_duplicates


def find_print_and_remove_near_duplicates(json_data, remove_duplicates=False, threshold=0.75):
    """
    Searches each key in the first JSON object for duplicates across a list of JSON objects.
    Prints the duplicates if found.
    """
    for key in json_data[0].keys():

        if remove_duplicates:
            json_data, near_duplicates = find_near_duplicates(json_data, key=key, threshold=threshold)
        else:
            _, near_duplicates = find_near_duplicates(json_data, key=key, threshold=threshold)
        separator = 50 * '='
        print(f"\n\n{separator}\nSearching '{key}' for duplicates ...\n{separator}")
        if not near_duplicates:
            print("No duplicates found")
        else:
            for dup in near_duplicates:
                print(
                    f"Duplicate pair found with similarity {dup[2]:.2f}:\n"
                    f"1. {dup[0][key]}\n2. {dup[1][key]}\n"
                )
    return json_data


if __name__ == "__main__":
    print("scikit-learn version:", sklearn_version)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--json_file",
        type=str,
        help=("Path to the dataset JSON file")
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.9,
        help=("A sensitivity threshold between 0 and 1 where 1 is strictest")
    )
    parser.add_argument(
        "--remove_duplicates",
        action='store_true',
        default=False,
        help=(
            "Removes duplicates based on the 'input' or 'output' keys "
            " (but not the 'instruction') and saves the cleaned JSON file as --json_output_file"
        )
    )
    parser.add_argument(
        "--json_output_file",
        type=str,
        help=("Path to the dataset JSON file")
    )

    args = parser.parse_args()

    if args.remove_duplicates and not args.json_output_file:
        raise ValueError(
            "Provide an output file via --json_output_file "
            "to save the cleaned JSON data."
        )

    if not args.json_file:
        json_data = example_data

    else:
        with open(args.json_file, "r") as file:
            json_data = json.load(file)

    json_data = find_print_and_remove_near_duplicates(
        json_data=json_data,
        remove_duplicates=args.remove_duplicates,
        threshold=args.threshold
    )

    if args.remove_duplicates:
        with open(args.json_output_file, "w") as file:
            json.dump(json_data, file, indent=4)
</file>

<file path="ch07/02_dataset-utilities/instruction-examples-modified.json">
[
    {
        "instruction": "Identify the verb in the following sentence: The cat sleeps on the couch.",
        "input": "",
        "output": "The verb in the sentence is \"sleeps.\"",
        "output_2": "The sentence is \"The verb in the sentence is slept.\""
    },
    {
        "instruction": "What is the plural form of \"goose\"?",
        "input": "",
        "output": "The plural form of \"goose\" is \"geese.\"",
        "output_2": "The plural form of \"goose\" is referred to as \"geese.\""
    },
    {
        "instruction": "List three primary colors.",
        "input": "",
        "output": "The three primary colors are red, blue, and yellow.",
        "output_2": "Red, blue, and yellow are the three primary colors."
    },
    {
        "instruction": "Change the following sentence to past perfect tense.",
        "input": "They finish the game.",
        "output": "They had finished the game.",
        "output_2": "The game had been finished by them."
    },
    {
        "instruction": "What is the abbreviation for \"Doctor of Philosophy\"?",
        "input": "",
        "output": "The abbreviation for \"Doctor of Philosophy\" is Ph.D.",
        "output_2": "The abbreviation \"Ph.D.\" is used for \"Doctor of Philosophy\"."
    },
    {
        "instruction": "Correct the misspelled word in the following sentence: I recieved your message.",
        "input": "",
        "output": "The misspelled word \"recieved\" should be spelled \"received.\"",
        "output_2": "The misspelled word \"recieved\" should be spelled \"received\" should be stated."
    },
    {
        "instruction": "Transform the following sentence to include an exclamation mark to show excitement.",
        "input": "I won the lottery",
        "output": "I won the lottery!",
        "output_2": "The lottery was won by me!"
    },
    {
        "instruction": "What is the antonym of \"ancient\"?",
        "input": "",
        "output": "The antonym of \"ancient\" is \"modern.\"",
        "output_2": "The antonym of \"ancient\" is said to be \"modern.\""
    },
    {
        "instruction": "Name a synonym for \"happiness.\"",
        "input": "",
        "output": "A synonym for \"happiness\" is \"joy.\"",
        "output_2": "A synonym for \"happiness\" is referred to as \"joy.\""
    },
    {
        "instruction": "Classify the following items: gold ring, daisy, python.",
        "input": "",
        "output": "Animals: Python\nPlants: Daisy\nMinerals: Gold ring",
        "output_2": "Animals: Python is possessed.\nPlants: Daisy is grown.\nMinerals: Gold ring is made."
    },
    {
        "instruction": "Generate three adjectives to describe a mountain.",
        "input": "",
        "output": "1. Majestic\n2. Rugged\n3. Towering",
        "output_2": "1. Majestic\n2. Rugged\n3. Towering\n\n(Note: The words provided are adjectives and not sentences, hence they cannot be converted into passive voice as they do not contain a verb.)"
    },
    {
        "instruction": "Determine the state of matter for helium at room temperature.",
        "input": "",
        "output": "Helium is in a gaseous state at room temperature.",
        "output_2": "At room temperature, helium is found in a gaseous state."
    },
    {
        "instruction": "Rewrite the following sentence to use a simile: The car raced down the road.",
        "input": "",
        "output": "The car raced down the road like a bullet.",
        "output_2": "Down the road like a bullet, the car was raced."
    },
    {
        "instruction": "What is the symbol for the chemical element iron?",
        "input": "",
        "output": "The symbol for the chemical element iron is Fe.",
        "output_2": "The symbol for the chemical element iron is represented by Fe."
    },
    {
        "instruction": "Convert the sentence into a question by adding a question tag.",
        "input": "You will help me",
        "output": "You will help me, won't you?",
        "output_2": "I will be helped by you, won't I?"
    },
    {
        "instruction": "List two synonyms for \"intelligent.\"",
        "input": "",
        "output": "Two synonyms for \"intelligent\" are \"smart\" and \"clever.\"",
        "output_2": "\"Smart\" and \"clever\" are two synonyms for \"intelligent.\""
    },
    {
        "instruction": "Name the process by which plants lose water vapor through their leaves.",
        "input": "",
        "output": "The process by which plants lose water vapor through their leaves is called transpiration.",
        "output_2": "Transpiration is called the process by which water vapor is lost through the leaves of plants."
    },
    {
        "instruction": "Provide a word that rhymes with \"care.\"",
        "input": "",
        "output": "A word that rhymes with \"care\" is \"fare.\"",
        "output_2": "A word with which \"care\" rhymes is \"fare.\""
    },
    {
        "instruction": "Rewrite this sentence to start with \"Despite\": She managed to finish her work, although she was tired.",
        "input": "",
        "output": "Despite being tired, she managed to finish her work.",
        "output_2": "Despite being tired, the work was managed to be finished by her."
    },
    {
        "instruction": "Convert the following sentence to use a relative clause: The man is a teacher. He lives next door.",
        "input": "",
        "output": "The man who lives next door is a teacher.",
        "output_2": "The man who is lived next door by is a teacher."
    },
    {
        "instruction": "Arrange the following events in chronological order: Invention of the airplane, Fall of the Berlin Wall, Discovery of America.",
        "input": "",
        "output": "1. Discovery of America\n2. Invention of the airplane\n3. Fall of the Berlin Wall",
        "output_2": "1. America was discovered.\n2. The airplane was invented.\n3. The Berlin Wall was fallen."
    },
    {
        "instruction": "Name three types of clouds.",
        "input": "",
        "output": "Three types of clouds are cumulus, stratus, and cirrus.",
        "output_2": "Cumulus, stratus, and cirrus are three types of clouds."
    },
    {
        "instruction": "Create a compound word using \"book.\"",
        "input": "",
        "output": "A compound word using \"book\" is \"bookshelf.\"",
        "output_2": "A compound word using \"book\" is referred to as \"bookshelf.\""
    },
    {
        "instruction": "Identify the main subject in the sentence: The noisy children played in the park.",
        "input": "",
        "output": "The main subject in the sentence is \"The noisy children.\"",
        "output_2": "\"The noisy children\" is the main subject in the sentence."
    },
    {
        "instruction": "Explain what a haiku is.",
        "input": "",
        "output": "A haiku is a form of traditional Japanese poetry that consists of three lines with a syllable pattern of 5-7-5.",
        "output_2": "A form of traditional Japanese poetry known as a haiku consists of three lines with a syllable pattern of 5-7-5."
    },
    {
        "instruction": "Provide the past tense form of the verb \"run.\"",
        "input": "",
        "output": "The past tense of the verb \"run\" is \"ran.\"",
        "output_2": "The past tense of the verb \"run\" is referred to as \"ran.\""
    },
    {
        "instruction": "List the ingredients required to make a basic salad.",
        "input": "",
        "output": "Ingredients for a basic salad typically include lettuce, tomatoes, cucumbers, and dressing.",
        "output_2": "Lettuce, tomatoes, cucumbers, and dressing are typically included as ingredients for a basic salad."
    },
    {
        "instruction": "What is the capital of Australia?",
        "input": "",
        "output": "The capital of Australia is Canberra.",
        "output_2": "Canberra is the capital of Australia."
    },
    {
        "instruction": "Name a punctuation mark used to indicate possession.",
        "input": "",
        "output": "A punctuation mark used to indicate possession is the apostrophe (').",
        "output_2": "A punctuation mark used to indicate possession is indicated by the apostrophe (')."
    },
    {
        "instruction": "Name an antonym for \"joy.\"",
        "input": "",
        "output": "An antonym for \"joy\" is \"sorrow.\"",
        "output_2": "An antonym for \"joy\" is considered to be \"sorrow.\""
    },
    {
        "instruction": "Classify the following items: apple, diamond, fern.",
        "input": "",
        "output": "Plants: Fern\nMinerals: Diamond\nFruits: Apple",
        "output_2": "Plants: Fern is grown.\nMinerals: Diamond is formed.\nFruits: Apple is cultivated."
    },
    {
        "instruction": "Generate three adjectives to describe the ocean.",
        "input": "",
        "output": "1. Vast\n2. Mysterious\n3. Serene",
        "output_2": "1. Vast is remained.\n2. Mysterious is considered.\n3. Serene is felt."
    },
    {
        "instruction": "Determine the state of matter for nitrogen at room temperature.",
        "input": "",
        "output": "Nitrogen is in a gaseous state at room temperature.",
        "output_2": "At room temperature, nitrogen is in a gaseous state."
    },
    {
        "instruction": "Convert the following statement into a question.",
        "input": "You were at the concert last night.",
        "output": "Were you at the concert last night?",
        "output_2": "Was the concert attended by you last night?"
    },
    {
        "instruction": "Arrange these historical periods in chronological order: Renaissance, Roman Empire, Victorian Era.",
        "input": "",
        "output": "1. Roman Empire\n2. Renaissance\n3. Victorian Era",
        "output_2": "1. Roman Empire\n2. Renaissance\n3. Victorian Era"
    },
    {
        "instruction": "Suggest a more formal synonym for \"happy.\"",
        "input": "",
        "output": "A more formal synonym for \"happy\" is \"content.\"",
        "output_2": "A more formal synonym for \"happy\" is considered to be \"content.\""
    },
    {
        "instruction": "Evaluate this sentence for spelling and grammar mistakes.",
        "input": "I recieved an invitation to an exciting evenement.",
        "output": "There are two errors in the sentence. The corrected sentence should be: \"I received an invitation to an exciting event.\"",
        "output_2": "Two errors are found in the sentence. The corrected sentence should be: \"An invitation to an exciting event was received by me.\""
    },
    {
        "instruction": "What is the capital of Germany?",
        "input": "",
        "output": "The capital city of Germany is Berlin.",
        "output_2": "Berlin is the capital city of Germany."
    },
    {
        "instruction": "Classify the following into animals, plants, and metals.",
        "input": "Rose, gold, cat",
        "output": "Animals: Cat\nPlants: Rose\nMetals: Gold",
        "output_2": "Animals: Cat is mentioned.\nPlants: Rose is mentioned.\nMetals: Gold is mentioned."
    },
    {
        "instruction": "Generate three verbs that mean the same as \"to run.\"",
        "input": "",
        "output": "1. To sprint\n2. To dash\n3. To jog",
        "output_2": "1. To be sprinted\n2. To be dashed\n3. To be jogged"
    },
    {
        "instruction": "Look up the melting point of iron.",
        "input": "",
        "output": "The melting point of iron is approximately 1538 degrees Celsius or 2800 degrees Fahrenheit.",
        "output_2": "The melting point of iron is approximately 1538 degrees Celsius or 2800 degrees Fahrenheit."
    },
    {
        "instruction": "Rewrite the following sentence using passive voice.",
        "input": "The team achieved great results.",
        "output": "Great results were achieved by the team.",
        "output_2": "The team achieved great results."
    },
    {
        "instruction": "Edit the following sentence to make it more formal.",
        "input": "He's got to see it to believe it.",
        "output": "He must see it to believe it.",
        "output_2": "It must be seen by him to be believed."
    },
    {
        "instruction": "Arrange the given numbers in descending order.",
        "input": "7, 10, 3, 6, 9",
        "output": "10, 9, 7, 6, 3.",
        "output_2": "10, 9, 7, 6, 3 were counted."
    },
    {
        "instruction": "Given a sentence, convert it into indirect speech.",
        "input": "She said, \"I am tired.\"",
        "output": "She said that she was tired.",
        "output_2": "It was said by her that she was tired."
    },
    {
        "instruction": "Transform the following sentence into the passive voice.",
        "input": "They will build a new bridge.",
        "output": "A new bridge will be built by them.",
        "output_2": "A new bridge will be built by them."
    },
    {
        "instruction": "What is the capital of Italy?",
        "input": "",
        "output": "The capital of Italy is Rome.",
        "output_2": "Rome is the capital of Italy."
    },
    {
        "instruction": "Generate a random alphanumeric code of 10 characters.",
        "input": "",
        "output": "Here is a randomly generated 10-character alphanumeric code: 3F8G6H7JQ2.",
        "output_2": "A randomly generated 10-character alphanumeric code is presented here: 3F8G6H7JQ2."
    },
    {
        "instruction": "Identify the correct spelling of the following word.",
        "input": "Necesary",
        "output": "The correct spelling is \"Necessary\".",
        "output_2": "The correct spelling is said to be \"Necessary\"."
    },
    {
        "instruction": "Change this sentence into the past continuous tense.",
        "input": "She sings in the choir.",
        "output": "She was singing in the choir.",
        "output_2": "Singing in the choir was being done by her."
    },
    {
        "instruction": "Make the first sentence a dependent clause.",
        "input": "I finished my work. I went home.",
        "output": "After finishing my work, I went home.",
        "output_2": "After my work was finished, I was gone home."
    },
    {
        "instruction": "Rewrite the sentence in future perfect tense.",
        "input": "They will complete the project.",
        "output": "They will have completed the project.",
        "output_2": "The project will have been completed by them."
    },
    {
        "instruction": "Convert the given sentence into active voice.",
        "input": "The law was passed by the government.",
        "output": "The government passed the law.",
        "output_2": "The law was passed by the government."
    },
    {
        "instruction": "Transform the following sentence into a question using \"could.\"",
        "input": "You can help me tomorrow.",
        "output": "Could you help me tomorrow?",
        "output_2": "Could I be helped by you tomorrow?"
    },
    {
        "instruction": "Correct the grammatical errors in this sentence.",
        "input": "He don't know nothing about it.",
        "output": "He doesn't know anything about it.",
        "output_2": "Anything about it is not known by him."
    },
    {
        "instruction": "Rewrite the following sentence to remove redundancy.",
        "input": "The actual fact is that he was late.",
        "output": "The fact is that he was late.",
        "output_2": "The fact is that he was late."
    },
    {
        "instruction": "Rewrite the following sentence so that it is in active voice.",
        "input": "The cookies were eaten by the children.",
        "output": "The children ate the cookies.",
        "output_2": "The cookies were eaten by the children."
    },
    {
        "instruction": "What is the contraction for \"will not\"?",
        "input": "",
        "output": "The contraction for \"will not\" is \"won't\".",
        "output_2": "The contraction for \"will not\" is referred to as \"won't\"."
    },
    {
        "instruction": "Generate a sentence that follows the pattern: \"Never have I ever _____ without _____\"",
        "input": "",
        "output": "Never have I ever traveled without a map.",
        "output_2": "Never has a map not been traveled with by me."
    },
    {
        "instruction": "Classify this text as a technical document or a narrative.",
        "input": "This manual provides instructions for installing the software.",
        "output": "Technical document",
        "output_2": "A technical document"
    },
    {
        "instruction": "Sort the following list in alphabetical order.",
        "input": "Zebra, Elephant, Crocodile",
        "output": "sorted in alphabetical order: Crocodile, Elephant, Zebra",
        "output_2": "Sorted in alphabetical order: Crocodile, Elephant, Zebra."
    },
    {
        "instruction": "Name the three main types of rocks.",
        "input": "",
        "output": "The three main types of rocks are igneous, sedimentary, and metamorphic.",
        "output_2": "The three main types of rocks are identified as igneous, sedimentary, and metamorphic."
    },
    {
        "instruction": "Edit the following sentence for grammar.",
        "input": "We goes to the park every Sunday.",
        "output": "We go to the park every Sunday.",
        "output_2": "The park is visited by us every Sunday."
    },
    {
        "instruction": "Find a rhyming word for the word \"care.\"",
        "input": "",
        "output": "A rhyming word for the word \"care\" could be \"bare.\"",
        "output_2": "A rhyming word for the word \"care\" could be \"bare\" by \"bare.\""
    },
    {
        "instruction": "Reword the following sentence to the future tense.",
        "input": "He is reading a novel inspired by his grandmother.",
        "output": "He will be reading a novel inspired by his grandmother.",
        "output_2": "A novel inspired by his grandmother will be being read by him."
    },
    {
        "instruction": "Classify the following fruit according to its type.",
        "input": "Watermelon",
        "output": "The fruit \"Watermelon\" is typically classified as a type of melon.",
        "output_2": "The fruit \"Watermelon\" is typically classified as a type of melon by people."
    },
    {
        "instruction": "Suggest a continuation for the sentence: \"When the sun set,\"",
        "input": "",
        "output": "\"the sky turned a brilliant shade of orange, casting a warm glow over the city.\"",
        "output_2": "A brilliant shade of orange was turned by the sky, casting a warm glow over the city."
    },
    {
        "instruction": "Classify the given sentiment as either positive, negative, or neutral.",
        "input": "This is a great opportunity!",
        "output": "Positive",
        "output_2": "Positive feedback is appreciated."
    },
    {
        "instruction": "Generate a question that could logically follow the statement below.",
        "input": "I just returned from a vacation in Spain.",
        "output": "How was your vacation in Spain?",
        "output_2": "How was your vacation in Spain enjoyed by you?"
    },
    {
        "instruction": "Create a sentence that begins with \"If only,\"",
        "input": "",
        "output": "If only I had known the truth earlier...",
        "output_2": "If only the truth had been known to me earlier..."
    },
    {
        "instruction": "Correct any spelling mistakes in the given sentence.",
        "input": "I prefer homemade cookies to store boaght.",
        "output": "I prefer homemade cookies to store bought.",
        "output_2": "Homemade cookies are preferred by me to store-bought."
    },
    {
        "instruction": "Edit the following sentence for clarity and style.",
        "input": "He response to the issue was quick.",
        "output": "His response to the issue was quick.",
        "output_2": "A quick response was given to the issue by him."
    },
    {
        "instruction": "Name three essential elements found in nature.",
        "input": "",
        "output": "1. Carbon\n2. Oxygen\n3. Iron",
        "output_2": "1. Carbon is converted.\n2. Oxygen is converted.\n3. Iron is converted."
    },
    {
        "instruction": "Name a dwarf planet in our solar system.",
        "input": "",
        "output": "One dwarf planet in our solar system is Pluto.",
        "output_2": "Pluto is one dwarf planet in our solar system."
    },
    {
        "instruction": "Name a dwarf planet in our solar system.",
        "input": "",
        "output": "One dwarf planet in our solar system is Pluto.",
        "output_2": "Pluto is one dwarf planet in our solar system."
    },
    {
        "instruction": "Classify the following words by their grammatical categories: swim, beautiful, quickly",
        "input": "",
        "output": "Swim: Verb\nBeautiful: Adjective\nQuickly: Adverb",
        "output_2": "Swum: Verb\nBeautiful: Adjective\nQuickly: Adverb"
    },
    {
        "instruction": "Pick out the correct verb from the following list.",
        "input": "run, mountain, quickly",
        "output": "The correct verb from the list is \"run.\"",
        "output_2": "The correct verb from the list, \"run,\" is chosen."
    },
    {
        "instruction": "Combine the two sentences into a single coherent sentence.",
        "input": "She did not attend the meeting. She was ill.",
        "output": "She did not attend the meeting because she was ill.",
        "output_2": "The meeting was not attended by her because she was ill."
    },
    {
        "instruction": "Classify the item as either a grain or a fruit.",
        "input": "Wheat",
        "output": "Grain",
        "output_2": "Grain is grown."
    },
    {
        "instruction": "Classify the following statement into one of these labels: [positive, negative, neutral]",
        "input": "My computer crashed.",
        "output": "The statement \"My computer crashed\" can be classified as negative.",
        "output_2": "The statement \"My computer crashed\" can be classified as negative."
    },
    {
        "instruction": "Rewrite the following sentence in the active voice",
        "input": "The meeting will be held by the board.",
        "output": "The board will hold the meeting.",
        "output_2": "The meeting will be held by the board."
    },
    {
        "instruction": "Re-word this sentence using an active voice.",
        "input": "The decision was made by the team.",
        "output": "The team made the decision.",
        "output_2": "The decision was made by the team."
    },
    {
        "instruction": "Change the given verb to its present perfect form.",
        "input": "go",
        "output": "Gone.",
        "output_2": "Gone."
    },
    {
        "instruction": "Perform a sentiment analysis on the given sentence.",
        "input": "I hate this song!",
        "output": "The sentiment of the sentence \"I hate this song!\" is negative. This is indicated by the word \"hate\" which shows strong aversion or dislike towards the subject, in this case, the song.",
        "output_2": "The sentiment of the sentence \"I hate this song!\" is identified as negative. This is indicated by the word \"hate,\" which shows strong aversion or dislike towards the subject, in this case, the song."
    },
    {
        "instruction": "Find the temperature of boiling water under normal atmospheric pressure.",
        "input": "",
        "output": "Under normal atmospheric pressure, the temperature of boiling water is 100 degrees Celsius (212 degrees Fahrenheit).",
        "output_2": "Under normal atmospheric pressure, the boiling point of water is reached at 100 degrees Celsius (212 degrees Fahrenheit)."
    },
    {
        "instruction": "Edit the following sentence to correct any grammar errors.",
        "input": "They was trying to fix the car all day.",
        "output": "They were trying to fix the car all day.",
        "output_2": "The car was being tried to be fixed all day."
    },
    {
        "instruction": "List 5 different types of vegetables",
        "input": "",
        "output": "1. Carrot\n2. Broccoli\n3. Cucumber\n4. Tomato (biologically a fruit, but often classified as a vegetable)\n5. Spinach",
        "output_2": "1. Carrot is grown.\n2. Broccoli is cultivated.\n3. Cucumber is raised.\n4. Tomato is produced (biologically a fruit, but often classified as a vegetable).\n5. Spinach is farmed."
    },
    {
        "instruction": "Turn the following statement into a question.",
        "input": "The Golden State Warriors are a professional basketball team based in San Francisco.",
        "output": "Are the Golden State Warriors a professional basketball team based in San Francisco?",
        "output_2": "Is a professional basketball team based in San Francisco represented by the Golden State Warriors?"
    },
    {
        "instruction": "Fill in the blank to complete the sentence:\nThe most popular fruit in the world is the ____",
        "input": "",
        "output": "The most popular fruit in the world is the banana.",
        "output_2": "The banana is the most popular fruit in the world."
    },
    {
        "instruction": "Generate a question based on the following statement.",
        "input": "Plants produce oxygen through the process of photosynthesis.",
        "output": "How do plants produce oxygen?",
        "output_2": "How is oxygen produced by plants?"
    },
    {
        "instruction": "Correct any mistakes in the following sentence.",
        "input": "Everyones going to the game tomorrow.",
        "output": "Everyone's going to the game tomorrow.",
        "output_2": "The game is going to be attended by everyone tomorrow."
    },
    {
        "instruction": "Rewrite the following sentence by adding one adverb.",
        "input": "The cat slept on the mat.",
        "output": "The cat peacefully slept on the mat.",
        "output_2": "On the mat, the cat was peacefully slept."
    },
    {
        "instruction": "Identify the tense used in the sentence.",
        "input": "They are watching a movie.",
        "output": "The tense used in the sentence \"They are watching a movie\" is the present continuous tense.",
        "output_2": "The tense used in the sentence \"A movie is being watched by them\" is the present continuous tense."
    },
    {
        "instruction": "Categorize the following sentence as either factual or opinion-based.",
        "input": "Chocolate is the best dessert.",
        "output": "Opinion-based.",
        "output_2": "Opinion-based was written."
    },
    {
        "instruction": "Classify each sentence as either declarative, interrogative, imperative, or exclamatory.",
        "input": "Can you help me?",
        "output": "The classification of the sentence \"Can you help me?\" is interrogative.",
        "output_2": "The sentence \"Can you help me?\" is classified as interrogative."
    },
    {
        "instruction": "Reverse this array",
        "input": "[10, 20, 30, 40, 50]",
        "output": "Here is the reversed array: [50, 40, 30, 20, 10]",
        "output_2": "The reversed array is presented here: [50, 40, 30, 20, 10]."
    },
    {
        "instruction": "What is the chemical formula for salt?",
        "input": "",
        "output": "The chemical formula for salt (sodium chloride) is NaCl.",
        "output_2": "The chemical formula for salt (sodium chloride) is given as NaCl."
    },
    {
        "instruction": "Rewrite this sentence in the present continuous tense.",
        "input": "They play football every Sunday.",
        "output": "They are playing football.",
        "output_2": "Football is being played by them."
    },
    {
        "instruction": "Categorize the following items into herbs and spices.",
        "input": "Cinnamon, Basil, Pepper",
        "output": "Herbs: Basil\nSpices: Cinnamon, Pepper",
        "output_2": "Herbs: Basil is used.\nSpices: Cinnamon, Pepper are used."
    },
    {
        "instruction": "Cut the following sentence into two, keeping the original meaning intact.",
        "input": "She started her homework but was soon distracted by a phone call.",
        "output": "She started her homework. However, she was soon distracted by a phone call.",
        "output_2": "Her homework was started by her. However, she was soon distracted by a phone call."
    },
    {
        "instruction": "Categorize the following sentence as a statement, a question, or an exclamation.",
        "input": "What a beautiful day!",
        "output": "Exclamation.",
        "output_2": "Exclamation was made."
    },
    {
        "instruction": "Given a list of six ingredients, select the two most essential for making a cake.",
        "input": "sugar, flour, eggs, salt, vanilla, water",
        "output": "The two most essential ingredients for making a cake from the given list are flour and eggs.",
        "output_2": "The two most essential ingredients for making a cake from the given list, flour and eggs, are considered."
    },
    {
        "instruction": "Generate a sentence that uses the word \u2018mystic\u2019.",
        "input": "",
        "output": "The ancient ruins had a mystic quality that captivated all who visited.",
        "output_2": "A mystic quality was had by the ancient ruins that captivated all who visited."
    },
    {
        "instruction": "Change the sentences from active voice to passive voice.",
        "input": "The chef will prepare the meal.",
        "output": "The meal will be prepared by the chef.",
        "output_2": "The meal will be prepared by the chef."
    },
    {
        "instruction": "Transform the given statement into an imperative sentence.",
        "input": "I would like you to turn off the light.",
        "output": "Please turn off the light.",
        "output_2": "The light is requested to be turned off."
    },
    {
        "instruction": "Edit the sentence to make it more formal.",
        "input": "You're gonna want to see this.",
        "output": "You will want to observe this.",
        "output_2": "This will be wanted to be observed by you."
    },
    {
        "instruction": "Give me a synonym for the word 'strong'.",
        "input": "",
        "output": "Powerful.",
        "output_2": "Powerful."
    },
    {
        "instruction": "Rewrite the following sentence in a more formal way:\n\"I need to find out what's wrong.\"",
        "input": "",
        "output": "I must ascertain what is incorrect.",
        "output_2": "What is incorrect must be ascertained by me."
    },
    {
        "instruction": "Generate a humorous anecdote.",
        "input": "",
        "output": "Why was the math book sad? Because it had too many problems!",
        "output_2": "Why was the math book sad? Because too many problems were had by it!"
    },
    {
        "instruction": "Change the sentence from passive to active voice.",
        "input": "The song was sung by the choir.",
        "output": "The choir sang the song.",
        "output_2": "The song was sung by the choir."
    },
    {
        "instruction": "Label whether the following statements are true or false.",
        "input": "The moon is a planet.",
        "output": "False",
        "output_2": "False is not a sentence and therefore cannot be converted into passive voice."
    },
    {
        "instruction": "Combine the given sentences into one cohesive sentence.",
        "input": "It is raining. I cannot go to the park.",
        "output": "Since it is raining, I cannot go to the park.",
        "output_2": "Since it is raining, the park cannot be gone to by me."
    },
    {
        "instruction": "Rewrite the given passage in the past tense.",
        "input": "They are building a new library.",
        "output": "They built a new library.",
        "output_2": "A new library was built."
    },
    {
        "instruction": "Change the following sentence so that it is in the past tense.",
        "input": "He runs to the store.",
        "output": "He ran to the store.",
        "output_2": "The store was run to by him."
    },
    {
        "instruction": "Edit the following sentence to remove all passive voice.",
        "input": "The song was composed by the artist.",
        "output": "The artist composed the song.",
        "output_2": "The song was composed by the artist."
    },
    {
        "instruction": "Edit the sentence below to remove any redundant words and make it more concise.",
        "input": "I actually think that",
        "output": "I think.",
        "output_2": "It is thought by me."
    },
    {
        "instruction": "Edit the following sentence to remove any redundant words.",
        "input": "My personal belief is that it could potentially be dangerous.",
        "output": "I believe it could be dangerous.",
        "output_2": "It is believed by me that it could be dangerous."
    },
    {
        "instruction": "Sort the following numbers in ascending order.",
        "input": "5, 18, 3, 11.",
        "output": "The numbers in ascending order would be: 3, 5, 11, 18.",
        "output_2": "In ascending order, the numbers would be: 3, 5, 11, 18."
    },
    {
        "instruction": "Classify this speech as either formal or informal.",
        "input": "Could you please pass the salt?",
        "output": "This speech is formal.",
        "output_2": "This speech is considered formal."
    },
    {
        "instruction": "Classify the following online review as being positive or negative.",
        "input": "I found this book to be incredibly insightful and informative.",
        "output": "The online review is positive.",
        "output_2": "The online review is considered positive."
    },
    {
        "instruction": "Reverse the following list alphabetically.",
        "input": "apple, orange, banana, melon",
        "output": "The reverse alphabetical order of the list is: orange, melon, banana, apple.",
        "output_2": "The reverse alphabetical order of the list is given as: orange, melon, banana, apple."
    },
    {
        "instruction": "Rearrange the following list into descending order.",
        "input": "5, -3, 20, 10",
        "output": "20, 10, 5, -3",
        "output_2": "20, 10, 5, -3 were converted."
    },
    {
        "instruction": "Convert the phrase into a complete sentence.",
        "input": "night sky",
        "output": "The night sky was dotted with countless twinkling stars.",
        "output_2": "Countless twinkling stars dotted the night sky."
    },
    {
        "instruction": "Classify the following items as either solid, liquid, or gas.",
        "input": "Mercury, oxygen, wood",
        "output": "Mercury - Liquid\nOxygen - Gas\nWood - Solid",
        "output_2": "Mercury - Liquidated\nOxygen - Gassed\nWood - Solidified"
    },
    {
        "instruction": "Identify the correct word to complete the sentence.",
        "input": "She presented her ideas with _____.",
        "output": "clarity",
        "output_2": "Clarity is provided."
    },
    {
        "instruction": "Insert a suitable adjective to complete the sentence.",
        "input": "The movie was _____.",
        "output": "The movie was thrilling.",
        "output_2": "The audience was thrilled by the movie."
    },
    {
        "instruction": "Alter the content of the sentence to use the past tense.",
        "input": "The ship sails at dawn.",
        "output": "The ship sailed at dawn.",
        "output_2": "At dawn, the ship was sailed."
    },
    {
        "instruction": "Re-write the following sentence to use a different verb.",
        "input": "I will help you with your homework.",
        "output": "I will assist you with your homework.",
        "output_2": "You will be assisted with your homework by me."
    },
    {
        "instruction": "Pick out the odd word out in each sentence:",
        "input": "Pen, Book, Tree",
        "output": "The odd word out in the sentence \"Pen, Book, Tree\" is \"Tree\".",
        "output_2": "The odd word out in the sentence \"Pen, Book, Tree\" is identified as \"Tree\"."
    },
    {
        "instruction": "Classify the following text into either fiction or non-fiction.",
        "input": "The documentary covers the impact of climate change on polar bears.",
        "output": "Non-fiction.",
        "output_2": "Non-fiction is written."
    },
    {
        "instruction": "What type of number is 0?",
        "input": "",
        "output": "0 is an integer.",
        "output_2": "An integer is represented by 0."
    },
    {
        "instruction": "Split the following sentence into two declarative sentences: \"The book is interesting but the cover is dull\".",
        "input": "",
        "output": "The book is interesting.\nThe cover is dull.",
        "output_2": "The book is found interesting.\nThe cover is considered dull."
    },
    {
        "instruction": "Classify this sentence into one of the following topics: health, economy, technology, music",
        "input": "Spotify has revolutionized how we listen to music.",
        "output": "The topic of the given sentence is music.",
        "output_2": "The topic of music is given in the sentence."
    },
    {
        "instruction": "Generate a question from the following statement",
        "input": "Leonardo da Vinci painted the Mona Lisa.",
        "output": "Who painted the Mona Lisa?",
        "output_2": "By whom was the Mona Lisa painted?"
    },
    {
        "instruction": "Given the following sentence, rewrite it by removing the passive form",
        "input": "The concert tickets are being sold quickly.",
        "output": "They are selling the concert tickets quickly.",
        "output_2": "The concert tickets are being sold quickly."
    },
    {
        "instruction": "Given a sentence, count the number of words.",
        "input": "The quick brown fox jumps over the lazy dog.",
        "output": "The sentence \"The quick brown fox jumps over the lazy dog.\" contains 9 words.",
        "output_2": "It is contained by the sentence \"The quick brown fox jumps over the lazy dog.\" that there are 9 words."
    },
    {
        "instruction": "Rewrite the given sentence to describe the same thing in a positive way.",
        "input": "The meal was not delicious.",
        "output": "The meal could use some improvement.",
        "output_2": "Some improvement could be used in the meal."
    },
    {
        "instruction": "Rewrite the sentence so that it does not contain any passive voice.",
        "input": "The song was played on the radio.",
        "output": "The radio played the song.",
        "output_2": "The song was played by the radio."
    },
    {
        "instruction": "Find a synonym for the word 'start'.",
        "input": "",
        "output": "A synonym for the word 'start' could be \"commence.\"",
        "output_2": "A synonym for the word 'start' could be \"commence\" by some."
    },
    {
        "instruction": "Change the sentence \"This should not have happened\" into a declarative sentence.",
        "input": "",
        "output": "This should have happened differently.",
        "output_2": "Differently, this should have happened."
    },
    {
        "instruction": "Rewrite the sentence: \"The poem was composed in a lyrical manner.\"",
        "input": "",
        "output": "The poem was crafted lyrically.",
        "output_2": "The poem was crafted lyrically."
    },
    {
        "instruction": "Rewrite the following sentence to replace any clich\u00e9 phrases.",
        "input": "It's a piece of cake",
        "output": "It's very easy.",
        "output_2": "It is considered very easy."
    },
    {
        "instruction": "Pick the best synonym for the given word.",
        "input": "Gather",
        "output": "Collect.",
        "output_2": "Be collected."
    },
    {
        "instruction": "Rewrite the sentence using an idiom.",
        "input": "The storm started suddenly.",
        "output": "The storm came out of the blue.",
        "output_2": "The storm was come out of the blue."
    },
    {
        "instruction": "Scramble the given phrase.",
        "input": "A random phrase",
        "output": "A nmaodr hrasep",
        "output_2": "A nmaodr hrasep was written."
    },
    {
        "instruction": "rewrite this sentence as a question.",
        "input": "Your brother went to London last year.",
        "output": "Did your brother go to London last year?",
        "output_2": "Was London visited by your brother last year?"
    },
    {
        "instruction": "Rewrite the sentence below so that it is grammatically correct.",
        "input": "Him and I seen the movie.",
        "output": "He and I saw the movie.",
        "output_2": "The movie was seen by him and me."
    },
    {
        "instruction": "Edit the given text to ensure all plural nouns are spelled correctly.",
        "input": "The boxes was stacked neatly.",
        "output": "The boxes were stacked neatly.",
        "output_2": "The boxes had been stacked neatly."
    },
    {
        "instruction": "Rewrite this English sentence into passive voice.",
        "input": "Alex fixed the car.",
        "output": "The car was fixed by Alex.",
        "output_2": "The car was fixed by Alex."
    },
    {
        "instruction": "Rewrite this sentence in the form of a question.",
        "input": "This is the biggest tree in the park.",
        "output": "Is this the biggest tree in the park?",
        "output_2": "Is this tree considered the biggest in the park?"
    },
    {
        "instruction": "Edit the text so that it is grammatically correct.",
        "input": "He don't like going there",
        "output": "He doesn't like going there.",
        "output_2": "Going there is not liked by him."
    },
    {
        "instruction": "Identify the hypernym for the word \"car\".",
        "input": "",
        "output": "The hypernym for the word \"car\" is \"vehicle\".",
        "output_2": "The hypernym for the word \"vehicle\" is \"car\"."
    },
    {
        "instruction": "Construct a sentence using the words \"achieve\", \"goal\", and \"perseverance\".",
        "input": "",
        "output": "With perseverance, you can achieve any goal.",
        "output_2": "With perseverance, any goal can be achieved by you."
    },
    {
        "instruction": "Rewrite this statement as an imperative sentence.",
        "input": "You should check the document for errors.",
        "output": "Check the document for errors.",
        "output_2": "The document should be checked for errors."
    },
    {
        "instruction": "Capitalize each word in the sentence.",
        "input": "i love watching the sunrise",
        "output": "I Love Watching The Sunrise",
        "output_2": "The sunrise is loved to be watched by me."
    },
    {
        "instruction": "Create a new sentence by combining the two sentences.",
        "input": "He likes to swim. He goes to the pool every day.",
        "output": "He likes to swim and goes to the pool every day.",
        "output_2": "Swimming is liked by him, and the pool is gone to by him every day."
    },
    {
        "instruction": "Combine the two ideas in the sentence.",
        "input": "Learning a new language is useful. It can be challenging.",
        "output": "Learning a new language is useful but can be challenging.",
        "output_2": "A new language being learned is useful but can be challenging."
    },
    {
        "instruction": "Reword the sentence to use other words without changing the meaning.",
        "input": "It rained heavily during the night.",
        "output": "The night experienced significant rainfall.",
        "output_2": "Significant rainfall was experienced by the night."
    },
    {
        "instruction": "Rewrite the sentence to use a negative adverb.",
        "input": "She always remembers to call.",
        "output": "She never forgets to call.",
        "output_2": "It is never forgotten by her to call."
    },
    {
        "instruction": "Categorize the given list of animals.",
        "input": "Shark, Dolphin, Trout",
        "output": "Fish: Shark, Trout\nMammals: Dolphin",
        "output_2": "Fish: Shark, Trout are listed.\nMammals: Dolphin is listed."
    },
    {
        "instruction": "Rewrite the sentence using a synonym.",
        "input": "It's cold outside.",
        "output": "It's chilly outside.",
        "output_2": "It is considered chilly outside."
    },
    {
        "instruction": "Change the given sentence into passive voice.",
        "input": "They celebrate the festival annually.",
        "output": "The festival is celebrated annually.",
        "output_2": "The festival is celebrated annually."
    },
    {
        "instruction": "Generate a question with the keyword \u201cbacteria\u201d",
        "input": "",
        "output": "How do bacteria affect human health?",
        "output_2": "How is human health affected by bacteria?"
    },
    {
        "instruction": "Take the following noun and turn it into a verb.",
        "input": "Light",
        "output": "To light.",
        "output_2": "To be lit."
    },
    {
        "instruction": "Evaluate the following phrase by transforming it into the spelling given.",
        "input": "freind --> friend",
        "output": "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".",
        "output_2": "The spelling of the given phrase \"freind\" is identified as incorrect; the correct spelling is identified as \"friend\"."
    },
    {
        "instruction": "Select the noun phrase in the following sentence.",
        "input": "He enjoyed a delicious cake at the party.",
        "output": "The noun phrases in the given sentence are \"a delicious cake\" and \"the party.\"",
        "output_2": "The noun phrases \"a delicious cake\" and \"the party\" are identified in the given sentence."
    },
    {
        "instruction": "Rewrite the following phrase in the present tense.",
        "input": "They had played soccer all afternoon.",
        "output": "They play soccer all afternoon.",
        "output_2": "Soccer is played by them all afternoon."
    },
    {
        "instruction": "Rewrite the following phrase as a question.",
        "input": "We must finish the project by tomorrow.",
        "output": "Must we finish the project by tomorrow?",
        "output_2": "Must the project be finished by us by tomorrow?"
    },
    {
        "instruction": "Transform the sentence into a negative statement.",
        "input": "She always arrives on time.",
        "output": "She never arrives late.",
        "output_2": "Late is never arrived at by her."
    },
    {
        "instruction": "Suggest a simpler word to replace the word in braces.",
        "input": "He moved {expeditiously} through the crowd.",
        "output": "He moved quickly through the crowd.",
        "output_2": "Quickly, the crowd was moved through by him."
    },
    {
        "instruction": "Generate a five-word phrase using the words \"moon,\" \"crisp,\" and \"night.\"",
        "input": "",
        "output": "\"Crisp moon illuminates the night.\"",
        "output_2": "The night is illuminated by the crisp moon."
    },
    {
        "instruction": "Describe the color blue in 3 words.",
        "input": "",
        "output": "Deep, calming, cool.",
        "output_2": "Deeply calmed, cooled."
    },
    {
        "instruction": "Classify the following species as dogs or cats.",
        "input": "Siamese",
        "output": "Siamese is a cat.",
        "output_2": "A cat is Siamese."
    },
    {
        "instruction": "Classify an input string as either a noun or a verb.",
        "input": "jump",
        "output": "\"jump\" can be classified as a verb.",
        "output_2": "\"Jump\" can be classified as a verb."
    },
    {
        "instruction": "Insert the correct punctuation at the provided places in the following sentence.",
        "input": "Where are you going",
        "output": "Where are you going?",
        "output_2": "Where is it being gone to by you?"
    },
    {
        "instruction": "Convert 10 feet to meters.",
        "input": "",
        "output": "10 feet is equal to approximately 3.048 meters.",
        "output_2": "Approximately 3.048 meters is equal to 10 feet."
    },
    {
        "instruction": "Simplify the given sentence.",
        "input": "The teacher explained that the reason we were studying this was because it is important.",
        "output": "The teacher explained that we were studying this because it's important.",
        "output_2": "It was explained by the teacher that this was being studied by us because it's important."
    },
    {
        "instruction": "Determine whether the given two words are synonyms or antonyms.",
        "input": "Open - Close",
        "output": "Open and Close are antonyms.",
        "output_2": "It is acknowledged that Open and Close are antonyms."
    },
    {
        "instruction": "Change this sentence grammatically.",
        "input": "They has seen the movie.",
        "output": "They have seen the movie.",
        "output_2": "The movie has been seen by them."
    },
    {
        "instruction": "Generate a past-tense verb that describes a person singing.",
        "input": "",
        "output": "Sang.",
        "output_2": "Was sung."
    },
    {
        "instruction": "Find the synonym for the given verb.",
        "input": "Begin",
        "output": "Commence",
        "output_2": "The following text is to be converted to passive voice: Commence."
    },
    {
        "instruction": "Name 3 different animals that are active during the day.",
        "input": "",
        "output": "1. Squirrel\n2. Eagle\n3. Tiger",
        "output_2": "1. Squirrel is chased.\n2. Eagle is seen.\n3. Tiger is admired."
    },
    {
        "instruction": "Fill in the blanks with appropriate words.",
        "input": "The sky was filled with colorful ________ at sunset.",
        "output": "The sky was filled with colorful clouds at sunset.",
        "output_2": "Colorful clouds filled the sky at sunset."
    },
    {
        "instruction": "Categorize the following items into one of the following categories: appliance, clothing, or food.",
        "input": "Microwave, T-shirt, Bread",
        "output": "Appliance: Microwave\nClothing: T-shirt\nFood: Bread",
        "output_2": "Appliance: Microwave is used.\nClothing: T-shirt is worn.\nFood: Bread is eaten."
    },
    {
        "instruction": "Edit the sentence: \"We enjoys watching movies\"",
        "input": "",
        "output": "We enjoy watching movies.",
        "output_2": "Movies are enjoyed by us."
    },
    {
        "instruction": "Edit the given sentence to make it grammatically correct.",
        "input": "She dont have any money",
        "output": "She doesn't have any money.",
        "output_2": "Any money isn't had by her."
    },
    {
        "instruction": "Classify the following animal as carnivore, herbivore, or omnivore.",
        "input": "Deer",
        "output": "Herbivore",
        "output_2": "I'm sorry, but the word \"Herbivore\" does not form a sentence and thus cannot be converted into passive voice. Passive voice requires a sentence with a clear subject, verb, and object, which is not applicable to a single noun."
    },
    {
        "instruction": "Fix the following grammar mistake.",
        "input": "He want to go to the movies, but he has no money.",
        "output": "He wants to go to the movies, but he has no money.",
        "output_2": "It is wanted by him to go to the movies, but no money is had by him."
    },
    {
        "instruction": "Automatically correct the given sentence.",
        "input": "I want to go to london.",
        "output": "I want to go to London.",
        "output_2": "London is wanted to be gone to by me."
    },
    {
        "instruction": "Generate a random sequence of 10 digits.",
        "input": "",
        "output": "Here is a randomly generated sequence of 10 digits: 9274036185.",
        "output_2": "A randomly generated sequence of 10 digits, 9274036185, is presented here."
    },
    {
        "instruction": "Generate an antonym for the word 'optimistic'.",
        "input": "",
        "output": "An antonym for the word 'optimistic' could be 'pessimistic'.",
        "output_2": "An antonym for the word 'optimistic' could be considered 'pessimistic'."
    },
    {
        "instruction": "Edit the text for grammar and punctuation.",
        "input": "today was amazing I went hiking and saw a bear",
        "output": "\"Today was amazing. I went hiking and saw a bear.\"",
        "output_2": "Today was amazing. Hiking was gone on by me and a bear was seen."
    },
    {
        "instruction": "Rewrite this sentence to eliminate the passive voice.",
        "input": "The cake was made by Susan.",
        "output": "Susan made the cake.",
        "output_2": "The cake was made by Susan."
    },
    {
        "instruction": "Sort the following array in ascending order.",
        "input": "[9, 2, 7, 4]",
        "output": "Here is the sorted array in ascending order: [2, 4, 7, 9].",
        "output_2": "The sorted array in ascending order is presented here: [2, 4, 7, 9]."
    },
    {
        "instruction": "Add the correct verb to the sentence:\nThe students _________ the quiz answers.",
        "input": "",
        "output": "The students discussed the quiz answers.",
        "output_2": "The quiz answers were discussed by the students."
    },
    {
        "instruction": "Put the following words in alphabetical order: kitten, zebra, alligator.",
        "input": "",
        "output": "Alligator, kitten, zebra.",
        "output_2": "Alligator, kitten, zebra."
    },
    {
        "instruction": "Delete the redundant words in the following sentence:\nWe went to see to the mountains.",
        "input": "",
        "output": "We went to see the mountains.",
        "output_2": "The mountains were gone to see by us."
    },
    {
        "instruction": "Rewrite this sentence using appropriate grammar rules:",
        "input": "he saw a old man",
        "output": "He saw an old man.",
        "output_2": "An old man was seen by him."
    },
    {
        "instruction": "Find the next 3 terms in the following sequence: 3, 5, 7, 9",
        "input": "",
        "output": "The next three terms in the sequence are: 11, 13, 15.",
        "output_2": "The next three terms in the sequence are identified as: 11, 13, 15."
    },
    {
        "instruction": "Reverse the order of the given phrase.",
        "input": "sun and moon",
        "output": "moon and sun",
        "output_2": "The moon and the sun are referred to."
    }
]
</file>

<file path="ch07/02_dataset-utilities/instruction-examples.json">
[
    {
        "instruction": "Identify the verb in the following sentence: The cat sleeps on the couch.",
        "input": "",
        "output": "The verb in the sentence is \"sleeps.\""
    },
    {
        "instruction": "What is the plural form of \"goose\"?",
        "input": "",
        "output": "The plural form of \"goose\" is \"geese.\""
    },
    {
        "instruction": "List three primary colors.",
        "input": "",
        "output": "The three primary colors are red, blue, and yellow."
    },
    {
        "instruction": "Change the following sentence to past perfect tense.",
        "input": "They finish the game.",
        "output": "They had finished the game."
    },
    {
        "instruction": "What is the abbreviation for \"Doctor of Philosophy\"?",
        "input": "",
        "output": "The abbreviation for \"Doctor of Philosophy\" is Ph.D."
    },
    {
        "instruction": "Correct the misspelled word in the following sentence: I recieved your message.",
        "input": "",
        "output": "The misspelled word \"recieved\" should be spelled \"received.\""
    },
    {
        "instruction": "Transform the following sentence to include an exclamation mark to show excitement.",
        "input": "I won the lottery",
        "output": "I won the lottery!"
    },
    {
        "instruction": "What is the antonym of \"ancient\"?",
        "input": "",
        "output": "The antonym of \"ancient\" is \"modern.\""
    },
    {
        "instruction": "Name a synonym for \"happiness.\"",
        "input": "",
        "output": "A synonym for \"happiness\" is \"joy.\""
    },
    {
        "instruction": "Classify the following items: gold ring, daisy, python.",
        "input": "",
        "output": "Animals: Python\nPlants: Daisy\nMinerals: Gold ring"
    },
    {
        "instruction": "Generate three adjectives to describe a mountain.",
        "input": "",
        "output": "1. Majestic\n2. Rugged\n3. Towering"
    },
    {
        "instruction": "Determine the state of matter for helium at room temperature.",
        "input": "",
        "output": "Helium is in a gaseous state at room temperature."
    },
    {
        "instruction": "Rewrite the following sentence to use a simile: The car raced down the road.",
        "input": "",
        "output": "The car raced down the road like a bullet."
    },
    {
        "instruction": "What is the symbol for the chemical element iron?",
        "input": "",
        "output": "The symbol for the chemical element iron is Fe."
    },
    {
        "instruction": "Convert the sentence into a question by adding a question tag.",
        "input": "You will help me",
        "output": "You will help me, won't you?"
    },
    {
        "instruction": "List two synonyms for \"intelligent.\"",
        "input": "",
        "output": "Two synonyms for \"intelligent\" are \"smart\" and \"clever.\""
    },
    {
        "instruction": "Name the process by which plants lose water vapor through their leaves.",
        "input": "",
        "output": "The process by which plants lose water vapor through their leaves is called transpiration."
    },
    {
        "instruction": "Provide a word that rhymes with \"care.\"",
        "input": "",
        "output": "A word that rhymes with \"care\" is \"fare.\""
    },
    {
        "instruction": "Rewrite this sentence to start with \"Despite\": She managed to finish her work, although she was tired.",
        "input": "",
        "output": "Despite being tired, she managed to finish her work."
    },
    {
        "instruction": "Convert the following sentence to use a relative clause: The man is a teacher. He lives next door.",
        "input": "",
        "output": "The man who lives next door is a teacher."
    },
    {
        "instruction": "Arrange the following events in chronological order: Invention of the airplane, Fall of the Berlin Wall, Discovery of America.",
        "input": "",
        "output": "1. Discovery of America\n2. Invention of the airplane\n3. Fall of the Berlin Wall"
    },
    {
        "instruction": "Name three types of clouds.",
        "input": "",
        "output": "Three types of clouds are cumulus, stratus, and cirrus."
    },
    {
        "instruction": "Create a compound word using \"book.\"",
        "input": "",
        "output": "A compound word using \"book\" is \"bookshelf.\""
    },
    {
        "instruction": "Identify the main subject in the sentence: The noisy children played in the park.",
        "input": "",
        "output": "The main subject in the sentence is \"The noisy children.\""
    },
    {
        "instruction": "Explain what a haiku is.",
        "input": "",
        "output": "A haiku is a form of traditional Japanese poetry that consists of three lines with a syllable pattern of 5-7-5."
    },
    {
        "instruction": "Provide the past tense form of the verb \"run.\"",
        "input": "",
        "output": "The past tense of the verb \"run\" is \"ran.\""
    },
    {
        "instruction": "List the ingredients required to make a basic salad.",
        "input": "",
        "output": "Ingredients for a basic salad typically include lettuce, tomatoes, cucumbers, and dressing."
    },
    {
        "instruction": "What is the capital of Australia?",
        "input": "",
        "output": "The capital of Australia is Canberra."
    },
    {
        "instruction": "Name a punctuation mark used to indicate possession.",
        "input": "",
        "output": "A punctuation mark used to indicate possession is the apostrophe (')."
    },
    {
        "instruction": "Name an antonym for \"joy.\"",
        "input": "",
        "output": "An antonym for \"joy\" is \"sorrow.\""
    },
    {
        "instruction": "Classify the following items: apple, diamond, fern.",
        "input": "",
        "output": "Plants: Fern\nMinerals: Diamond\nFruits: Apple"
    },
    {
        "instruction": "Generate three adjectives to describe the ocean.",
        "input": "",
        "output": "1. Vast\n2. Mysterious\n3. Serene"
    },
    {
        "instruction": "Determine the state of matter for nitrogen at room temperature.",
        "input": "",
        "output": "Nitrogen is in a gaseous state at room temperature."
    },
    {
        "instruction": "Convert the following statement into a question.",
        "input": "You were at the concert last night.",
        "output": "Were you at the concert last night?"
    },
    {
        "instruction": "Arrange these historical periods in chronological order: Renaissance, Roman Empire, Victorian Era.",
        "input": "",
        "output": "1. Roman Empire\n2. Renaissance\n3. Victorian Era"
    },
    {
        "instruction": "Suggest a more formal synonym for \"happy.\"",
        "input": "",
        "output": "A more formal synonym for \"happy\" is \"content.\""
    },
    {
        "instruction": "Evaluate this sentence for spelling and grammar mistakes.",
        "input": "I recieved an invitation to an exciting evenement.",
        "output": "There are two errors in the sentence. The corrected sentence should be: \"I received an invitation to an exciting event.\""
    },
    {
        "instruction": "What is the capital of Germany?",
        "input": "",
        "output": "The capital city of Germany is Berlin."
    },
    {
        "instruction": "Classify the following into animals, plants, and metals.",
        "input": "Rose, gold, cat",
        "output": "Animals: Cat\nPlants: Rose\nMetals: Gold"
    },
    {
        "instruction": "Generate three verbs that mean the same as \"to run.\"",
        "input": "",
        "output": "1. To sprint\n2. To dash\n3. To jog"
    },
    {
        "instruction": "Look up the melting point of iron.",
        "input": "",
        "output": "The melting point of iron is approximately 1538 degrees Celsius or 2800 degrees Fahrenheit."
    },
    {
        "instruction": "Rewrite the following sentence using passive voice.",
        "input": "The team achieved great results.",
        "output": "Great results were achieved by the team."
    },
    {
        "instruction": "Edit the following sentence to make it more formal.",
        "input": "He's got to see it to believe it.",
        "output": "He must see it to believe it."
    },
    {
        "instruction": "Arrange the given numbers in descending order.",
        "input": "7, 10, 3, 6, 9",
        "output": "10, 9, 7, 6, 3."
    },
    {
        "instruction": "Given a sentence, convert it into indirect speech.",
        "input": "She said, \"I am tired.\"",
        "output": "She said that she was tired."
    },
    {
        "instruction": "Transform the following sentence into the passive voice.",
        "input": "They will build a new bridge.",
        "output": "A new bridge will be built by them."
    },
    {
        "instruction": "What is the capital of Italy?",
        "input": "",
        "output": "The capital of Italy is Rome."
    },
    {
        "instruction": "Generate a random alphanumeric code of 10 characters.",
        "input": "",
        "output": "Here is a randomly generated 10-character alphanumeric code: 3F8G6H7JQ2."
    },
    {
        "instruction": "Identify the correct spelling of the following word.",
        "input": "Necesary",
        "output": "The correct spelling is \"Necessary\"."
    },
    {
        "instruction": "Change this sentence into the past continuous tense.",
        "input": "She sings in the choir.",
        "output": "She was singing in the choir."
    },
    {
        "instruction": "Make the first sentence a dependent clause.",
        "input": "I finished my work. I went home.",
        "output": "After finishing my work, I went home."
    },
    {
        "instruction": "Rewrite the sentence in future perfect tense.",
        "input": "They will complete the project.",
        "output": "They will have completed the project."
    },
    {
        "instruction": "Convert the given sentence into active voice.",
        "input": "The law was passed by the government.",
        "output": "The government passed the law."
    },
    {
        "instruction": "Transform the following sentence into a question using \"could.\"",
        "input": "You can help me tomorrow.",
        "output": "Could you help me tomorrow?"
    },
    {
        "instruction": "Correct the grammatical errors in this sentence.",
        "input": "He don't know nothing about it.",
        "output": "He doesn't know anything about it."
    },
    {
        "instruction": "Rewrite the following sentence to remove redundancy.",
        "input": "The actual fact is that he was late.",
        "output": "The fact is that he was late."
    },
    {
        "instruction": "Rewrite the following sentence so that it is in active voice.",
        "input": "The cookies were eaten by the children.",
        "output": "The children ate the cookies."
    },
    {
        "instruction": "What is the contraction for \"will not\"?",
        "input": "",
        "output": "The contraction for \"will not\" is \"won't\"."
    },
    {
        "instruction": "Generate a sentence that follows the pattern: \"Never have I ever _____ without _____\"",
        "input": "",
        "output": "Never have I ever traveled without a map."
    },
    {
        "instruction": "Classify this text as a technical document or a narrative.",
        "input": "This manual provides instructions for installing the software.",
        "output": "Technical document"
    },
    {
        "instruction": "Sort the following list in alphabetical order.",
        "input": "Zebra, Elephant, Crocodile",
        "output": "sorted in alphabetical order: Crocodile, Elephant, Zebra"
    },
    {
        "instruction": "Name the three main types of rocks.",
        "input": "",
        "output": "The three main types of rocks are igneous, sedimentary, and metamorphic."
    },
    {
        "instruction": "Edit the following sentence for grammar.",
        "input": "We goes to the park every Sunday.",
        "output": "We go to the park every Sunday."
    },
    {
        "instruction": "Find a rhyming word for the word \"care.\"",
        "input": "",
        "output": "A rhyming word for the word \"care\" could be \"bare.\""
    },
    {
        "instruction": "Reword the following sentence to the future tense.",
        "input": "He is reading a novel inspired by his grandmother.",
        "output": "He will be reading a novel inspired by his grandmother."
    },
    {
        "instruction": "Classify the following fruit according to its type.",
        "input": "Watermelon",
        "output": "The fruit \"Watermelon\" is typically classified as a type of melon."
    },
    {
        "instruction": "Suggest a continuation for the sentence: \"When the sun set,\"",
        "input": "",
        "output": "\"the sky turned a brilliant shade of orange, casting a warm glow over the city.\""
    },
    {
        "instruction": "Classify the given sentiment as either positive, negative, or neutral.",
        "input": "This is a great opportunity!",
        "output": "Positive"
    },
    {
        "instruction": "Generate a question that could logically follow the statement below.",
        "input": "I just returned from a vacation in Spain.",
        "output": "How was your vacation in Spain?"
    },
    {
        "instruction": "Create a sentence that begins with \"If only,\"",
        "input": "",
        "output": "If only I had known the truth earlier..."
    },
    {
        "instruction": "Correct any spelling mistakes in the given sentence.",
        "input": "I prefer homemade cookies to store boaght.",
        "output": "I prefer homemade cookies to store bought."
    },
    {
        "instruction": "Edit the following sentence for clarity and style.",
        "input": "He response to the issue was quick.",
        "output": "His response to the issue was quick."
    },
    {
        "instruction": "Name three essential elements found in nature.",
        "input": "",
        "output": "1. Carbon\n2. Oxygen\n3. Iron"
    },
    {
        "instruction": "Name a dwarf planet in our solar system.",
        "input": "",
        "output": "One dwarf planet in our solar system is Pluto."
    },
    {
        "instruction": "Name a dwarf planet in our solar system.",
        "input": "",
        "output": "One dwarf planet in our solar system is Pluto."
    },
    {
        "instruction": "Classify the following words by their grammatical categories: swim, beautiful, quickly",
        "input": "",
        "output": "Swim: Verb\nBeautiful: Adjective\nQuickly: Adverb"
    },
    {
        "instruction": "Pick out the correct verb from the following list.",
        "input": "run, mountain, quickly",
        "output": "The correct verb from the list is \"run.\""
    },
    {
        "instruction": "Combine the two sentences into a single coherent sentence.",
        "input": "She did not attend the meeting. She was ill.",
        "output": "She did not attend the meeting because she was ill."
    },
    {
        "instruction": "Classify the item as either a grain or a fruit.",
        "input": "Wheat",
        "output": "Grain"
    },
    {
        "instruction": "Classify the following statement into one of these labels: [positive, negative, neutral]",
        "input": "My computer crashed.",
        "output": "The statement \"My computer crashed\" can be classified as negative."
    },
    {
        "instruction": "Rewrite the following sentence in the active voice",
        "input": "The meeting will be held by the board.",
        "output": "The board will hold the meeting."
    },
    {
        "instruction": "Re-word this sentence using an active voice.",
        "input": "The decision was made by the team.",
        "output": "The team made the decision."
    },
    {
        "instruction": "Change the given verb to its present perfect form.",
        "input": "go",
        "output": "Gone."
    },
    {
        "instruction": "Perform a sentiment analysis on the given sentence.",
        "input": "I hate this song!",
        "output": "The sentiment of the sentence \"I hate this song!\" is negative. This is indicated by the word \"hate\" which shows strong aversion or dislike towards the subject, in this case, the song."
    },
    {
        "instruction": "Find the temperature of boiling water under normal atmospheric pressure.",
        "input": "",
        "output": "Under normal atmospheric pressure, the temperature of boiling water is 100 degrees Celsius (212 degrees Fahrenheit)."
    },
    {
        "instruction": "Edit the following sentence to correct any grammar errors.",
        "input": "They was trying to fix the car all day.",
        "output": "They were trying to fix the car all day."
    },
    {
        "instruction": "List 5 different types of vegetables",
        "input": "",
        "output": "1. Carrot\n2. Broccoli\n3. Cucumber\n4. Tomato (biologically a fruit, but often classified as a vegetable)\n5. Spinach"
    },
    {
        "instruction": "Turn the following statement into a question.",
        "input": "The Golden State Warriors are a professional basketball team based in San Francisco.",
        "output": "Are the Golden State Warriors a professional basketball team based in San Francisco?"
    },
    {
        "instruction": "Fill in the blank to complete the sentence:\nThe most popular fruit in the world is the ____",
        "input": "",
        "output": "The most popular fruit in the world is the banana."
    },
    {
        "instruction": "Generate a question based on the following statement.",
        "input": "Plants produce oxygen through the process of photosynthesis.",
        "output": "How do plants produce oxygen?"
    },
    {
        "instruction": "Correct any mistakes in the following sentence.",
        "input": "Everyones going to the game tomorrow.",
        "output": "Everyone's going to the game tomorrow."
    },
    {
        "instruction": "Rewrite the following sentence by adding one adverb.",
        "input": "The cat slept on the mat.",
        "output": "The cat peacefully slept on the mat."
    },
    {
        "instruction": "Identify the tense used in the sentence.",
        "input": "They are watching a movie.",
        "output": "The tense used in the sentence \"They are watching a movie\" is the present continuous tense."
    },
    {
        "instruction": "Categorize the following sentence as either factual or opinion-based.",
        "input": "Chocolate is the best dessert.",
        "output": "Opinion-based."
    },
    {
        "instruction": "Classify each sentence as either declarative, interrogative, imperative, or exclamatory.",
        "input": "Can you help me?",
        "output": "The classification of the sentence \"Can you help me?\" is interrogative."
    },
    {
        "instruction": "Reverse this array",
        "input": "[10, 20, 30, 40, 50]",
        "output": "Here is the reversed array: [50, 40, 30, 20, 10]"
    },
    {
        "instruction": "What is the chemical formula for salt?",
        "input": "",
        "output": "The chemical formula for salt (sodium chloride) is NaCl."
    },
    {
        "instruction": "Rewrite this sentence in the present continuous tense.",
        "input": "They play football every Sunday.",
        "output": "They are playing football."
    },
    {
        "instruction": "Categorize the following items into herbs and spices.",
        "input": "Cinnamon, Basil, Pepper",
        "output": "Herbs: Basil\nSpices: Cinnamon, Pepper"
    },
    {
        "instruction": "Cut the following sentence into two, keeping the original meaning intact.",
        "input": "She started her homework but was soon distracted by a phone call.",
        "output": "She started her homework. However, she was soon distracted by a phone call."
    },
    {
        "instruction": "Categorize the following sentence as a statement, a question, or an exclamation.",
        "input": "What a beautiful day!",
        "output": "Exclamation."
    },
    {
        "instruction": "Given a list of six ingredients, select the two most essential for making a cake.",
        "input": "sugar, flour, eggs, salt, vanilla, water",
        "output": "The two most essential ingredients for making a cake from the given list are flour and eggs."
    },
    {
        "instruction": "Generate a sentence that uses the word ‘mystic’.",
        "input": "",
        "output": "The ancient ruins had a mystic quality that captivated all who visited."
    },
    {
        "instruction": "Change the sentences from active voice to passive voice.",
        "input": "The chef will prepare the meal.",
        "output": "The meal will be prepared by the chef."
    },
    {
        "instruction": "Transform the given statement into an imperative sentence.",
        "input": "I would like you to turn off the light.",
        "output": "Please turn off the light."
    },
    {
        "instruction": "Edit the sentence to make it more formal.",
        "input": "You're gonna want to see this.",
        "output": "You will want to observe this."
    },
    {
        "instruction": "Give me a synonym for the word 'strong'.",
        "input": "",
        "output": "Powerful."
    },
    {
        "instruction": "Rewrite the following sentence in a more formal way:\n\"I need to find out what's wrong.\"",
        "input": "",
        "output": "I must ascertain what is incorrect."
    },
    {
        "instruction": "Generate a humorous anecdote.",
        "input": "",
        "output": "Why was the math book sad? Because it had too many problems!"
    },
    {
        "instruction": "Change the sentence from passive to active voice.",
        "input": "The song was sung by the choir.",
        "output": "The choir sang the song."
    },
    {
        "instruction": "Label whether the following statements are true or false.",
        "input": "The moon is a planet.",
        "output": "False"
    },
    {
        "instruction": "Combine the given sentences into one cohesive sentence.",
        "input": "It is raining. I cannot go to the park.",
        "output": "Since it is raining, I cannot go to the park."
    },
    {
        "instruction": "Rewrite the given passage in the past tense.",
        "input": "They are building a new library.",
        "output": "They built a new library."
    },
    {
        "instruction": "Change the following sentence so that it is in the past tense.",
        "input": "He runs to the store.",
        "output": "He ran to the store."
    },
    {
        "instruction": "Edit the following sentence to remove all passive voice.",
        "input": "The song was composed by the artist.",
        "output": "The artist composed the song."
    },
    {
        "instruction": "Edit the sentence below to remove any redundant words and make it more concise.",
        "input": "I actually think that",
        "output": "I think."
    },
    {
        "instruction": "Edit the following sentence to remove any redundant words.",
        "input": "My personal belief is that it could potentially be dangerous.",
        "output": "I believe it could be dangerous."
    },
    {
        "instruction": "Sort the following numbers in ascending order.",
        "input": "5, 18, 3, 11.",
        "output": "The numbers in ascending order would be: 3, 5, 11, 18."
    },
    {
        "instruction": "Classify this speech as either formal or informal.",
        "input": "Could you please pass the salt?",
        "output": "This speech is formal."
    },
    {
        "instruction": "Classify the following online review as being positive or negative.",
        "input": "I found this book to be incredibly insightful and informative.",
        "output": "The online review is positive."
    },
    {
        "instruction": "Reverse the following list alphabetically.",
        "input": "apple, orange, banana, melon",
        "output": "The reverse alphabetical order of the list is: orange, melon, banana, apple."
    },
    {
        "instruction": "Rearrange the following list into descending order.",
        "input": "5, -3, 20, 10",
        "output": "20, 10, 5, -3"
    },
    {
        "instruction": "Convert the phrase into a complete sentence.",
        "input": "night sky",
        "output": "The night sky was dotted with countless twinkling stars."
    },
    {
        "instruction": "Classify the following items as either solid, liquid, or gas.",
        "input": "Mercury, oxygen, wood",
        "output": "Mercury - Liquid\nOxygen - Gas\nWood - Solid"
    },
    {
        "instruction": "Identify the correct word to complete the sentence.",
        "input": "She presented her ideas with _____.",
        "output": "clarity"
    },
    {
        "instruction": "Insert a suitable adjective to complete the sentence.",
        "input": "The movie was _____.",
        "output": "The movie was thrilling."
    },
    {
        "instruction": "Alter the content of the sentence to use the past tense.",
        "input": "The ship sails at dawn.",
        "output": "The ship sailed at dawn."
    },
    {
        "instruction": "Re-write the following sentence to use a different verb.",
        "input": "I will help you with your homework.",
        "output": "I will assist you with your homework."
    },
    {
        "instruction": "Pick out the odd word out in each sentence:",
        "input": "Pen, Book, Tree",
        "output": "The odd word out in the sentence \"Pen, Book, Tree\" is \"Tree\"."
    },
    {
        "instruction": "Classify the following text into either fiction or non-fiction.",
        "input": "The documentary covers the impact of climate change on polar bears.",
        "output": "Non-fiction."
    },
    {
        "instruction": "What type of number is 0?",
        "input": "",
        "output": "0 is an integer."
    },
    {
        "instruction": "Split the following sentence into two declarative sentences: \"The book is interesting but the cover is dull\".",
        "input": "",
        "output": "The book is interesting.\nThe cover is dull."
    },
    {
        "instruction": "Classify this sentence into one of the following topics: health, economy, technology, music",
        "input": "Spotify has revolutionized how we listen to music.",
        "output": "The topic of the given sentence is music."
    },
    {
        "instruction": "Generate a question from the following statement",
        "input": "Leonardo da Vinci painted the Mona Lisa.",
        "output": "Who painted the Mona Lisa?"
    },
    {
        "instruction": "Given the following sentence, rewrite it by removing the passive form",
        "input": "The concert tickets are being sold quickly.",
        "output": "They are selling the concert tickets quickly."
    },
    {
        "instruction": "Given a sentence, count the number of words.",
        "input": "The quick brown fox jumps over the lazy dog.",
        "output": "The sentence \"The quick brown fox jumps over the lazy dog.\" contains 9 words."
    },
    {
        "instruction": "Rewrite the given sentence to describe the same thing in a positive way.",
        "input": "The meal was not delicious.",
        "output": "The meal could use some improvement."
    },
    {
        "instruction": "Rewrite the sentence so that it does not contain any passive voice.",
        "input": "The song was played on the radio.",
        "output": "The radio played the song."
    },
    {
        "instruction": "Find a synonym for the word 'start'.",
        "input": "",
        "output": "A synonym for the word 'start' could be \"commence.\""
    },
    {
        "instruction": "Change the sentence \"This should not have happened\" into a declarative sentence.",
        "input": "",
        "output": "This should have happened differently."
    },
    {
        "instruction": "Rewrite the sentence: \"The poem was composed in a lyrical manner.\"",
        "input": "",
        "output": "The poem was crafted lyrically."
    },
    {
        "instruction": "Rewrite the following sentence to replace any cliché phrases.",
        "input": "It's a piece of cake",
        "output": "It's very easy."
    },
    {
        "instruction": "Pick the best synonym for the given word.",
        "input": "Gather",
        "output": "Collect."
    },
    {
        "instruction": "Rewrite the sentence using an idiom.",
        "input": "The storm started suddenly.",
        "output": "The storm came out of the blue."
    },
    {
        "instruction": "Scramble the given phrase.",
        "input": "A random phrase",
        "output": "A nmaodr hrasep"
    },
    {
        "instruction": "rewrite this sentence as a question.",
        "input": "Your brother went to London last year.",
        "output": "Did your brother go to London last year?"
    },
    {
        "instruction": "Rewrite the sentence below so that it is grammatically correct.",
        "input": "Him and I seen the movie.",
        "output": "He and I saw the movie."
    },
    {
        "instruction": "Edit the given text to ensure all plural nouns are spelled correctly.",
        "input": "The boxes was stacked neatly.",
        "output": "The boxes were stacked neatly."
    },
    {
        "instruction": "Rewrite this English sentence into passive voice.",
        "input": "Alex fixed the car.",
        "output": "The car was fixed by Alex."
    },
    {
        "instruction": "Rewrite this sentence in the form of a question.",
        "input": "This is the biggest tree in the park.",
        "output": "Is this the biggest tree in the park?"
    },
    {
        "instruction": "Edit the text so that it is grammatically correct.",
        "input": "He don't like going there",
        "output": "He doesn't like going there."
    },
    {
        "instruction": "Identify the hypernym for the word \"car\".",
        "input": "",
        "output": "The hypernym for the word \"car\" is \"vehicle\"."
    },
    {
        "instruction": "Construct a sentence using the words \"achieve\", \"goal\", and \"perseverance\".",
        "input": "",
        "output": "With perseverance, you can achieve any goal."
    },
    {
        "instruction": "Rewrite this statement as an imperative sentence.",
        "input": "You should check the document for errors.",
        "output": "Check the document for errors."
    },
    {
        "instruction": "Capitalize each word in the sentence.",
        "input": "i love watching the sunrise",
        "output": "I Love Watching The Sunrise"
    },
    {
        "instruction": "Create a new sentence by combining the two sentences.",
        "input": "He likes to swim. He goes to the pool every day.",
        "output": "He likes to swim and goes to the pool every day."
    },
    {
        "instruction": "Combine the two ideas in the sentence.",
        "input": "Learning a new language is useful. It can be challenging.",
        "output": "Learning a new language is useful but can be challenging."
    },
    {
        "instruction": "Reword the sentence to use other words without changing the meaning.",
        "input": "It rained heavily during the night.",
        "output": "The night experienced significant rainfall."
    },
    {
        "instruction": "Rewrite the sentence to use a negative adverb.",
        "input": "She always remembers to call.",
        "output": "She never forgets to call."
    },
    {
        "instruction": "Categorize the given list of animals.",
        "input": "Shark, Dolphin, Trout",
        "output": "Fish: Shark, Trout\nMammals: Dolphin"
    },
    {
        "instruction": "Rewrite the sentence using a synonym.",
        "input": "It's cold outside.",
        "output": "It's chilly outside."
    },
    {
        "instruction": "Change the given sentence into passive voice.",
        "input": "They celebrate the festival annually.",
        "output": "The festival is celebrated annually."
    },
    {
        "instruction": "Generate a question with the keyword “bacteria”",
        "input": "",
        "output": "How do bacteria affect human health?"
    },
    {
        "instruction": "Take the following noun and turn it into a verb.",
        "input": "Light",
        "output": "To light."
    },
    {
        "instruction": "Evaluate the following phrase by transforming it into the spelling given.",
        "input": "freind --> friend",
        "output": "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\"."
    },
    {
        "instruction": "Select the noun phrase in the following sentence.",
        "input": "He enjoyed a delicious cake at the party.",
        "output": "The noun phrases in the given sentence are \"a delicious cake\" and \"the party.\""
    },
    {
        "instruction": "Rewrite the following phrase in the present tense.",
        "input": "They had played soccer all afternoon.",
        "output": "They play soccer all afternoon."
    },
    {
        "instruction": "Rewrite the following phrase as a question.",
        "input": "We must finish the project by tomorrow.",
        "output": "Must we finish the project by tomorrow?"
    },
    {
        "instruction": "Transform the sentence into a negative statement.",
        "input": "She always arrives on time.",
        "output": "She never arrives late."
    },
    {
        "instruction": "Suggest a simpler word to replace the word in braces.",
        "input": "He moved {expeditiously} through the crowd.",
        "output": "He moved quickly through the crowd."
    },
    {
        "instruction": "Generate a five-word phrase using the words \"moon,\" \"crisp,\" and \"night.\"",
        "input": "",
        "output": "\"Crisp moon illuminates the night.\""
    },
    {
        "instruction": "Describe the color blue in 3 words.",
        "input": "",
        "output": "Deep, calming, cool."
    },
    {
        "instruction": "Classify the following species as dogs or cats.",
        "input": "Siamese",
        "output": "Siamese is a cat."
    },
    {
        "instruction": "Classify an input string as either a noun or a verb.",
        "input": "jump",
        "output": "\"jump\" can be classified as a verb."
    },
    {
        "instruction": "Insert the correct punctuation at the provided places in the following sentence.",
        "input": "Where are you going",
        "output": "Where are you going?"
    },
    {
        "instruction": "Convert 10 feet to meters.",
        "input": "",
        "output": "10 feet is equal to approximately 3.048 meters."
    },
    {
        "instruction": "Simplify the given sentence.",
        "input": "The teacher explained that the reason we were studying this was because it is important.",
        "output": "The teacher explained that we were studying this because it's important."
    },
    {
        "instruction": "Determine whether the given two words are synonyms or antonyms.",
        "input": "Open - Close",
        "output": "Open and Close are antonyms."
    },
    {
        "instruction": "Change this sentence grammatically.",
        "input": "They has seen the movie.",
        "output": "They have seen the movie."
    },
    {
        "instruction": "Generate a past-tense verb that describes a person singing.",
        "input": "",
        "output": "Sang."
    },
    {
        "instruction": "Find the synonym for the given verb.",
        "input": "Begin",
        "output": "Commence"
    },
    {
        "instruction": "Name 3 different animals that are active during the day.",
        "input": "",
        "output": "1. Squirrel\n2. Eagle\n3. Tiger"
    },
    {
        "instruction": "Fill in the blanks with appropriate words.",
        "input": "The sky was filled with colorful ________ at sunset.",
        "output": "The sky was filled with colorful clouds at sunset."
    },
    {
        "instruction": "Categorize the following items into one of the following categories: appliance, clothing, or food.",
        "input": "Microwave, T-shirt, Bread",
        "output": "Appliance: Microwave\nClothing: T-shirt\nFood: Bread"
    },
    {
        "instruction": "Edit the sentence: \"We enjoys watching movies\"",
        "input": "",
        "output": "We enjoy watching movies."
    },
    {
        "instruction": "Edit the given sentence to make it grammatically correct.",
        "input": "She dont have any money",
        "output": "She doesn't have any money."
    },
    {
        "instruction": "Classify the following animal as carnivore, herbivore, or omnivore.",
        "input": "Deer",
        "output": "Herbivore"
    },
    {
        "instruction": "Fix the following grammar mistake.",
        "input": "He want to go to the movies, but he has no money.",
        "output": "He wants to go to the movies, but he has no money."
    },
    {
        "instruction": "Automatically correct the given sentence.",
        "input": "I want to go to london.",
        "output": "I want to go to London."
    },
    {
        "instruction": "Generate a random sequence of 10 digits.",
        "input": "",
        "output": "Here is a randomly generated sequence of 10 digits: 9274036185."
    },
    {
        "instruction": "Generate an antonym for the word 'optimistic'.",
        "input": "",
        "output": "An antonym for the word 'optimistic' could be 'pessimistic'."
    },
    {
        "instruction": "Edit the text for grammar and punctuation.",
        "input": "today was amazing I went hiking and saw a bear",
        "output": "\"Today was amazing. I went hiking and saw a bear.\""
    },
    {
        "instruction": "Rewrite this sentence to eliminate the passive voice.",
        "input": "The cake was made by Susan.",
        "output": "Susan made the cake."
    },
    {
        "instruction": "Sort the following array in ascending order.",
        "input": "[9, 2, 7, 4]",
        "output": "Here is the sorted array in ascending order: [2, 4, 7, 9]."
    },
    {
        "instruction": "Add the correct verb to the sentence:\nThe students _________ the quiz answers.",
        "input": "",
        "output": "The students discussed the quiz answers."
    },
    {
        "instruction": "Put the following words in alphabetical order: kitten, zebra, alligator.",
        "input": "",
        "output": "Alligator, kitten, zebra."
    },
    {
        "instruction": "Delete the redundant words in the following sentence:\nWe went to see to the mountains.",
        "input": "",
        "output": "We went to see the mountains."
    },
    {
        "instruction": "Rewrite this sentence using appropriate grammar rules:",
        "input": "he saw a old man",
        "output": "He saw an old man."
    },
    {
        "instruction": "Find the next 3 terms in the following sequence: 3, 5, 7, 9",
        "input": "",
        "output": "The next three terms in the sequence are: 11, 13, 15."
    },
    {
        "instruction": "Reverse the order of the given phrase.",
        "input": "sun and moon",
        "output": "moon and sun"
    }
]
</file>

<file path="ch07/02_dataset-utilities/README.md">
# 第七章：微调以遵循指令

此文件夹包含可用于准备指令数据集的工具代码。

通过以下命令安装额外的包依赖：

```bash
pip install -r requirements-extra.txt
```





### 查找近似重复项

`find-near-duplicates.py` 函数可用于在指令数据集中识别重复项和近似重复项。例如，



```bash
python find-near-duplicates.py --json_file instruction-examples.json
```

```
scikit-learn version: 1.3.1


==================================================
Searching 'instruction' for duplicates ...
==================================================
Duplicate pair found with similarity 0.94:
1. Edit the following sentence to make it more formal.
2. Edit the sentence to make it more formal.

Duplicate pair found with similarity 1.00:
1. Name a dwarf planet in our solar system.
2. Name a dwarf planet in our solar system.

Duplicate pair found with similarity 0.91:
1. Change the sentences from active voice to passive voice.
2. Change the sentence from passive to active voice.



==================================================
Searching 'input' for duplicates ...
==================================================
No duplicates found


==================================================
Searching 'output' for duplicates ...
==================================================
Duplicate pair found with similarity 1.00:
1. One dwarf planet in our solar system is Pluto.
2. One dwarf planet in our solar system is Pluto.


```
&nbsp;
您可以使用 `--threshold` 设置，取值范围为 0 到 1，用于调节灵敏度的高低。
默认阈值为 0.9。

&nbsp;
## 创建被动语态条目

- [create-passive-voice-entries.ipynb](create-passive-voice-entries.ipynb) 笔记本使用 OpenAI 的 GPT-4 为指令数据集创建“被动语态”条目，如下所示：

 ```python
 {  
    'instruction': 'Identify the verb in the following sentence',
    'input': 'The cat sleeps on the couch.',
    'output': 'The verb in the sentence is "sleeps."',
    'output_2': 'The sentence is "sleeps."'   #  <---- Newly created entry
 }  
 ```
</file>

<file path="ch07/02_dataset-utilities/requirements-extra.txt">
openai>=1.30.3
scikit-learn>=1.3.1
tqdm>=4.65.0
</file>

<file path="ch07/03_model-evaluation/scores/correlation-analysis.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bc1c1a-53bc-4b86-9140-4f1af0128037",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250207d-f811-46df-9d16-4ac1e9ce1c66",
   "metadata": {},
   "source": [
    "# Score Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc7ffb-d51c-4de0-97c5-b54cf3e28315",
   "metadata": {},
   "source": [
    "- This notebook analyses the correlation between the different evaluation method scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa39424b-e058-4351-94ec-249b812ae8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"gpt4-model-1-response.json\", \"r\") as file:\n",
    "    gpt4_model_1 = json.load(file)\n",
    "\n",
    "with open(\"llama3-8b-model-1-response.json\", \"r\") as file:\n",
    "    llama3_8b_model_1 = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef67d30-7602-4695-a190-16209a152621",
   "metadata": {},
   "source": [
    "## GPT-4 vs Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0d4288-507f-414c-afde-9742935cd8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "list1, list2 = gpt4_model_1, llama3_8b_model_1\n",
    "\n",
    "plt.scatter(list1, list2)\n",
    "plt.plot(\n",
    "    np.unique(list1),\n",
    "    np.poly1d(np.polyfit(list1, list2, 1))(np.unique(list1))\n",
    ")\n",
    "plt.xlabel(\"GPT-4\")\n",
    "plt.ylabel(\"Llama3 8B\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28b2d2-7f31-4c5f-853b-1e71dc715a25",
   "metadata": {},
   "source": [
    "### Correlation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ef7e9a-1f07-4e94-bdc5-d5271616ef6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Kendall Tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Results</th>\n",
       "      <td>0.80489</td>\n",
       "      <td>0.698406</td>\n",
       "      <td>0.57292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pearson  Spearman  Kendall Tau\n",
       "Results  0.80489  0.698406      0.57292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "pearson_correlation = np.corrcoef(list1, list2)[0, 1]\n",
    "spearman_correlation, _ = spearmanr(list1, list2)\n",
    "kendall_tau_correlation, _ = kendalltau(list1, list2)\n",
    "\n",
    "correlation_table = pd.DataFrame({\n",
    "    \"Pearson\": [pearson_correlation],\n",
    "    \"Spearman\": [spearman_correlation],\n",
    "    \"Kendall Tau\": [kendall_tau_correlation]\n",
    "}, index=['Results'])\n",
    "\n",
    "correlation_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bd708-ba5d-4290-abe3-ee736059c2cd",
   "metadata": {},
   "source": [
    "- For comparison, below are the correlation coefficients from the Prometheus 2 paper by Kim et al. 2024 ([https://arxiv.org/abs/2405.01535](https://arxiv.org/abs/2405.01535)), which are all in the same ballpark as the ones reported for Llama 3 above\n",
    "- Note that Prometheus 2 is a model specifically finetuned for LLM rating and evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc033f4-8a11-42be-a683-6cef7eb23468",
   "metadata": {},
   "source": [
    "#### Pearson\n",
    "\n",
    "| Evaluator LM          | VICUNA Bench | VICUNA Bench  | MT Bench   | MT Bench      | FLASK      | FLASK         | FLASK     | Feedback Bench |\n",
    "|-----------------------|--------------|---------------|------------|---------------|------------|---------------|-----------|----------------|\n",
    "|                       | GPT-4-1106   | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | Humans    | GPT-4-0613     |\n",
    "| LLAMA2-CHAT 7B        | 0.205        | 0.243         | 0.036      | 0.055         | 0.317      | 0.256         | 0.299     | 0.523          |\n",
    "| LLAMA2-CHAT 13B       | 0.185        | 0.141         | -0.042     | -0.002        | 0.239      | 0.247         | 0.263     | 0.545          |\n",
    "| LLAMA2-CHAT 70B       | 0.350        | 0.463         | 0.178      | 0.228         | 0.388      | 0.402         | 0.317     | 0.592          |\n",
    "| MISTRAL-INSTRUCT-7B   | 0.486        | 0.561         | 0.284      | 0.396         | 0.448      | 0.437         | 0.377     | 0.586          |\n",
    "| MIXTRAL-INSTRUCT-8X7B | 0.566        | 0.579         | 0.551      | 0.539         | 0.483      | 0.495         | 0.420     | 0.673          |\n",
    "| **PROMETHEUS-7B**     | **0.484**    | **0.528**     | **0.378**  | **0.382**     | **0.352**  | **0.331**     | **0.348** | **0.847**      |\n",
    "| **PROMETHEUS-13B**    | **0.492**    | **0.534**     | **0.404**  | **0.477**     | **0.462**  | **0.470**     | **0.449** | **0.860**      |\n",
    "| AUTO-J (13B)          | 0.351        | 0.262         | 0.432      | 0.375         | 0.430      | 0.370         | 0.473     | 0.637          |\n",
    "| **PROMETHEUS-2-7B**   | **0.642**    | **0.610**     | **0.543**  | **0.554**     | **0.645**  | **0.578**     | **0.544** | **0.878**      |\n",
    "| **PROMETHEUS-2-8X7B** | **0.685**    | **0.635**     | **0.665**  | **0.614**     | **0.659**  | **0.626**     | **0.555** | **0.898**      |\n",
    "| GPT-3.5-TURBO-0613    | 0.335        | 0.349         | 0.183      | 0.194         | 0.437      | 0.396         | 0.450     | 0.594          |\n",
    "| GPT-4-1106            | /            | 0.694         | /          | 0.717         | /          | 0.736         | 0.679     | 0.753          |\n",
    "| CLAUDE-3-OPUS         | 0.694        | /             | 0.717      | /             | 0.736      | /             | 0.573     | 0.788          |\n",
    "\n",
    "#### Spearman\n",
    "\n",
    "| Evaluator LM          | VICUNA Bench | VICUNA Bench  | MT Bench   | MT Bench      | MT Bench   | FLASK         | FLASK     | Feedback Bench |\n",
    "|-----------------------|--------------|---------------|------------|---------------|------------|---------------|-----------|----------------|\n",
    "|                       | GPT-4-1106   | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | Humans    | GPT-4-0613     |\n",
    "| LLAMA2-CHAT 7B        | 0.236        | 0.255         | 0.084      | 0.089         | 0.301      | 0.244         | 0.279     | 0.511          |\n",
    "| LLAMA2-CHAT 13B       | 0.178        | 0.179         | -0.025     | 0.044         | 0.206      | 0.222         | 0.224     | 0.543          |\n",
    "| LLAMA2-CHAT 70B       | 0.348        | 0.466         | 0.197      | 0.252         | 0.391      | 0.389         | 0.298     | 0.585          |\n",
    "| MISTRAL-INSTRUCT-7B   | 0.389        | 0.480         | 0.266      | 0.358         | 0.499      | 0.478         | 0.374     | 0.563          |\n",
    "| MIXTRAL-INSTRUCT-8X7B | 0.476        | 0.556         | 0.545      | 0.517         | 0.505      | 0.500         | 0.386     | 0.659          |\n",
    "| **PROMETHEUS-7B**     | **0.508**    | **0.528**     | **0.385**  | **0.349**     | **0.367**  | **0.326**     | **0.317** | **0.876**      |\n",
    "| **PROMETHEUS-13B**    | **0.492**    | **0.534**     | **0.401**  | **0.470**     | **0.474**  | **0.454**     | **0.398** | **0.893**      |\n",
    "| AUTO-J (13B)          | 0.337        | 0.297         | 0.408      | 0.365         | 0.402      | 0.358         | 0.408     | 0.623          |\n",
    "| **PROMETHEUS-2-7B**   | **0.643**    | **0.584**     | **0.550**  | **0.524**     | **0.626**  | **0.569**     | **0.490** | **0.909**      |\n",
    "| **PROMETHEUS-2-8X7B** | **0.660**    | **0.615**     | **0.669**  | **0.605**     | **0.642**  | **0.618**     | **0.496** | **0.912**      |\n",
    "| GPT-3.5-TURBO-0613    | 0.319        | 0.354         | 0.192      | 0.198         | 0.446      | 0.390         | 0.374     | 0.565          |\n",
    "| GPT-4-1106            | /            | 0.659         | /          | 0.721         | /          | 0.729         | 0.650     | 0.753          |\n",
    "| CLAUDE-3-OPUS         | 0.659        | /             | 0.721      | /             | 0.729      | /             | 0.567     | 0.784          |\n",
    "\n",
    "#### Kendall-Tau\n",
    "\n",
    "| Evaluator LM          | VICUNA Bench | VICUNA Bench  | MT Bench   | MT Bench      | FLASK      | FLASK         | FLASK     | Feedback Bench |\n",
    "|-----------------------|--------------|---------------|------------|---------------|------------|---------------|-----------|----------------|\n",
    "|                       | GPT-4-1106   | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | GPT-4-1106 | Claude-3-Opus | Humans    | GPT-4-0613     |\n",
    "| LLAMA2-CHAT 7B        | 0.183        | 0.203         | 0.065      | 0.070         | 0.229      | 0.186         | 0.211     | 0.419          |\n",
    "| LLAMA2-CHAT 13B       | 0.145        | 0.146         | -0.019     | 0.037         | 0.160      | 0.174         | 0.174     | 0.453          |\n",
    "| LLAMA2-CHAT 70B       | 0.282        | 0.382         | 0.150      | 0.196         | 0.310      | 0.310         | 0.221     | 0.487          |\n",
    "| MISTRAL-INSTRUCT-7B   | 0.314        | 0.391         | 0.208      | 0.281         | 0.395      | 0.384         | 0.287     | 0.454          |\n",
    "| MIXTRAL-INSTRUCT-8X7B | 0.395        | 0.468         | 0.433      | 0.419         | 0.410      | 0.408         | 0.304     | 0.551          |\n",
    "| **PROMETHEUS-7B**     | **0.405**    | **0.425**     | **0.290**  | **0.263**     | **0.282**  | **0.251**     | **0.236** | **0.770**      |\n",
    "| **PROMETHEUS-13B**    | **0.397**    | **0.434**     | **0.299**  | **0.352**     | **0.365**  | **0.352**     | **0.299** | **0.793**      |\n",
    "| AUTO-J (13B)          | 0.282        | 0.242         | 0.303      | 0.272         | 0.312      | 0.282         | 0.312     | 0.515          |\n",
    "| **PROMETHEUS-2-7B**   | **0.515**    | **0.478**     | **0.458**  | **0.421**     | **0.500**  | **0.454**     | **0.376** | **0.773**      |\n",
    "| **PROMETHEUS-2-8X7B** | **0.559**    | **0.515**     | **0.535**  | **0.483**     | **0.526**  | **0.507**     | **0.388** | **0.800**      |\n",
    "| GPT-3.5-TURBO-0613    | 0.255        | 0.287         | 0.148      | 0.157         | 0.360      | 0.315         | 0.298     | 0.489          |\n",
    "| GPT-4-1106            | /            | 0.553         | /          | 0.590         | /          | 0.609         | 0.517     | 0.662          |\n",
    "| CLAUDE-3-OPUS         | 0.553        | /             | 0.590      | /             | 0.609      | /             | 0.453     | 0.693          |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/03_model-evaluation/scores/gpt4-model-1-response.json">
[0, 50, 20, 100, 0, 100, 0, 100, 100, 100, 55, 0, 100, 100, 100, 100, 100, 0, 98, 100, 100, 0, 100, 100, 100, 100, 100, 100, 0, 100, 100, 0, 100, 100, 85, 100, 0, 0, 100, 100, 100, 100, 100, 100, 0, 100, 100, 95, 20, 50, 85, 100, 100, 100, 100, 55, 100, 100, 100, 0, 100, 98, 100, 100, 100, 0, 85, 100, 100, 98, 100, 100, 100, 0, 100, 100, 100, 100, 0, 100, 0, 100, 100, 0, 0, 100, 50, 100, 100, 10, 100, 100, 100, 100, 0, 100, 100, 25, 100, 30]
</file>

<file path="ch07/03_model-evaluation/scores/gpt4-model-2-response.json">
[0, 100, 0, 100, 0, 100, 0, 100, 0, 0, 50, 0, 100, 100, 100, 100, 100, 100, 100, 95, 0, 50, 100, 100, 0, 0, 100, 0, 0, 100, 0, 0, 100, 0, 67, 0, 0, 0, 100, 100, 95, 100, 100, 100, 0, 0, 0, 0, 100, 100, 100, 0, 55, 100, 0, 100, 65, 100, 100, 0, 100, 100, 100, 0, 100, 0, 85, 100, 100, 85, 0, 75, 100, 0, 0, 100, 100, 100, 0, 100, 0, 50, 100, 100, 0, 100, 0, 0, 100, 85, 100, 0, 100, 100, 0, 100, 100, 0, 0, 0]
</file>

<file path="ch07/03_model-evaluation/scores/llama3-8b-model-1-response.json">
[20, 92, 85, 90, 20, 90, 22, 97, 60, 96, 20, 20, 98, 95, 90, 98, 95, 20, 98, 98, 92, 20, 96, 96, 100, 98, 98, 95, 20, 95, 98, 20, 85, 95, 80, 97, 40, 21, 100, 85, 95, 98, 92, 98, 69, 98, 80, 60, 60, 20, 80, 68, 80, 96, 96, 68, 80, 95, 80, 20, 95, 98, 80, 98, 94, 20, 40, 98, 100, 85, 98, 90, 95, 85, 95, 80, 98, 98, 25, 98, 40, 92, 95, 82, 87, 98, 80, 90, 95, 4, 90, 90, 80, 98, 20, 98, 98, 40, 92, 98]
</file>

<file path="ch07/03_model-evaluation/scores/llama3-8b-model-2-response.json">
[76, 85, 67, 90, 20, 98, 22, 96, 40, 80, 40, 20, 90, 98, 80, 92, 98, 98, 95, 99, 55, 99, 80, 90, 20, 4, 98, 4, 40, 95, 14, 44, 95, 44, 80, 4, 4, 40, 95, 80, 98, 95, 92, 98, 68, 20, 20, 60, 95, 90, 98, 0, 20, 80, 20, 80, 92, 98, 98, 20, 95, 100, 95, 85, 98, 4, 40, 98, 98, 65, 20, 76, 100, 67, 44, 92, 75, 97, 27, 98, 20, 60, 90, 96, 67, 98, 80, 10, 80, 98, 100, 40, 92, 98, 20, 98, 98, 20, 20]
</file>

<file path="ch07/03_model-evaluation/config.json">
{
    "OPENAI_API_KEY": "sk-...",
    "_comment": "Enter your API key from https://platform.openai.com/api-keys"
}
</file>

<file path="ch07/03_model-evaluation/eval-example-data.json">
[
    {
        "instruction": "Calculate the hypotenuse of a right triangle with legs of 6 cm and 8 cm.",
        "input": "",
        "output": "The hypotenuse of the triangle is 10 cm.",
        "model 1 response": "\nThe hypotenuse of the triangle is 3 cm.",
        "model 2 response": "\nThe hypotenuse of the triangle is 12 cm."
    },
    {
        "instruction": "Name 3 different animals that are active during the day.",
        "input": "",
        "output": "1. Squirrel\n2. Eagle\n3. Tiger",
        "model 1 response": "\n1. Squirrel\n2. Tiger\n3. Eagle\n4. Cobra\n5. Tiger\n6. Cobra",
        "model 2 response": "\n1. Squirrel\n2. Eagle\n3. Tiger"
    },
    {
        "instruction": "Rewrite the following sentence in a more formal way:\n\"I need to find out what's wrong.\"",
        "input": "",
        "output": "I must ascertain what is incorrect.",
        "model 1 response": "\nWhat is incorrect?",
        "model 2 response": "\nI need to find out what's wrong."
    },
    {
        "instruction": "Identify the interjection in the sentence.",
        "input": "Wow, that was an amazing trick!",
        "output": "The interjection in the sentence is 'Wow'.",
        "model 1 response": "\nThe interjection in the sentence is 'Wow'.",
        "model 2 response": "\nThe interjection in the sentence is 'Wow'."
    },
    {
        "instruction": "Identify the type of sentence.",
        "input": "Did you finish the report?",
        "output": "The type of sentence is interrogative.",
        "model 1 response": "\nThe type of sentence is exclamatory.",
        "model 2 response": "\nThe type of sentence is exclamatory."
    },
    {
        "instruction": "Convert the following sentence into future tense.",
        "input": "She walks to school.",
        "output": "She will walk to school.",
        "model 1 response": "\nShe will walk to school.",
        "model 2 response": "\nShe will walk to school."
    },
    {
        "instruction": "Convert 500 grams to ounces.",
        "input": "",
        "output": "500 grams is approximately 17.64 ounces.",
        "model 1 response": "\n500 grams is approximately 99.21 ounces.",
        "model 2 response": "\n500 grams is approximately 0.05 ounces."
    },
    {
        "instruction": "Edit the following sentence to remove all passive voice.",
        "input": "The song was composed by the artist.",
        "output": "The artist composed the song.",
        "model 1 response": "\nThe artist composed the song.",
        "model 2 response": "\nThe artist composed the song."
    },
    {
        "instruction": "Provide a word that rhymes with \"care.\"",
        "input": "",
        "output": "A word that rhymes with \"care\" is \"fare.\"",
        "model 1 response": "\nA word that rhymes with \"care\" is \"bare.\"",
        "model 2 response": "\nA word that rhymes with \"care\" is \"care-free.\""
    },
    {
        "instruction": "Rewrite the given sentence to describe the same thing in a positive way.",
        "input": "The meal was not delicious.",
        "output": "The meal could use some improvement.",
        "model 1 response": "\nThe meal could use some improvement.",
        "model 2 response": "\nThe meal was delicious."
    },
    {
        "instruction": "Generate a sentence using the word 'elusive'.",
        "input": "",
        "output": "The meaning of the poem was elusive, escaping easy interpretation.",
        "model 1 response": "\nThe poem was elusive, elusive, and elusive.",
        "model 2 response": "\nThe elusive object that is so elusive that it is impossible to describe is the elusive cat."
    },
    {
        "instruction": "Arrange the given numbers in descending order.",
        "input": "7, 10, 3, 6, 9",
        "output": "10, 9, 7, 6, 3.",
        "model 1 response": "\n10 numbers in descending order: 3, 6, 3.",
        "model 2 response": "\nThe numbers in descending order are: 3, 6, 9."
    },
    {
        "instruction": "Convert this direct speech to indirect speech.",
        "input": "'I am tired,' he said.",
        "output": "He said that he was tired.",
        "model 1 response": "\nHe said that he was tired.",
        "model 2 response": "\nHe said that he was tired."
    },
    {
        "instruction": "What is the capital of Turkey?",
        "input": "",
        "output": "The capital of Turkey is Ankara.",
        "model 1 response": "\nThe capital of Turkey is Ankara.",
        "model 2 response": "\nThe capital of Turkey is Ankara."
    },
    {
        "instruction": "Convert the sentence to future tense.",
        "input": "They play football every Sunday.",
        "output": "They will play football every Sunday.",
        "model 1 response": "\nThey will play football every Sunday.",
        "model 2 response": "\nThey will play football every Sunday."
    },
    {
        "instruction": "Provide a synonym for 'quick'.",
        "input": "",
        "output": "A synonym for 'quick' is 'fast'.",
        "model 1 response": "\nA synonym for 'quick' is 'fast'.",
        "model 2 response": "\nA synonym for 'quick' is 'fast'."
    },
    {
        "instruction": "What is a synonym of 'luminous'?",
        "input": "",
        "output": "A synonym for 'luminous' is 'bright'.",
        "model 1 response": "\nA synonym for 'luminous' is 'bright'.",
        "model 2 response": "\nA synonym for 'luminous' is 'bright'."
    },
    {
        "instruction": "Name the process by which plants lose water vapor through their leaves.",
        "input": "",
        "output": "The process by which plants lose water vapor through their leaves is called transpiration.",
        "model 1 response": "\nThe process is called evaporation.",
        "model 2 response": "\nThe process by which plants lose water vapor through their leaves is called transpiration."
    },
    {
        "instruction": "Identify the verb in the sentence.",
        "input": "Birds fly south for the winter.",
        "output": "The verb in the sentence is 'fly'.",
        "model 1 response": "\nThe verb in the sentence is \"fly.\"",
        "model 2 response": "\nThe verb in the sentence is 'fly'."
    },
    {
        "instruction": "What type of bond is formed when one atom donates an electron to another atom?",
        "input": "",
        "output": "The type of bond formed when one atom donates an electron to another atom is an ionic bond.",
        "model 1 response": "\nThe type of bond formed when one atom donates an electron to another atom is an ionic bond.",
        "model 2 response": "\nThe type of bond formed when one atom donates an electron to another atom is ionic."
    },
    {
        "instruction": "Rewrite the following sentence to include a simile.",
        "input": "The car moved fast.",
        "output": "The car moved as fast as a cheetah.",
        "model 1 response": "\nThe car moved as fast as a cheetah.",
        "model 2 response": "\nThe car moved at a fast pace."
    },
    {
        "instruction": "Identify the correct spelling of the following word.",
        "input": "Necesary",
        "output": "The correct spelling is \"Necessary\".",
        "model 1 response": "\nThe correct spelling is \"Necesary\".",
        "model 2 response": "\nThe correct spelling of the word \"Necessary\" is \"Necessary\"."
    },
    {
        "instruction": "Rewrite this sentence in the active voice.",
        "input": "A novel was written by Mary.",
        "output": "Mary wrote a novel.",
        "model 1 response": "\nMary wrote a novel.",
        "model 2 response": "\nMary wrote a novel."
    },
    {
        "instruction": "rewrite this sentence as a question.",
        "input": "Your brother went to London last year.",
        "output": "Did your brother go to London last year?",
        "model 1 response": "\nDid your brother go to London last year?",
        "model 2 response": "\nDid your brother go to London last year?"
    },
    {
        "instruction": "Explain what a haiku is.",
        "input": "",
        "output": "A haiku is a form of traditional Japanese poetry that consists of three lines with a syllable pattern of 5-7-5.",
        "model 1 response": "\nA haiku is a form of traditional Japanese poetry that consists of three lines with a syllable pattern of 5-7-5.",
        "model 2 response": "\nA haiku is a form of traditional Japanese poetry that consists of three lines that are composed of three syllables."
    },
    {
        "instruction": "Split the following sentence into two declarative sentences: \"The book is interesting but the cover is dull\".",
        "input": "",
        "output": "The book is interesting.\nThe cover is dull.",
        "model 1 response": "\nThe book is interesting.\nThe cover is dull.",
        "model 2 response": "\nThe book is dull."
    },
    {
        "instruction": "What is the antonym of 'sparse'?",
        "input": "",
        "output": "The antonym of 'sparse' is 'dense'.",
        "model 1 response": "\nThe antonym of 'sparse' is 'dense'.",
        "model 2 response": "\nThe antonym of 'sparse' is 'dense'."
    },
    {
        "instruction": "Identify the type of sentence.",
        "input": "What a beautiful view!",
        "output": "The type of sentence is exclamatory.",
        "model 1 response": "\nThe type of sentence is exclamatory.",
        "model 2 response": "\nThe type of sentence is subjunctive."
    },
    {
        "instruction": "What is the boiling point of mercury in Celsius?",
        "input": "",
        "output": "The boiling point of mercury is 356.7 degrees Celsius.",
        "model 1 response": "\nThe boiling point of mercury is approximately 100 degrees Celsius.",
        "model 2 response": "\nThe boiling point of mercury is 100 degrees Celsius."
    },
    {
        "instruction": "Define the term 'kinetic energy'.",
        "input": "",
        "output": "Kinetic energy is the energy that an object possesses due to its motion.",
        "model 1 response": "\nKinetic energy is the energy that an object possesses due to its motion.",
        "model 2 response": "\nKinetic energy is the energy that an object possesses due to its motion."
    },
    {
        "instruction": "Change the following sentence to past perfect tense.",
        "input": "They finish the game.",
        "output": "They had finished the game.",
        "model 1 response": "\nThey had finished the game.",
        "model 2 response": "\nThey will finish the game."
    },
    {
        "instruction": "Classify the following items as either solid, liquid, or gas.",
        "input": "Mercury, oxygen, wood",
        "output": "Mercury - Liquid\nOxygen - Gas\nWood - Solid",
        "model 1 response": "\nMercury: Solid\nOxygen: Gas\nWood: Solid\nPlants: Vegetable\nMinerals: Iron\nMinerals: Gold\nMinerals: Coal\nMinerals: Bacteria: E. coli\nFruits: Cherries\nMinerals: Wheat\nMinerals: Vinegar\nMinerals: Fish\nMinerals: Mercury\nMinerals: Oxygen\nMinerals: Carbon\nMinerals: Wood\nMinerals: Gas\nMinerals: Carbon\nMinerals: Wood\nMinerals: Solid\nMinerals: Gas\nMinerals: Solid\nMinerals: Gas\nMinerals: Solid\nMinerals: Solid\nMinerals: Gas\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\nMinerals: Solid\n",
        "model 2 response": "\nMercury is a solid. Oxygen is a liquid. Wood is a gas."
    },
    {
        "instruction": "What is the synonym for 'enormous'?",
        "input": "",
        "output": "A synonym for 'enormous' is 'gigantic'.",
        "model 1 response": "\nThe synonym for 'enormous' is 'gigantic'.",
        "model 2 response": "\nThe synonym for 'enormous' is 'gigantic'."
    },
    {
        "instruction": "Translate the term 'sustainable development' into German.",
        "input": "",
        "output": "The German translation of 'sustainable development' is 'nachhaltige Entwicklung'.",
        "model 1 response": "\nThe German translation of 'sustainable development' is 'nachhaltige Entwicklung'.",
        "model 2 response": "\nThe German translation of 'sustainable development' is 'sustainable development f\u00fcr das Entwicklung'."
    },
    {
        "instruction": "Name three essential elements found in nature.",
        "input": "",
        "output": "1. Carbon\n2. Oxygen\n3. Iron",
        "model 1 response": "\n1. Carbon\n2. Oxygen\n3. Oxygen\n### Response:\n1. Carbon\n2. Oxygen\n3. Oxygen",
        "model 2 response": "\n1. Carbon\n2. Iron\n3. Titanium"
    },
    {
        "instruction": "Rewrite the following phrase as a question.",
        "input": "We must finish the project by tomorrow.",
        "output": "Must we finish the project by tomorrow?",
        "model 1 response": "\nMust we finish the project by tomorrow?",
        "model 2 response": "\nWe must finish the project by tomorrow."
    },
    {
        "instruction": "Combine the given sentences into one cohesive sentence.",
        "input": "It is raining. I cannot go to the park.",
        "output": "Since it is raining, I cannot go to the park.",
        "model 1 response": "\nIt is raining. I cannot go to the park.",
        "model 2 response": "\nIt is raining. I cannot go to the park."
    },
    {
        "instruction": "Calculate the volume of a cube with side length of 3 meters.",
        "input": "",
        "output": "The volume of the cube is 27 cubic meters.",
        "model 1 response": "\nThe volume of the cube is approximately 3 cubic meters.",
        "model 2 response": "\nThe volume of the cube is approximately 3.5 cubic meters."
    },
    {
        "instruction": "Re-word this sentence using an active voice.",
        "input": "The decision was made by the team.",
        "output": "The team made the decision.",
        "model 1 response": "\nThe team made the decision.",
        "model 2 response": "\nThe team made the decision."
    },
    {
        "instruction": "Convert the sentence to use an indefinite pronoun.",
        "input": "Someone left a message.",
        "output": "A message was left by someone.",
        "model 1 response": "\nA message was left by someone.",
        "model 2 response": "\nA message was left by someone."
    },
    {
        "instruction": "Classify the following statement into one of these labels: [positive, negative, neutral]",
        "input": "My computer crashed.",
        "output": "The statement \"My computer crashed\" can be classified as negative.",
        "model 1 response": "\nThe statement \"My computer crashed\" can be classified as negative.",
        "model 2 response": "\nThe statement \"My computer crashed\" is classified as negative."
    },
    {
        "instruction": "What is the derivative of x^2?",
        "input": "",
        "output": "The derivative of x^2 is 2x.",
        "model 1 response": "\nThe derivative of x^2 is 2x.",
        "model 2 response": "\nThe derivative of x^2 is 2x."
    },
    {
        "instruction": "Fill in the blanks with appropriate words.",
        "input": "The sky was filled with colorful ________ at sunset.",
        "output": "The sky was filled with colorful clouds at sunset.",
        "model 1 response": "\nThe sky was filled with colorful clouds at sunset.",
        "model 2 response": "\nThe sky was filled with colorful clouds at sunset."
    },
    {
        "instruction": "Change the sentence from passive to active voice.",
        "input": "The song was sung by the choir.",
        "output": "The choir sang the song.",
        "model 1 response": "\nThe choir sang the song.",
        "model 2 response": "\nThe choir sang the song."
    },
    {
        "instruction": "What is the atomic mass of carbon?",
        "input": "",
        "output": "The atomic mass of carbon is approximately 12.01 atomic mass units.",
        "model 1 response": "\nThe atomic mass of carbon is approximately 3.01 atomic mass units.",
        "model 2 response": "\nThe atomic mass of carbon is approximately 3.0 atomic mass units."
    },
    {
        "instruction": "Convert the sentence to present perfect tense: 'He wins the race.'",
        "input": "He wins the race.",
        "output": "He has won the race.",
        "model 1 response": "\nHe has won the race.",
        "model 2 response": "\nHe will win the race."
    },
    {
        "instruction": "Rewrite the following sentence to include an idiom.",
        "input": "It started raining suddenly.",
        "output": "It started raining cats and dogs.",
        "model 1 response": "\nIt started raining cats and dogs.",
        "model 2 response": "\nIt started raining suddenly."
    },
    {
        "instruction": "What is the atomic number of helium?",
        "input": "",
        "output": "The atomic number of helium is 2.",
        "model 1 response": "\nThe atomic number of helium is 2.0.",
        "model 2 response": "\nThe atomic number of helium is 3."
    },
    {
        "instruction": "Name three types of clouds.",
        "input": "",
        "output": "Three types of clouds are cumulus, stratus, and cirrus.",
        "model 1 response": "\nThree types of clouds are cirrus, cirrus-like, and cirrus-like-ring.",
        "model 2 response": "\nThree types of clouds are cumulus, stratus, and cirrus."
    },
    {
        "instruction": "Identify the onomatopoeia in the sentence.",
        "input": "The bees buzzed as they flew past.",
        "output": "The onomatopoeia in the sentence is 'buzzed'.",
        "model 1 response": "\nThe onomatopoeia in the sentence is that the bees buzzed as they flew past.",
        "model 2 response": "\nThe onomatopoeia in the sentence is 'buzzed'."
    },
    {
        "instruction": "Name the process by which water changes from liquid to gas.",
        "input": "",
        "output": "The process by which water changes from liquid to gas is called evaporation.",
        "model 1 response": "\nThe process is called evaporation.",
        "model 2 response": "\nThe process by which water changes from liquid to gas is called evaporation."
    },
    {
        "instruction": "Find the synonym for the given verb.",
        "input": "Begin",
        "output": "Commence",
        "model 1 response": "\nCommence",
        "model 2 response": "\nBegin\n\n### Input:\nbegin\n\n### Response:\nBegin\n\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin\n### Response:\nbegin\n### Input:\nbegin"
    },
    {
        "instruction": "Classify the following numbers as prime or composite.",
        "input": ": 11, 14, 19.",
        "output": "Prime numbers: 11, 19\nComposite numbers: 14",
        "model 1 response": "\nPrime numbers: 11, 19\n\nComposite numbers: 14\n\n### Response:\nPrime numbers: 11, 19\nComposite numbers: 14",
        "model 2 response": "\nPrime numbers: 11, 19, 19. Composite numbers: 14, 19, 19."
    },
    {
        "instruction": "Classify the item as either a grain or a fruit.",
        "input": "Wheat",
        "output": "Grain",
        "model 1 response": "\nGrain\n\n### Response:\nGrain\n### Response:\nWheat\n### Response:\nGrain",
        "model 2 response": "\nGrain"
    },
    {
        "instruction": "Provide the plural form of 'thesis'.",
        "input": "",
        "output": "The plural form of 'thesis' is 'theses'.",
        "model 1 response": "\nThe plural form of 'thesis' is 'theses'.",
        "model 2 response": "\nThe plural form of 'thesis' is 'thesis'."
    },
    {
        "instruction": "Generate three verbs that mean the same as \"to run.\"",
        "input": "",
        "output": "1. To sprint\n2. To dash\n3. To jog",
        "model 1 response": "\n1. To sprint\n2. To dash\n3. To dash\n4. To dash\n5. To dash\n6. To dash",
        "model 2 response": "\n1. To sprint\n2. To dash\n3. To jog"
    },
    {
        "instruction": "Generate a question that could logically follow the statement below.",
        "input": "I just returned from a vacation in Spain.",
        "output": "How was your vacation in Spain?",
        "model 1 response": "\nHow was your vacation in Spain?",
        "model 2 response": "\nWhat was your vacation in Spain?"
    },
    {
        "instruction": "What is the normal boiling point of water?",
        "input": "",
        "output": "The normal boiling point of water is 100 degrees Celsius.",
        "model 1 response": "\nThe normal boiling point of water is 100 degrees Celsius.",
        "model 2 response": "\nThe normal boiling point of water is 100 degrees Celsius."
    },
    {
        "instruction": "Convert the sentence to active voice.",
        "input": "The book was read by many.",
        "output": "Many people read the book.",
        "model 1 response": "\nMany people read the book.",
        "model 2 response": "\nMany people read the book."
    },
    {
        "instruction": "Identify the tense used in the sentence.",
        "input": "They are watching a movie.",
        "output": "The tense used in the sentence \"They are watching a movie\" is the present continuous tense.",
        "model 1 response": "\nThe tense used in the sentence is future continuous.",
        "model 2 response": "\nThe tense used in the sentence is future continuous."
    },
    {
        "instruction": "Explain what GDP stands for and what it measures.",
        "input": "",
        "output": "GDP stands for Gross Domestic Product, which measures the total value of all goods and services produced over a specific time period within a country's borders.",
        "model 1 response": "\nGDP stands for Gross Domestic Product, which measures the total value of all goods and services produced over a specific time period within a country's borders.",
        "model 2 response": "\nGDP stands for Gross Domestic Product, which measures the total value of all goods and services produced over a specific time period within a country's borders."
    },
    {
        "instruction": "Identify the verb in the following sentence",
        "input": "The sun shines brightly.",
        "output": "The verb in the sentence is 'shines'.",
        "model 1 response": "\nThe verb in the sentence is \"shines.\"",
        "model 2 response": "\nThe verb in the sentence is 'shines'."
    },
    {
        "instruction": "Rewrite the following sentence to include a metaphor.'",
        "input": "Life can be hard.",
        "output": "Life is a rocky road.",
        "model 1 response": "\nLife is a rocky road.",
        "model 2 response": "\nLife is a rocky road."
    },
    {
        "instruction": "What is the derivative of the function f(x) = x^4?",
        "input": "",
        "output": "The derivative of the function f(x) = x^4 is f'(x) = 4x^3.",
        "model 1 response": "\nThe derivative of the function f(x) = x^4 is f'(x) = 4x^3.",
        "model 2 response": "\nThe derivative of the function f(x) = x^4 is f'(x) = 4x."
    },
    {
        "instruction": "Convert the temperature from Celsius to Kelvin.",
        "input": "25 degrees Celsius",
        "output": "25 degrees Celsius is 298.15 Kelvin.",
        "model 1 response": "\n25 degrees Celsius is approximately 298.15 Kelvin.",
        "model 2 response": "\n25 degrees Celsius is approximately 298.15 Kelvin."
    },
    {
        "instruction": "Rewrite the following sentence to include a hyperbole: 'I'm very hungry.'",
        "input": "I'm very hungry.",
        "output": "I'm so hungry I could eat a horse.",
        "model 1 response": "\nI'm very hungry.",
        "model 2 response": "\nI'm very hungry."
    },
    {
        "instruction": "List the ingredients required to make a basic salad.",
        "input": "",
        "output": "Ingredients for a basic salad typically include lettuce, tomatoes, cucumbers, and dressing.",
        "model 1 response": "\nIngredients for a basic salad typically include lettuce, tomatoes, cucumbers, and cucumbers.",
        "model 2 response": "\nIngredients for a basic salad typically include lettuce, tomatoes, dressing, and dressingings."
    },
    {
        "instruction": "Select the noun phrase in the following sentence.",
        "input": "He enjoyed a delicious cake at the party.",
        "output": "The noun phrases in the given sentence are \"a delicious cake\" and \"the party.\"",
        "model 1 response": "\nThe noun phrases in the sentence are \"a delicious cake\" and \"the party.\"",
        "model 2 response": "\nThe noun phrases in the sentence are \"a delicious cake\" and \"the party.\""
    },
    {
        "instruction": "What is the contraction for \"will not\"?",
        "input": "",
        "output": "The contraction for \"will not\" is \"won't\".",
        "model 1 response": "\nThe contraction for \"will not\" is \"won't\".",
        "model 2 response": "\nThe contraction for \"will not\" is \"won't\"."
    },
    {
        "instruction": "Generate a sentence using the word 'elucidate'.",
        "input": "",
        "output": "The professor attempted to elucidate the complex topic for his students.",
        "model 1 response": "\nThe professor elucidated the complex topic for his students.",
        "model 2 response": "\nThe professor elucidated the topic of the class."
    },
    {
        "instruction": "Convert this sentence to passive voice",
        "input": "The chef cooked a delicious meal.",
        "output": "A delicious meal was cooked by the chef.",
        "model 1 response": "\nA delicious meal was cooked by the chef.",
        "model 2 response": "\nThe chef cooked a delicious meal."
    },
    {
        "instruction": "What is the pH of lemon juice?",
        "input": "",
        "output": "The pH of lemon juice is approximately 2.0.",
        "model 1 response": "\nThe pH of lemon juice is approximately 2.0.",
        "model 2 response": "\nThe pH of lemon juice is typically around 3.0."
    },
    {
        "instruction": "What is the capital of Norway?",
        "input": "",
        "output": "The capital of Norway is Oslo.",
        "model 1 response": "\nThe capital of Norway is Oslo.",
        "model 2 response": "\nThe capital of Norway is Oslo."
    },
    {
        "instruction": "Generate a question with the keyword \u201cbacteria\u201d",
        "input": "",
        "output": "How do bacteria affect human health?",
        "model 1 response": "\nWhat is the pH of baking soda solution?\n\n### Response:\nThe pH of a baking soda solution is typically around 2.0.",
        "model 2 response": "\nWhat is the pH of a neutral solution at 25\u00b0C?\n\n### Input:\nThe pH of a neutral solution at 25\u00b0C is typically around 3.0."
    },
    {
        "instruction": "Generate a sentence using the word 'spontaneous'.",
        "input": "",
        "output": "His decision to travel was completely spontaneous, made on a whim.",
        "model 1 response": "\nHis decision to travel was completely spontaneous, made on a whim.",
        "model 2 response": "\nThe decision was made by the team."
    },
    {
        "instruction": "Convert the sentence to future perfect tense.",
        "input": "They close the store.",
        "output": "They will have closed the store.",
        "model 1 response": "\nThey will have closed the store.",
        "model 2 response": "\nThey will have closed the store."
    },
    {
        "instruction": "Provide the past participle form of 'break'.",
        "input": "",
        "output": "The past participle form of 'break' is 'broken'.",
        "model 1 response": "\nThe past participle form of 'break' is 'broken'.",
        "model 2 response": "\nThe past participle form of 'break' is 'broken'."
    },
    {
        "instruction": "Provide the correct contraction for 'they are'.",
        "input": "",
        "output": "The contraction for 'they are' is 'they're'.",
        "model 1 response": "\nThe contraction for 'they are' is 'they're'.",
        "model 2 response": "\nThe contraction for 'they are' is 'they're'."
    },
    {
        "instruction": "Convert the temperature from Fahrenheit to Celsius: 68\u00b0F.",
        "input": "68 degrees Fahrenheit",
        "output": "68 degrees Fahrenheit is approximately 20 degrees Celsius.",
        "model 1 response": "\n68 degrees Fahrenheit is approximately 37 degrees Celsius.",
        "model 2 response": "\n68 degrees Fahrenheit is approximately 37 degrees Celsius."
    },
    {
        "instruction": "Turn the following statement into a question.",
        "input": "The Golden State Warriors are a professional basketball team based in San Francisco.",
        "output": "Are the Golden State Warriors a professional basketball team based in San Francisco?",
        "model 1 response": "\nAre the Golden State Warriors a professional basketball team based in San Francisco?",
        "model 2 response": "\nAre the Golden State Warriors a professional basketball team based in San Francisco?"
    },
    {
        "instruction": "What is the derivative of the function f(x) = 3x^3?",
        "input": "",
        "output": "The derivative of the function f(x) = 3x^3 is f'(x) = 9x^2.",
        "model 1 response": "\nThe derivative of the function f(x) = 3x^3 is f'(x) = 6x.",
        "model 2 response": "\nThe derivative of the function f(x) = 3x^3 is f'(x) = 4x."
    },
    {
        "instruction": "Rewrite the following phrase in the present tense.",
        "input": "They had played soccer all afternoon.",
        "output": "They play soccer all afternoon.",
        "model 1 response": "\nThey play soccer all afternoon.",
        "model 2 response": "\nThey played soccer all afternoon."
    },
    {
        "instruction": "Rewrite this sentence to start with \"Despite\": She managed to finish her work, although she was tired.",
        "input": "",
        "output": "Despite being tired, she managed to finish her work.",
        "model 1 response": "\nDespite being tired, she managed to finish her work.",
        "model 2 response": "\nDespite being tired, she managed to finish her work."
    },
    {
        "instruction": "Convert the sentence to future continuous tense.",
        "input": "She plays the piano.",
        "output": "She will be playing the piano.",
        "model 1 response": "\nShe will play the piano.",
        "model 2 response": "\nShe will be playing the piano."
    },
    {
        "instruction": "Capitalize each word in the sentence.",
        "input": "i love watching the sunrise",
        "output": "I Love Watching The Sunrise",
        "model 1 response": "\nI love watching the sunrise.",
        "model 2 response": "\nI love watching the sunrise."
    },
    {
        "instruction": "What is the antonym of 'complicated'?",
        "input": "",
        "output": "The antonym of 'complicated' is 'simple'.",
        "model 1 response": "\nThe antonym of 'complicated' is 'simple'.",
        "model 2 response": "\nThe antonym of 'complicated' is 'simple'."
    },
    {
        "instruction": "Sort the following numbers in ascending order.",
        "input": "5, 18, 3, 11.",
        "output": "The numbers in ascending order would be: 3, 5, 11, 18.",
        "model 1 response": "\nThe numbers in ascending order are: 3, 5, 11.",
        "model 2 response": "\nThe numbers in ascending order are: 3, 11, 5, 18."
    },
    {
        "instruction": "Identify the theme of the story.",
        "input": "The Tortoise and the Hare",
        "output": "The theme of the story is that slow and steady wins the race.",
        "model 1 response": "\nThe theme of the story is that slow and steady wins the race.",
        "model 2 response": "\nThe theme of the story is that the Tortoise and the Hare are the two most popular animals in the world."
    },
    {
        "instruction": "Correct the verb tense error in the sentence.",
        "input": "She go to school every day.",
        "output": "The corrected sentence should be: 'She goes to school every day.'",
        "model 1 response": "\nThe corrected sentence should be: 'She goes to school every day.'",
        "model 2 response": "\nThe corrected sentence should be: 'She goes to school every day.'"
    },
    {
        "instruction": "Evaluate this sentence for spelling and grammar mistakes.",
        "input": "I recieved an invitation to an exciting evenement.",
        "output": "There are two errors in the sentence. The corrected sentence should be: \"I received an invitation to an exciting event.\"",
        "model 1 response": "\nAn invitation to an exciting event?",
        "model 2 response": "\nThe corrected sentence should be: 'I received an invitation to an exciting event.'"
    },
    {
        "instruction": "Identify the hypernym for the word \"car\".",
        "input": "",
        "output": "The hypernym for the word \"car\" is \"vehicle\".",
        "model 1 response": "\nThe hypernym for the word \"car\" is \"vehicle\".",
        "model 2 response": "\nThe hypernym for the word \"car\" is \"vehicle\"."
    },
    {
        "instruction": "Name an antonym for \"joy.\"",
        "input": "",
        "output": "An antonym for \"joy\" is \"sorrow.\"",
        "model 1 response": "\nAn antonym for \"joy\" is \"sorrow.\"",
        "model 2 response": "\nAn antonym for \"joy\" is \"contentment.\""
    },
    {
        "instruction": "Convert the sentence into a question",
        "input": "She likes ice cream.",
        "output": "Does she like ice cream?",
        "model 1 response": "\nDoes she like ice cream?",
        "model 2 response": "\nDoes she like ice cream?"
    },
    {
        "instruction": "Rewrite this sentence in the form of a question.",
        "input": "This is the biggest tree in the park.",
        "output": "Is this the biggest tree in the park?",
        "model 1 response": "\nIs this the biggest tree in the park?",
        "model 2 response": "\nIs this the biggest tree in the park?"
    },
    {
        "instruction": "Find a rhyme for the word 'sing'.",
        "input": "",
        "output": "A word that rhymes with 'sing' is 'ring'.",
        "model 1 response": "\nA word that rhymes with 'ring' is 'ring-a'.",
        "model 2 response": "\nThe word 'sing' is a derivative of the word 'sing'."
    },
    {
        "instruction": "Convert this sentence to future perfect tense.",
        "input": "She will finish the project.",
        "output": "She will have finished the project.",
        "model 1 response": "\nShe will have finished the project.",
        "model 2 response": "\nShe will have finished the project."
    },
    {
        "instruction": "What type of number is 0?",
        "input": "",
        "output": "0 is an integer.",
        "model 1 response": "\n0 is an integer.",
        "model 2 response": "\n0 is an integer."
    },
    {
        "instruction": "Classify the following animals",
        "input": "Eagle, Shark, Cobra.",
        "output": "Birds: Eagle\nFish: Shark\nReptiles: Cobra",
        "model 1 response": "\nBirds: Eagle\nFish: Shark\nReptiles: Cobra\nReptiles: Squirrel\nReptiles: Tiger\nReptiles: Cobra\nReptiles: Tiger\nReptiles: Squirrel\nReptiles: Tiger\nReptiles: Cobra\nReptiles: Tiger\nReptiles: Cobra",
        "model 2 response": "\nBirds: Eagle\nFish: Shark\nReptiles: Cobra\nPlants: Eagle\nMinerals: Eagle\nFruits: Shark\nFruits: Shark"
    },
    {
        "instruction": "Take the following noun and turn it into a verb.",
        "input": "Light",
        "output": "To light.",
        "model 1 response": "\nTo light.",
        "model 2 response": "\nLight is the most abundant element in the periodic table."
    },
    {
        "instruction": "Generate a random alphanumeric code of 10 characters.",
        "input": "",
        "output": "Here is a randomly generated 10-character alphanumeric code: 3F8G6H7JQ2.",
        "model 1 response": "\nHere is a randomly generated sequence of 10 characters: 3F8G6H7JQHWQ2JQ3H6H-\n\n### Response:\nHere is a sequence of 10 characters: 3F8G6H7JQHWQ2JQ3H6H-",
        "model 2 response": "\nThe alphanumeric code for the word 'happy' is 'happiness'."
    }
]
</file>

<file path="ch07/03_model-evaluation/requirements-extra.txt">
openai>=1.30.3
tqdm>=4.65.0
</file>

<file path="ch07/04_preference-tuning-with-dpo/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-6.
# This file can be run as a standalone script.


import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


#####################################
# Chapter 2
#####################################


class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):

        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]

        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)

        # Focus only on the last time step
        # (batch, n_token, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]

        # Get the idx of the vocab entry with the highest logits value
        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx


#####################################
# Chapter 5
#####################################
def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx


def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            tokens_seen += input_batch.numel()
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Print a sample text after each epoch
        generate_and_print_sample(
            model, tokenizer, device, start_context
        )

    return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label=f"Training {label}")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig(f"{label}-plot.pdf")
    plt.show()
</file>

<file path="ch07/05_dataset-generation/config.json">
{
  "OPENAI_API_KEY": "sk-...",
  "_comment": "Enter your API key from https://platform.openai.com/api-keys"
}
</file>

<file path="ch07/05_dataset-generation/requirements-extra.txt">
openai>=1.30.3
tqdm>=4.65.0
</file>

<file path="ch07/06_user_interface/app.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

from pathlib import Path
import sys

import tiktoken
import torch
import chainlit

from previous_chapters import (
    generate,
    GPTModel,
    text_to_token_ids,
    token_ids_to_text,
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def get_model_and_tokenizer():
    """
    Code to load a GPT-2 model with finetuned weights generated in chapter 7.
    This requires that you run the code in chapter 7 first, which generates the necessary gpt2-medium355M-sft.pth file.
    """

    GPT_CONFIG_355M = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Shortened context length (orig: 1024)
        "emb_dim": 1024,         # Embedding dimension
        "n_heads": 16,           # Number of attention heads
        "n_layers": 24,          # Number of layers
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    tokenizer = tiktoken.get_encoding("gpt2")

    model_path = Path("..") / "01_main-chapter-code" / "gpt2-medium355M-sft.pth"
    if not model_path.exists():
        print(
            f"Could not find the {model_path} file. Please run the chapter 7 code "
            " (ch07.ipynb) to generate the gpt2-medium355M-sft.pt file."
        )
        sys.exit()

    checkpoint = torch.load(model_path, weights_only=True)
    model = GPTModel(GPT_CONFIG_355M)
    model.load_state_dict(checkpoint)
    model.to(device)

    return tokenizer, model, GPT_CONFIG_355M


def extract_response(response_text, input_text):
    return response_text[len(input_text):].replace("### Response:", "").strip()


# Obtain the necessary tokenizer and model files for the chainlit function below
tokenizer, model, model_config = get_model_and_tokenizer()


@chainlit.on_message
async def main(message: chainlit.Message):
    """
    The main Chainlit function.
    """

    torch.manual_seed(123)

    prompt = f"""Below is an instruction that describes a task. Write a response
    that appropriately completes the request.

    ### Instruction:
    {message.content}
    """

    token_ids = generate(  # function uses `with torch.no_grad()` internally already
        model=model,
        idx=text_to_token_ids(prompt, tokenizer).to(device),  # The user text is provided via as `message.content`
        max_new_tokens=35,
        context_size=model_config["context_length"],
        eos_id=50256
    )

    text = token_ids_to_text(token_ids, tokenizer)
    response = extract_response(text, prompt)

    await chainlit.Message(
        content=f"{response}",  # This returns the model response to the interface
    ).send()
</file>

<file path="ch07/06_user_interface/previous_chapters.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch
#
# This file collects all the relevant code that we covered thus far
# throughout Chapters 2-5.

import json
import os
import urllib

import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn
from tqdm import tqdm


#####################################
# Chapter 3
#####################################
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec


#####################################
# Chapter 4
#####################################
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift


class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x


class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


#####################################
# Chapter 5
#####################################
def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def download_and_load_gpt2(model_size, models_dir):
    # Validate model size
    allowed_sizes = ("124M", "355M", "774M", "1558M")
    if model_size not in allowed_sizes:
        raise ValueError(f"Model size not in {allowed_sizes}")

    # Define paths
    model_dir = os.path.join(models_dir, model_size)
    base_url = "https://openaipublic.blob.core.windows.net/gpt-2/models"
    filenames = [
        "checkpoint", "encoder.json", "hparams.json",
        "model.ckpt.data-00000-of-00001", "model.ckpt.index",
        "model.ckpt.meta", "vocab.bpe"
    ]

    # Download files
    os.makedirs(model_dir, exist_ok=True)
    for filename in filenames:
        file_url = os.path.join(base_url, model_size, filename)
        file_path = os.path.join(model_dir, filename)
        download_file(file_url, file_path)

    # Load settings and params
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    settings = json.load(open(os.path.join(model_dir, "hparams.json")))
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)

    return settings, params


def download_file(url, destination):
    # Send a GET request to download the file
    with urllib.request.urlopen(url) as response:
        # Get the total file size from headers, defaulting to 0 if not present
        file_size = int(response.headers.get("Content-Length", 0))

        # Check if file exists and has the same size
        if os.path.exists(destination):
            file_size_local = os.path.getsize(destination)
            if file_size == file_size_local:
                print(f"File already exists and is up-to-date: {destination}")
                return

        # Define the block size for reading the file
        block_size = 1024  # 1 Kilobyte

        # Initialize the progress bar with total file size
        progress_bar_description = os.path.basename(url)  # Extract filename from URL
        with tqdm(total=file_size, unit="iB", unit_scale=True, desc=progress_bar_description) as progress_bar:
            # Open the destination file in binary write mode
            with open(destination, "wb") as file:
                # Read the file in chunks and write to destination
                while True:
                    chunk = response.read(block_size)
                    if not chunk:
                        break
                    file.write(chunk)
                    progress_bar.update(len(chunk))  # Update progress bar


def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):
    # Initialize parameters dictionary with empty blocks for each layer
    params = {"blocks": [{} for _ in range(settings["n_layer"])]}

    # Iterate over each variable in the checkpoint
    for name, _ in tf.train.list_variables(ckpt_path):
        # Load the variable and remove singleton dimensions
        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))

        # Process the variable name to extract relevant parts
        variable_name_parts = name.split("/")[1:]  # Skip the 'model/' prefix

        # Identify the target dictionary for the variable
        target_dict = params
        if variable_name_parts[0].startswith("h"):
            layer_number = int(variable_name_parts[0][1:])
            target_dict = params["blocks"][layer_number]

        # Recursively access or create nested dictionaries
        for key in variable_name_parts[1:-1]:
            target_dict = target_dict.setdefault(key, {})

        # Assign the variable array to the last key
        last_key = variable_name_parts[-1]
        target_dict[last_key] = variable_array

    return params


def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))


def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
</file>

<file path="ch07/06_user_interface/requirements-extra.txt">
chainlit>=1.2.0
</file>

<file path="ch07/README.md">
# 第7章：微调以执行指令

&nbsp;
## 主要章节代码

- [01_main-chapter-code](01_main-chapter-code) 包含主要章节代码和习题解答

&nbsp;
## 附加材料

- [02_dataset-utilities](02_dataset-utilities) 包含用于准备指令数据集的实用代码
- [03_model-evaluation](03_model-evaluation) 包含用于使用本地 Llama 3 模型和 GPT-4 API 评估指令响应的实用代码
- [04_preference-tuning-with-dpo](04_preference-tuning-with-dpo) 实现了使用直接偏好优化（DPO）进行偏好微调的代码
- [05_dataset-generation](05_dataset-generation) 包含用于生成和改进指令微调的合成数据集的代码
- [06_user_interface](06_user_interface) 实现了一个互动用户界面，用于与预训练的 LLM 进行交互
</file>

<file path="setup/02_installing-python-libraries/python_environment_check.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31e08b0-f551-4d67-b95e-41f49de3b392",
   "metadata": {},
   "source": [
    "<font size=\"1\">\n",
    "Supplementary code for \"Build a Large Language Model From Scratch\": <a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">https://www.manning.com/books/build-a-large-language-model-from-scratch</a> by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f6f7ed-b67d-465b-bf6f-a99b0d996930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Your Python version is 3.10.12\n",
      "[OK] numpy 1.26.0\n",
      "[OK] matplotlib 3.8.2\n",
      "[OK] jupyterlab 4.0.6\n",
      "[OK] tensorflow 2.15.0\n",
      "[OK] torch 2.2.1\n",
      "[OK] tqdm 4.66.1\n",
      "[OK] tiktoken 0.5.1\n"
     ]
    }
   ],
   "source": [
    "from python_environment_check import check_packages, get_requirements_dict\n",
    "\n",
    "d = get_requirements_dict()\n",
    "check_packages(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="setup/02_installing-python-libraries/python_environment_check.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

from importlib.metadata import PackageNotFoundError, import_module
import importlib.metadata
from os.path import dirname, exists, join, realpath
from packaging.version import parse as version_parse
import platform
import sys

if version_parse(platform.python_version()) < version_parse("3.9"):
    print("[FAIL] We recommend Python 3.9 or newer but"
          " found version %s" % (sys.version))
else:
    print("[OK] Your Python version is %s" % (platform.python_version()))


def get_packages(pkgs):
    versions = []
    for p in pkgs:
        try:
            imported = import_module(p)
            try:
                version = (getattr(imported, "__version__", None) or
                           getattr(imported, "version", None) or
                           getattr(imported, "version_info", None))
                if version is None:
                    # If common attributes don"t exist, use importlib.metadata
                    version = importlib.metadata.version(p)
                versions.append(version)
            except PackageNotFoundError:
                # Handle case where package is not installed
                versions.append("0.0")
        except ImportError:
            # Fallback if importlib.import_module fails for unexpected reasons
            versions.append("0.0")
    return versions


def get_requirements_dict():
    PROJECT_ROOT = dirname(realpath(__file__))
    PROJECT_ROOT_UP_TWO = dirname(dirname(PROJECT_ROOT))
    REQUIREMENTS_FILE = join(PROJECT_ROOT_UP_TWO, "requirements.txt")
    if not exists(REQUIREMENTS_FILE):
        REQUIREMENTS_FILE = join(PROJECT_ROOT, "requirements.txt")

    d = {}
    with open(REQUIREMENTS_FILE) as f:
        for line in f:
            if not line.strip():
                continue
            if "," in line:
                left, right = line.split(",")
                lower = right.split("#")[0].strip()
                package, _, upper = left.split(" ")
                package = package.strip()
                _, lower = lower.split(" ")
                lower = lower.strip()
                upper = upper.strip()
                d[package] = (upper, lower)
            else:
                line = line.split("#")[0].strip()
                line = line.split(" ")
                line = [ln.strip() for ln in line]
                d[line[0]] = line[-1]
    return d


def check_packages(d):
    versions = get_packages(d.keys())

    for (pkg_name, suggested_ver), actual_ver in zip(d.items(), versions):
        if isinstance(suggested_ver, tuple):
            lower, upper = suggested_ver[0], suggested_ver[1]
        else:
            lower = suggested_ver
            upper = None
        if actual_ver == "N/A":
            continue
        actual_ver = version_parse(actual_ver)
        lower = version_parse(lower)
        if upper is not None:
            upper = version_parse(upper)
        if actual_ver < lower and upper is None:
            print(f"[FAIL] {pkg_name} {actual_ver}, please upgrade to >= {lower}")
        elif actual_ver < lower:
            print(f"[FAIL] {pkg_name} {actual_ver}, please upgrade to >= {lower} and < {upper}")
        elif upper is not None and actual_ver >= upper:
            print(f"[FAIL] {pkg_name} {actual_ver}, please downgrade to >= {lower} and < {upper}")
        else:
            print(f"[OK] {pkg_name} {actual_ver}")


def main():
    d = get_requirements_dict()
    check_packages(d)


if __name__ == "__main__":
    main()
</file>

<file path="setup/02_installing-python-libraries/tests.py">
# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

# File for internal use (unit tests)

from python_environment_check import main


def test_main(capsys):
    main()
    captured = capsys.readouterr()
    assert "FAIL" not in captured.out
</file>

<file path="setup/03_optional-docker-environment/.devcontainer/devcontainer.json">
{
  "name": "LLMs From Scratch",
  "build": {
    "context": "..",
    "dockerfile": "Dockerfile"
  },
  "runArgs": ["--runtime=nvidia", "--gpus=all"],
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-azuretools.vscode-docker",
        "ms-toolsai.jupyter",
        "yahyabatulu.vscode-markdown-alert",
        "tomoki1207.pdf",
        "mechatroner.rainbow-csv"
      ]
    }
  }
}
</file>

<file path="setup/03_optional-docker-environment/.devcontainer/Dockerfile">
# Install PyTorch 2.5 with CUDA 12.4
FROM pytorch/pytorch:2.5.0-cuda12.4-cudnn9-runtime

# Install Ubuntu packages
RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y rsync && \
    apt-get install -y git && \
    apt-get install -y curl && \
    rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements.txt requirements.txt
RUN pip install --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt
</file>

<file path="setup/03_optional-docker-environment/.devcontainer/README.md">
# Optional Docker Environment

This is an optional Docker environment for those users who prefer Docker. In case you are interested in using this Docker DevContainer, please see the *Using Docker DevContainers* section in the [../../README.md](../../README.md) for more information.
</file>

<file path="setup/04_optional-aws-sagemaker-notebook/cloudformation-template.yml">
AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template to create a GPU-enabled Jupyter notebook in SageMaker with an execution role and 
LLMs-from-scratch Repo'

Parameters:
  NotebookName:
    Type: String
    Default: 'LLMsFromScratchNotebook'
  DefaultRepoUrl:
    Type: String
    Default: 'https://github.com/rasbt/LLMs-from-scratch.git'

Resources:
  SageMakerExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - sagemaker.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess

  KmsKey:
    Type: AWS::KMS::Key
    Properties:
      Description: 'KMS key for SageMaker notebook'
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
      EnableKeyRotation: true

  KmsKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub 'alias/${NotebookName}-kms-key'
      TargetKeyId: !Ref KmsKey

  TensorConfigLifecycle:
    Type: AWS::SageMaker::NotebookInstanceLifecycleConfig
    Properties:
      NotebookInstanceLifecycleConfigName: "TensorConfigv241128"
      OnCreate:
        - Content: !Base64 |
            #!/bin/bash
            set -e

            # Create a startup script that will run in the background
            cat << 'EOF' > /home/ec2-user/SageMaker/setup-environment.sh
            #!/bin/bash
            
            sudo -u ec2-user -i <<'INNEREOF'
            unset SUDO_UID

            # Install a separate conda installation via Miniconda
            WORKING_DIR=/home/ec2-user/SageMaker/custom-miniconda
            mkdir -p "$WORKING_DIR"
            wget https://repo.anaconda.com/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O "$WORKING_DIR/miniconda.sh"
            bash "$WORKING_DIR/miniconda.sh" -b -u -p "$WORKING_DIR/miniconda" 
            rm -rf "$WORKING_DIR/miniconda.sh"

            # Ensure we're using the Miniconda conda
            export PATH="$WORKING_DIR/miniconda/bin:$PATH"

            # Initialize conda
            "$WORKING_DIR/miniconda/bin/conda" init bash
            source ~/.bashrc

            # Create and activate environment
            KERNEL_NAME="tensorflow2_p39"
            PYTHON="3.9"
            "$WORKING_DIR/miniconda/bin/conda" create --yes --name "$KERNEL_NAME" python="$PYTHON"
            eval "$("$WORKING_DIR/miniconda/bin/conda" shell.bash activate "$KERNEL_NAME")"

            # Install CUDA toolkit and cuDNN
            "$WORKING_DIR/miniconda/bin/conda" install --yes cudatoolkit=11.8 cudnn

            # Install ipykernel
            "$WORKING_DIR/miniconda/envs/$KERNEL_NAME/bin/pip" install --quiet ipykernel

            # Install PyTorch with CUDA support
            "$WORKING_DIR/miniconda/envs/$KERNEL_NAME/bin/pip3" install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118

            # Install other packages
            "$WORKING_DIR/miniconda/envs/tensorflow2_p39/bin/pip" install tensorflow[gpu]
            "$WORKING_DIR/miniconda/bin/conda" install --yes tensorflow-gpu
            "$WORKING_DIR/miniconda/envs/tensorflow2_p39/bin/pip" install tensorflow==2.15.0
            "$WORKING_DIR/miniconda/bin/conda" install --yes setuptools tiktoken tqdm numpy pandas psutil

            "$WORKING_DIR/miniconda/bin/conda" install -y jupyterlab==4.0
            "$WORKING_DIR/miniconda/envs/tensorflow2_p39/bin/pip" install matplotlib==3.7.1

            # Create a flag file to indicate setup is complete
            touch /home/ec2-user/SageMaker/setup-complete

            INNEREOF
            EOF

            # Make the script executable and run it in the background
            chmod +x /home/ec2-user/SageMaker/setup-environment.sh
            sudo -u ec2-user nohup /home/ec2-user/SageMaker/setup-environment.sh > /home/ec2-user/SageMaker/setup.log 2>&1 &

      OnStart:
        - Content: !Base64 |
            #!/bin/bash
            set -e

            # Check if setup is still running or not started
            if ! [ -f /home/ec2-user/SageMaker/setup-complete ]; then
                echo "Setup still in progress or not started. Check setup.log for details."
                exit 0
            fi

            sudo -u ec2-user -i <<'EOF'
            unset SUDO_UID

            WORKING_DIR=/home/ec2-user/SageMaker/custom-miniconda
            source "$WORKING_DIR/miniconda/bin/activate"

            for env in $WORKING_DIR/miniconda/envs/*; do
                BASENAME=$(basename "$env")
                source activate "$BASENAME"
                python -m ipykernel install --user --name "$BASENAME" --display-name "Custom ($BASENAME)"
            done
            EOF

            echo "Restarting the Jupyter server.."
            CURR_VERSION=$(cat /etc/os-release)
            if [[ $CURR_VERSION == *$"http://aws.amazon.com/amazon-linux-ami/"* ]]; then
                sudo initctl restart jupyter-server --no-wait
            else
                sudo systemctl --no-block restart jupyter-server.service
            fi

  SageMakerNotebookInstance:
    Type: AWS::SageMaker::NotebookInstance
    Properties:
      InstanceType: ml.g4dn.xlarge
      NotebookInstanceName: !Ref NotebookName
      RoleArn: !GetAtt SageMakerExecutionRole.Arn
      DefaultCodeRepository: !Ref DefaultRepoUrl
      KmsKeyId: !GetAtt KmsKey.Arn
      PlatformIdentifier: notebook-al2-v2
      VolumeSizeInGB: 50
      LifecycleConfigName: !GetAtt TensorConfigLifecycle.NotebookInstanceLifecycleConfigName

Outputs:
  NotebookInstanceName:
    Description: The name of the created SageMaker Notebook Instance
    Value: !Ref SageMakerNotebookInstance
  ExecutionRoleArn:
    Description: The ARN of the created SageMaker Execution Role
    Value: !GetAtt SageMakerExecutionRole.Arn
  KmsKeyArn:
    Description: The ARN of the created KMS Key for the notebook
    Value: !GetAtt KmsKey.Arn
</file>

<file path="setup/04_optional-aws-sagemaker-notebook/README.md">
# AWS CloudFormation 模板：包含 LLMs-from-scratch 仓库的 Jupyter Notebook

此 CloudFormation 模板在 Amazon SageMaker 中创建一个启用 GPU 的 Jupyter Notebook，并配置执行角色和 LLMs-from-scratch GitHub 仓库。

## 功能说明：

1. 创建一个具有必要权限的 IAM 角色，用于 SageMaker Notebook 实例。
2. 创建一个 KMS 密钥和别名，用于加密 Notebook 实例。
3. 配置 Notebook 实例生命周期配置脚本，执行以下操作：
   - 在用户主目录下安装独立的 Miniconda。
   - 创建一个自定义 Python 环境，安装 TensorFlow 2.15.0 和 PyTorch 2.1.0，并支持 CUDA。
   - 安装其他常用库，如 Jupyter Lab、Matplotlib 等。
   - 将自定义环境注册为 Jupyter 内核。
4. 使用指定配置创建 SageMaker Notebook 实例，包括启用 GPU 的实例类型、执行角色和默认代码仓库。

## 使用步骤：

1. 下载 CloudFormation 模板文件（`cloudformation-template.yml`）。
2. 在 AWS 管理控制台中，导航至 CloudFormation 服务。
3. 创建新堆栈并上传模板文件。
4. 为 Notebook 实例命名（例如：“LLMsFromScratchNotebook”）（默认使用 LLMs-from-scratch GitHub 仓库）。
5. 审核并接受模板参数后创建堆栈。
6. 堆栈创建完成后，SageMaker Notebook 实例将在 SageMaker 控制台中可用。
7. 打开 Notebook 实例，开始使用预配置环境进行 LLMs-from-scratch 项目开发。

## 关键要点：

- 模板创建启用 GPU 的 Notebook 实例（`ml.g4dn.xlarge`），并配备 50GB 存储。
- 配置一个自定义 Miniconda 环境，安装 TensorFlow 2.15.0 和 PyTorch 2.1.0，支持 CUDA。
- 自定义环境注册为 Jupyter 内核，供 Notebook 使用。
- 模板还创建了一个 KMS 密钥以加密 Notebook 实例，并配置了所需权限的 IAM 角色。
</file>

<file path="setup/README.md">
# 可选的安装设置说明

本文档列出了设置机器并使用本仓库中代码的不同方法。我建议从上到下浏览各个部分，然后决定哪种方法最适合您的需求。

&nbsp;

## 快速开始

如果您的机器上已经安装了Python，最快的开始方式是通过执行以下pip安装命令，从本代码仓库的根目录安装[../requirements.txt](../requirements.txt)文件中的包依赖：
```bash
pip install -r requirements.txt
```

&nbsp;

# 本地设置

本节提供了在本地运行本书代码的建议。请注意，本书主章节中的代码设计为能够在常规笔记本电脑上运行，并且在合理的时间内完成，不需要专用硬件。我在M3款MacBook Air笔记本电脑上测试了所有主章节的代码。此外，如果您的笔记本或台式电脑配备了NVIDIA GPU，代码将自动利用该硬件。

&nbsp;

## 设置Python

如果您的机器上还没有设置Python，我在以下目录中写了我的个人Python设置偏好：

- [01_optional-python-setup-preferences](./01_optional-python-setup-preferences)
- [02_installing-python-libraries](./02_installing-python-libraries)

下面的*使用DevContainers*部分介绍了在您的机器上安装项目依赖的另一种方法。

&nbsp;

## 使用Docker DevContainers

作为上述*设置Python*部分的替代方案，如果您更倾向于一种隔离项目依赖和配置的开发环境，使用Docker是一个非常有效的解决方案。这种方法避免了手动安装软件包和库，并确保开发环境的一致性。您可以在以下链接中找到有关设置Docker和使用DevContainer的更多说明：

- [03_optional-docker-environment](03_optional-docker-environment)

&nbsp;

## Visual Studio Code 编辑器

有许多优秀的代码编辑器可供选择。我的首选是流行的开源编辑器[Visual Studio Code (VSCode)](https://code.visualstudio.com)，它可以通过许多实用的插件和扩展进行轻松增强（更多信息请见下面的*VSCode扩展*部分）。有关macOS、Linux和Windows的下载说明，请访问[VSCode官网](https://code.visualstudio.com)。

&nbsp;

## VSCode 扩展

如果您使用Visual Studio Code (VSCode)作为主要代码编辑器，您可以在`.vscode`子文件夹中找到推荐的扩展。这些扩展提供了增强的功能和对本仓库有帮助的工具。

要安装这些扩展，请在VSCode中打开此“setup”文件夹（文件 -> 打开文件夹...），然后点击右下角弹出菜单中的“安装”按钮。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/README/vs-code-extensions.webp?1" alt="1" width="700">

另外，您也可以将`.vscode`扩展文件夹移动到此GitHub仓库的根目录中：

```bash
mv setup/.vscode ./
```

然后，每次您打开`LLMs-from-scratch`主文件夹时，VSCode会自动检查推荐的扩展是否已经安装在您的系统中。

&nbsp;

# 云资源

本节介绍了运行本书中代码的云平台替代方案。

虽然代码可以在没有专用GPU的常规笔记本电脑和台式机上运行，但使用配备NVIDIA GPU的云平台可以显著提高代码的运行速度，特别是在第5到第7章中。

&nbsp;

## 使用Lightning Studio

为了在云端获得顺畅的开发体验，我推荐使用[Lightning AI Studio](https://lightning.ai/)平台，该平台允许用户设置持久化的环境，并在云端的CPU和GPU上使用VSCode和Jupyter Lab。

一旦您启动了新的Studio，可以打开终端并执行以下设置步骤来克隆仓库并安装依赖：

```bash
git clone https://github.com/rasbt/LLMs-from-scratch.git
cd LLMs-from-scratch
pip install -r requirements.txt
```

（与Google Colab不同，这些步骤只需执行一次，因为Lightning AI Studio环境是持久化的，即使在CPU和GPU机器之间切换，也不需要重新执行。）

接下来，导航到您想要运行的Python脚本或Jupyter笔记本。可选地，您还可以轻松地连接GPU来加速代码的运行速度，例如，在第5章进行LLM预训练或在第6和第7章进行微调时。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/README/studio.webp" alt="1" width="700">

&nbsp;

## 使用Google Colab

要在云端使用Google Colab环境，请访问[https://colab.research.google.com/](https://colab.research.google.com/)，并通过GitHub菜单或将笔记本拖入*上传*字段来打开相应章节的笔记本，如下图所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/README/colab_1.webp" alt="1" width="700">


同时，请确保将相关文件（数据集文件和笔记本所导入的.py文件）上传到Colab环境中，如下所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/README/colab_2.webp" alt="2" width="700">


您还可以通过更改*运行时*来选择性地在GPU上运行代码，如下图所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/README/colab_3.webp" alt="3" width="700">

&nbsp;

# 问题的交流

如果您有任何问题，请随时通过本GitHub仓库中的[讨论区](https://github.com/rasbt/LLMs-from-scratch/discussions)与他们联系。
</file>

<file path=".repomixignore">
.git/
imgs/
**/.ipynb_checkpoints/
**/__pycache__/
.venv/
setup/.vscode/
ch02/02_bonus_bytepair-encoder/gpt2_model/encoder.json
ch02/02_bonus_bytepair-encoder/gpt2_model/vocab.bpe
ch06/01_main-chapter-code/sms_spam_collection*
ch06/01_main-chapter-code/*.csv
ch07/**/instruction-data*.json
</file>

<file path="AGENTS.md">
# Repository Guidelines

本仓库是上游 `rasbt/LLMs-from-scratch` 的中文翻译版本（含笔记与代码）。贡献应以翻译准确性、表达清晰和小范围修复为主，避免会偏离书籍内容的大改动/重构。

## 项目结构与模块组织

- `ch01/` … `ch07/`：各章材料（以 `*.ipynb` 为主，配套 `*.py`）。
  - 主要实现通常在 `chXX/01_main-chapter-code/`（例如 `ch04/01_main-chapter-code/gpt.py`）。
  - 可选/扩展内容通常在 `02_*`、`03_*` 等同级目录。
- `appendix-*/`：附录笔记本与脚本。
- `setup/`：环境搭建说明、Docker/DevContainer、环境检查脚本。
- `imgs/`：文档图片与资源。

## 构建、测试与开发命令

- 使用 conda 创建环境 `llms` 并安装核心依赖：
  - `conda create -n llms python=3.10 -y -c defaults`
  - `conda activate llms`
  - `python -m pip install -r requirements.txt`
- 启动笔记本：`jupyter lab`
- 运行环境检查：`python setup/02_installing-python-libraries/python_environment_check.py`
- 部分目录需要额外依赖（见各自的 `requirements-extra.txt`），例如：`pip install -r ch07/06_user_interface/requirements-extra.txt`

## 代码风格与命名约定

- Python：4 空格缩进，遵循常见 PEP 8 命名（函数/变量 `snake_case`，类 `CapWords`），并尽量与当前章节既有风格保持一致。
- 保持章节目录结构（例如 `ch05/07_gpt_to_llama/...`）与相对导入可用；不要随意挪动文件/重命名入口。
- 不要提交生成物：`.ipynb_checkpoints/`、`.DS_Store`、以及包含大量输出的笔记本。

## 测试指南

- 有测试的地方通常使用 `pytest`（常见为 `tests.py` 或 `tests/test*.py`）。
- 示例：`python -m pytest setup/02_installing-python-libraries/tests.py`
- Llama 转换相关测试：先 `pip install -r ch05/07_gpt_to_llama/tests/test-requirements-extra.txt`，再 `python -m pytest ch05/07_gpt_to_llama/tests`

## Commit 与 Pull Request 指南

- 历史提交信息较随意（常见 `update`、`Update README.md` 等）。新提交建议用“章节 + 动作”的短句，例如：`ch03: 修正翻译错字`、`ch07: 更新说明文档`。
- PR 需说明改动范围，关联 issue/discussion；涉及 UI 或笔记本展示变更时附截图；新增依赖请在描述中说明（指向对应 `requirements(-extra).txt`）。
- 不要提交真实密钥：仓库中的 `config.json` 仅为占位示例；请使用本地私有配置或环境变量（例如 `OPENAI_API_KEY`）。

## 学习建议（面向读者）

> 说明：本节是给学习者的使用建议，不影响仓库代码结构与贡献规范。

### 快速上手（先跑通环境）

- 使用 conda 创建环境 `llms` 并安装依赖：
  - `conda create -n llms python=3.10 -y -c defaults`
  - `conda activate llms`
  - `python -m pip install -r requirements.txt`
- （可选）运行环境检查：`python setup/02_installing-python-libraries/python_environment_check.py`
- 启动 Jupyter：`jupyter lab`

### 推荐学习路径（主线优先）

- 先看 `ch01/README.md`（概览与学习路线，本章无代码）
- 按章节主线跑通：`ch02/01_main-chapter-code/ch02.ipynb` → `ch03/01_main-chapter-code/ch03.ipynb` → `ch04/01_main-chapter-code/ch04.ipynb` → `ch05/01_main-chapter-code/ch05.ipynb` → `ch06/01_main-chapter-code/ch06.ipynb` → `ch07/01_main-chapter-code/ch07.ipynb`
- 需要加餐时再看同章的 `02_*`、`03_*` 等 bonus 目录

### 学习方法（每章通用）

- 先“跑通 notebook”，再对照阅读同目录的 `.py`（通常是把 notebook 逻辑整理成模块/脚本）
- 用“小改动实验”建立直觉：改 `context_length`、`stride`、`batch_size`、学习率等，观察 loss/输出变化
- 有 `tests.py` 的目录用 `pytest` 做自检（例如：`pytest ch04/01_main-chapter-code/tests.py`）
</file>

<file path="CITATION.cff">
cff-version: 1.2.0
message: "If you use this book or its accompanying code, please cite it as follows."
title: "Build A Large Language Model (From Scratch), Published by Manning, ISBN 978-1633437166"
abstract: "This book provides a comprehensive, step-by-step guide to implementing a ChatGPT-like large language model from scratch in PyTorch."
date-released: 2024-09-12
authors:
  - family-names: "Raschka"
    given-names: "Sebastian"
license: "Apache-2.0"
url: "https://www.manning.com/books/build-a-large-language-model-from-scratch"
repository-code: "https://github.com/rasbt/LLMs-from-scratch"
keywords:
  - large language models
  - natural language processing
  - artificial intelligence
  - PyTorch
  - machine learning
  - deep learning
</file>

<file path="LICENSE.txt">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications, 
      explicitly excluding any books specific to this software and any related images, 
      and includes but is not limited to software source code,
      documentation source (excluding books and images related to this software),
      and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2023-2025 Sebastian Raschka

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="requirements.txt">
torch >= 2.0.1        # all
jupyterlab >= 4.0     # all
tiktoken >= 0.5.1     # ch02; ch04; ch05
matplotlib >= 3.7.1   # ch04; ch05
tensorflow >= 2.15.0  # ch05
tqdm >= 4.66.1        # ch05; ch07
numpy >= 1.25, < 2.0  # dependency of several other libraries like torch and pandas
pandas >= 2.2.1       # ch06
psutil >= 5.9.5       # ch07; already installed automatically as dependency of torch
</file>

<file path="the-verdict.txt">
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. "Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of." The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's "Moon-dancers" to say, with tears in her eyes: "We shall not look upon its like again"?

Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome "obituary" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of "Gisburns" went up.

It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had "dragged him down." For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.

Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to "lift him up"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.

The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.

I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was "interesting": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the "deadening atmosphere of mediocrity" (I quote Miss Croft) was having on him.

I have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.

"Money's only excuse is to put beauty into circulation," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: "Jack is so morbidly sensitive to every form of beauty."

Poor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.

"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle," was his only protest, as he rose from the table and strolled out onto the sunlit terrace.

I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend's feet, and I wondered if a tinge of jealousy underlay the latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their "Grindles."

I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.

"Why _has_ he chucked painting?" I asked abruptly.

She raised her eyebrows with a hint of good-humoured surprise.

"Oh, he doesn't _have_ to now, you know; and I want him to enjoy himself," she said quite simply.

I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.

"Has he chucked his pictures too? I haven't seen a single one in the house."

A slight shade of constraint crossed Mrs. Gisburn's open countenance. "It's his ridiculous modesty, you know. He says they're not fit to have about; he's sent them all away except one--my portrait--and that I have to keep upstairs."

His ridiculous modesty--Jack's modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: "I must really see your portrait, you know."

She glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound's head between his knees.

"Well, come while he's not looking," she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.

In the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!

Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: "If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay."

Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's "strongest," as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown's ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted "strongly" because she was tired of being painted "sweetly"--and yet not to lose an atom of the sweetness.

"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride. "The last but one," she corrected herself--"but the other doesn't count, because he destroyed it."

"Destroyed it?" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.

As he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.

His wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.

"Mr. Rickham wanted to see it," she began, as if excusing herself. He shrugged his shoulders, still smiling.

"Oh, Rickham found me out long ago," he said lightly; then, passing his arm through mine: "Come and see the rest of the house."

He showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire's domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: "Yes, I really don't see how people manage to live without that."

Well--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: "Be dissatisfied with your leisure!" as once one had longed to say: "Be dissatisfied with your work!"

But, with the cry on my lips, my diagnosis suffered an unexpected check.

"This is my own lair," he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no "effects"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.

The fact brought home to me the absolute finality of Jack's break with his old life.

"Don't you ever dabble with paint any more?" I asked, still looking about for a trace of such activity.

"Never," he said briefly.

"Or water-colour--or etching?"

His confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.

"Never think of it, my dear fellow--any more than if I'd never touched a brush."

And his tone told me in a flash that he never thought of anything else.

I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.

"Oh, by Jove!" I said.

It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.

"By Jove--a Stroud!" I cried.

He was silent; but I felt him close behind me, breathing a little quickly.

"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?"

He answered slowly: "Mrs. Stroud gave it to me."

"Ah--I didn't know you even knew the Strouds. He was such an inflexible hermit."

"I didn't--till after. . . . She sent for me to paint him when he was dead."

"When he was dead? You?"

I must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: "Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter."

"Ah, poor Stroud--as you say. Was _that_ his history?"

"That was his history. She believed in him, gloried in him--or thought she did. But she couldn't bear not to have all the drawing-rooms with her. She couldn't bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She's just a fragment groping for other fragments. Stroud is the only whole I ever knew."

"You ever knew? But you just said--"

Gisburn had a curious smile in his eyes.

"Oh, I knew him, and he knew me--only it happened after he was dead."

I dropped my voice instinctively. "When she sent for you?"

"Yes--quite insensible to the irony. She wanted him vindicated--and by me!"

He laughed again, and threw back his head to look up at the sketch of the donkey. "There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. That's the reason why I don't dabble any more, my dear Rickham; or rather Stroud himself is the reason."

For the first time my idle curiosity about my companion turned into a serious desire to understand him better.

"I wish you'd tell me how it happened," I said.

He stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.

"I'd rather like to tell you--because I've always suspected you of loathing my work."

I made a deprecating gesture, which he negatived with a good-humoured shrug.

"Oh, I didn't care a straw when I believed in myself--and now it's an added tie between us!"

He laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. "There: make yourself comfortable--and here are the cigars you like."

He placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.

"How it happened? I can tell you in five minutes--and it didn't take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud's note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.

"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud's career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.

"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.

"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a 'subject.' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.

"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn't watching the showy bits--I couldn't distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!

"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .

"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn't been born of me--I had just adopted them. . . .

"Hang it, Rickham, with that face watching me I couldn't do another stroke. The plain truth was, I didn't know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don't you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my 'technique' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'

"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?

"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . ."

He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.

"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day."

And, in answer to a question I put half-mechanically--"Begin again?" he flashed out. "When the one thing that brings me anywhere near him is that I knew enough to leave off?"

He stood up and laid his hand on my shoulder with a laugh. "Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art."
</file>

<file path="appendix-D/01_main-chapter-code/appendix-D.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5936bd-af17-4a7e-a4d2-e910411708ea",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53bcb1-ff9d-49c7-a0bc-5b8d32ff975b",
   "metadata": {},
   "source": [
    "## 附录 D：为训练循环添加额外的功能与优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58c142-9434-49af-b33a-356b80a45b86",
   "metadata": {},
   "source": [
    "- 在本附录中，我们为训练函数添加了一些更高级的功能，这些功能通常用于预训练和微调；微调将在第6章和第7章中详细讨论。\n",
    "- 以下三节将介绍学习率预热、余弦衰减和梯度裁剪。\n",
    "- 最后一节将这些技术整合到训练函数中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744def4f-c03f-42ee-97bb-5d7d5b89b723",
   "metadata": {},
   "source": [
    "- 我们首先通过复用第5章的代码来初始化一个模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8755bd5e-bc06-4e6e-9e63-c7c82b816cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "# 输出当前 PyTorch 的版本\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "# 定义 GPT 模型的配置\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度 （原始长度:1024）\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头的数量\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # Dropout 比率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "# 根据可用的设备选择运行环境\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 提示：\n",
    "# 如果需要在 Apple Silicon 芯片上运行，可以取消注释以下代码。\n",
    "# 经测试，在 M3 MacBook Air 上运行速度约为 Apple CPU 的两倍。\n",
    "# 注意：但这样可能会导致损失值略有不同。\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 初始化模型\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()  # 禁用推理期间的 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51574e57-a098-412c-83e8-66dafa5a0b99",
   "metadata": {},
   "source": [
    "- 用跟第五章相同的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386ca110-2bb4-42f1-bd54-8836df80acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae96992b-536a-4684-a924-658b9ffb7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# 训练集/验证集比率\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    text_data[:split_idx],\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    text_data[split_idx:],\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c08d8-257a-41c6-b842-019f7897ac74",
   "metadata": {},
   "source": [
    "## D.1 学习率预热"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafcd30-ddf7-4a9f-bcf4-b13c052b3133",
   "metadata": {},
   "source": [
    "- 引入学习率预热（Learning Rate Warmup）可以帮助稳定复杂模型（如大语言模型）的训练过程。\n",
    "- 在学习率预热中，我们将学习率从一个非常低的初始值（`initial_lr`）逐渐增加到用户指定的最大值（`peak_lr`）。\n",
    "- 通过这种方式，模型在训练开始时会以较小的权重更新启动，从而降低训练过程中因大幅更新导致不稳定的风险。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb4790b-b8b6-4e9e-adf4-704a04b31ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "initial_lr = 0.0001\n",
    "peak_lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3a8da-abc4-4b80-a5d8-f1cc1c7cc5f3",
   "metadata": {},
   "source": [
    "- 通常，预热步数占总步数的 0.1% 到 20%。\n",
    "- 我们可以将学习率的增量计算为 `peak_lr` 和 `initial_lr` 之差，再除以预热步数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6d083f-1b25-4c23-b46d-ef7783446690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * n_epochs\n",
    "warmup_steps = int(0.2 * total_steps) # 20%预热\n",
    "print(warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bbdc8-0104-459e-a7ed-b08be8578709",
   "metadata": {},
   "source": [
    "- 请注意，印刷版书籍中意外包含了一行多余的代码 `warmup_steps = 20`，该代码未被使用，可以安全忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e075f80e-a398-4809-be1d-8019e1d31c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "global_step = -1\n",
    "track_lrs = []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    \n",
    "        if global_step < warmup_steps:\n",
    "            lr = initial_lr + global_step * lr_increment\n",
    "        else:\n",
    "            lr = peak_lr\n",
    "        \n",
    "        # 将计算后的学习率应用到优化器上\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "        # 计算损失和更新权重\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6da121-eeed-4023-bdd8-3666c594b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Step\")\n",
    "total_training_steps = len(train_loader) * n_epochs\n",
    "plt.plot(range(total_training_steps), track_lrs)\n",
    "plt.tight_layout(); plt.savefig(\"1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3996b6-3f7a-420a-8584-c5760249f3d8",
   "metadata": {},
   "source": [
    "## D.2 余弦衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5216214-de79-40cf-a733-b1049a73023c",
   "metadata": {},
   "source": [
    "- 另一种用于训练复杂深度神经网络的常见技术是余弦衰减，它同样在训练过程中调整学习率。\n",
    "- 在余弦衰减中，学习率遵循一个余弦曲线，按照半余弦周期从初始值逐渐减少到接近零。\n",
    "- 这种逐步减小的设计旨在随着模型权重的优化逐渐减缓学习速度，从而降低在训练后期超过最小值的风险，这对于稳定训练过程至关重要。\n",
    "- 与线性衰减相比，余弦衰减因其学习率调整的平滑过渡而更受欢迎，但线性衰减在实践中也被广泛应用（例如，[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8d2068-a057-4abf-b478-f02cc37191f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "min_lr = 0.1 * initial_lr\n",
    "track_lrs = []\n",
    "\n",
    "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "global_step = -1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    \n",
    "        # 根据当前阶段（预热或余弦退火）调整学习率\n",
    "        if global_step < warmup_steps:\n",
    "            # 线性预热\n",
    "            lr = initial_lr + global_step * lr_increment  \n",
    "        else:\n",
    "            # 在预热后进行余弦衰减\n",
    "            progress = ((global_step - warmup_steps) / \n",
    "                        (total_training_steps - warmup_steps))\n",
    "            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n",
    "                1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        # 将计算后的学习率应用到优化器上\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "        # 计算损失和更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e779e33-8a44-4984-bb23-be0603dc4158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.plot(range(total_training_steps), track_lrs)\n",
    "plt.tight_layout(); plt.savefig(\"2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7512808-b48d-4146-86a1-5931b1e3aec1",
   "metadata": {},
   "source": [
    "## D.3 梯度裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a74f76-8d2b-4974-a03c-d645445cdc21",
   "metadata": {},
   "source": [
    "- 梯度裁剪（Gradient Clipping）是另一种在训练 LLM 时用于稳定训练过程的技术。\n",
    "- 通过设置一个阈值，超出该限制的梯度会被缩小到一个最大幅值，从而确保在反向传播过程中对模型参数的更新保持在可控范围内。\n",
    "- 例如，在 PyTorch 的 `clip_grad_norm_` 方法中设置 `max_norm=1.0`，意味着梯度的范数会被裁剪，确保其最大范数不超过 1.0。\n",
    "- 这里的“范数”指的是梯度向量在模型参数空间中的长度（或幅值）的度量。\n",
    "- 更具体地说，这指的是 L2 范数，也称为欧几里得范数。\n",
    "- 数学上，对于一个向量 $\\mathbf{v}$，其分量为 $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$，L2 范数定义为：\n",
    "$$\n",
    "\\| \\mathbf{v} \\|_2 = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44838a6-4322-47b2-a935-c00d3a88355f",
   "metadata": {},
   "source": [
    "- L2 范数对于矩阵的计算方式类似。\n",
    "- 假设我们的梯度矩阵为：\n",
    "$$\n",
    "G = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- 我们希望将这些梯度裁剪到 `max_norm` 为 1。\n",
    "\n",
    "- 首先，计算这些梯度的 L2 范数：\n",
    "$$\n",
    "\\|G\\|_2 = \\sqrt{1^2 + 2^2 + 2^2 + 4^2} = \\sqrt{25} = 5\n",
    "$$\n",
    "\n",
    "- 由于 $\\|G\\|_2 = 5$ 大于我们的 `max_norm`（1），需要缩放梯度，使它们的范数正好为 1。缩放因子计算为：\n",
    "$$\n",
    "\\text{缩放因子} = \\frac{max\\_norm}{\\|G\\|_2} = \\frac{1}{5}\n",
    "$$\n",
    "\n",
    "- 因此，缩放后的梯度矩阵 $G'$ 为：\n",
    "$$\n",
    "G' = \\frac{1}{5} \\times G = \\begin{bmatrix}\n",
    "\\frac{1}{5} & \\frac{2}{5} \\\\\n",
    "\\frac{2}{5} & \\frac{4}{5}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0c3c1-2cff-46f5-8127-24412184428c",
   "metadata": {},
   "source": [
    "- 让我们实际操作一下。\n",
    "- 首先，我们初始化一个新模型，并像在常规训练循环中一样，为一个训练批次计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e199e1ff-58c4-413a-855e-5edbe9292649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import calc_loss_batch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b60f3a-15ec-4846-838d-fdef3df99899",
   "metadata": {},
   "source": [
    "- 调用 `.backward()` 后，PyTorch 会计算梯度，并将其存储在每个权重（参数）矩阵的 `.grad` 属性中。\n",
    "- 让我们定义一个实用函数，用于基于所有模型权重计算最大梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70729a3-24d1-411d-a002-2529cd3a8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0411)\n"
     ]
    }
   ],
   "source": [
    "def find_highest_gradient(model):\n",
    "    max_grad = None\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_values = param.grad.data.flatten()\n",
    "            max_grad_param = grad_values.max()\n",
    "            if max_grad is None or max_grad_param > max_grad:\n",
    "                max_grad = max_grad_param\n",
    "    return max_grad\n",
    "\n",
    "print(find_highest_gradient(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f30e6-6b24-4d4b-ae91-e9a4b871113f",
   "metadata": {},
   "source": [
    "- 在运用梯度裁切后,我们可以发现最大梯度明显的减小了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa81ef8b-4280-400f-a93e-5210f3e62ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0185)\n"
     ]
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "print(find_highest_gradient(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c2af0-dac3-4742-be4b-4292c6753099",
   "metadata": {},
   "source": [
    "## D.4 D.4修改的训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76715332-94ec-4185-922a-75cb420819d5",
   "metadata": {},
   "source": [
    "- 现在，让我们将上述三个概念（学习率预热、余弦衰减和梯度裁剪）添加到第5章中介绍的 `train_model_simple` 函数中，以创建下面更高级的 `train_model` 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46eb9c84-a293-4016-a523-7ad726e171e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import evaluate_model, generate_and_print_sample\n",
    "\n",
    "BOOK_VERSION = True\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device,\n",
    "                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
    "                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 从优化器中获取最大学习率\n",
    "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # 计算训练过程中总的迭代次数\n",
    "    total_training_steps = len(train_loader) * n_epochs\n",
    "\n",
    "    # 计算预热阶段学习率的增量\n",
    "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # 根据当前阶段（预热或余弦衰减）调整学习率\n",
    "            if global_step < warmup_steps:\n",
    "                # 线性预热\n",
    "                lr = initial_lr + global_step * lr_increment  \n",
    "            else:\n",
    "                # 预热后余弦衰减\n",
    "                progress = ((global_step - warmup_steps) / \n",
    "                            (total_training_steps - warmup_steps))\n",
    "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "            # 将计算出的学习率应用到优化器中\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "            track_lrs.append(lr)  # 记录当前学习率\n",
    "\n",
    "            # 计算并反向传播损失\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # 在预热阶段后应用梯度裁剪，防止梯度爆炸\n",
    "            if BOOK_VERSION:\n",
    "                if global_step > warmup_steps:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            else:\n",
    "                if global_step >= warmup_steps:  # 原书版本使用 global_step > warmup_steps，导致预热后跳过一次梯度裁剪\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            # 定期对训练集和验证集进行评估\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader,\n",
    "                    device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                # 打印当前损失\n",
    "                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # 生成并打印模型的采样输出以监控训练进展\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen, track_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55fcd247-ba9d-4b93-a757-0f7ce04fee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Iter 000000): Train loss 10.934, Val loss 10.939\n",
      "Ep 1 (Iter 000005): Train loss 9.151, Val loss 9.461\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Iter 000010): Train loss 7.949, Val loss 8.184\n",
      "Ep 2 (Iter 000015): Train loss 6.362, Val loss 6.876\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,, the,,,,,,,,, the,,,,,,,,,,, the,,,,,,,,\n",
      "Ep 3 (Iter 000020): Train loss 5.851, Val loss 6.607\n",
      "Ep 3 (Iter 000025): Train loss 5.750, Val loss 6.634\n",
      "Every effort moves you. \"I\"I and I had to the to the to the and the of the to the of the to Gisburn, and the of the the of the of the to the to the of the of the of the to the of\n",
      "Ep 4 (Iter 000030): Train loss 5.225, Val loss 6.944\n",
      "Ep 4 (Iter 000035): Train loss 4.304, Val loss 6.512\n",
      "Every effort moves you know   \"--and--and--I                 \", and, and, and, and I had been, and, and \" it.   \n",
      "Ep 5 (Iter 000040): Train loss 3.736, Val loss 6.383\n",
      "Every effort moves you know the picture to have the picture--his--his, the donkey of a little: \"strong, with a little of the donkey, in the picture--as, with a little of his painting, the donkey, the donkey, with a little\n",
      "Ep 6 (Iter 000045): Train loss 2.395, Val loss 6.244\n",
      "Ep 6 (Iter 000050): Train loss 2.948, Val loss 6.279\n",
      "Every effort moves you?\"     I, and he had a little the in a flash that he was a little the fact, and in the picture. Gisburn's my unexpected discovery; and as I had the picture--the. He was his\n",
      "Ep 7 (Iter 000055): Train loss 2.316, Val loss 6.169\n",
      "Ep 7 (Iter 000060): Train loss 1.003, Val loss 6.343\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, so inevitably the last word. Gisburn's past! The women had been his pictures I remember getting off a prodigious phrase about the honour being _mine_--because he didn't say\n",
      "Ep 8 (Iter 000065): Train loss 0.860, Val loss 6.348\n",
      "Ep 8 (Iter 000070): Train loss 1.117, Val loss 6.375\n",
      "Every effort moves you?\" \"I that my hostess was \"interesting\": on that point I could have given Miss Croft the fact, and Mrs. \"I must have Jack himself, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Ep 9 (Iter 000075): Train loss 0.367, Val loss 6.498\n",
      "Ep 9 (Iter 000080): Train loss 0.289, Val loss 6.612\n",
      "Every effort moves you?\" \" on--forming, as it were, so inevitably the background of the house.\"  \" went on groping and muddling; then I looked at the donkey again. I may be pardoned the bull--that I found\n",
      "Ep 10 (Iter 000085): Train loss 0.263, Val loss 6.700\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 11 (Iter 000090): Train loss 0.151, Val loss 6.788\n",
      "Ep 11 (Iter 000095): Train loss 0.097, Val loss 6.805\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 12 (Iter 000100): Train loss 0.081, Val loss 6.832\n",
      "Ep 12 (Iter 000105): Train loss 0.089, Val loss 6.900\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 13 (Iter 000110): Train loss 0.045, Val loss 6.911\n",
      "Ep 13 (Iter 000115): Train loss 0.047, Val loss 6.903\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 14 (Iter 000120): Train loss 0.038, Val loss 6.907\n",
      "Ep 14 (Iter 000125): Train loss 0.040, Val loss 6.912\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 15 (Iter 000130): Train loss 0.041, Val loss 6.915\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 注意：\n",
    "# 取消注释以下代码以计算执行时间\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "peak_lr = 0.001  # 书中原始设置为 5e-4，这是一个错误\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # 书中错误地省略了 lr 的设置\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "n_epochs = 15\n",
    "train_losses, val_losses, tokens_seen, lrs = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
    "    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer, warmup_steps=warmup_steps, \n",
    "    initial_lr=1e-5, min_lr=1e-5\n",
    ")\n",
    "\n",
    "# 注意：\n",
    "# 取消注释以下代码以显示执行时间\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"训练完成，用时 {execution_time_minutes:.2f} 分钟。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e8d5e-0872-4b90-98ac-200c80ee2d53",
   "metadata": {},
   "source": [
    "- 从上面的结果可以看出，模型一开始会生成无法理解的词串，而到训练后期，它能够生成语法上或多或少正确的句子。\n",
    "- 如果检查模型在训练后期生成的一些段落，会发现其中部分内容与训练集中的内容高度相似——这表明模型只是简单地记住了训练数据。\n",
    "- 请注意，这里的过拟合是由于训练集非常非常小，并且我们对其进行了过多次迭代。\n",
    "  - 此处的 LLM 训练主要是出于教学目的；我们的主要目标是验证模型是否能够学习生成连贯的文本。\n",
    "  - 为避免花费数周或数月时间在昂贵硬件上训练大量数据，我们直接加载了预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decec45-4fdf-4ff6-85a7-1806613f8af7",
   "metadata": {},
   "source": [
    "- 画图看一下效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ebb8d2-8308-4a83-a2a6-730c3bf84452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(range(len(lrs)), lrs)\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f85b01-859b-4454-a3a3-c7ef593735a6",
   "metadata": {},
   "source": [
    "- 看一下损失图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "445d8155-6eae-4b50-a381-d0820ebc27cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(1, n_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "plt.tight_layout(); plt.savefig(\"3.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fa614-67e1-4254-8b7e-c3e2f690c29c",
   "metadata": {},
   "source": [
    "- 请注意，模型在此处出现过拟合是因为数据集被故意保持得非常小，这是为了教学目的（以便代码可以在笔记本电脑上运行）。\n",
    "- 如果需要更长时间的预训练并使用更大的数据集，请参见 [../../ch05/03_bonus_pretraining_on_gutenberg](../../ch05/03_bonus_pretraining_on_gutenberg)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99311e42-8467-458d-b918-632c8840b96f",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {},
   "source": [
    "# 第二章课后练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9978c-6d8e-401b-9731-bec3802cbb96",
   "metadata": {},
   "source": [
    "本次notebook所需的包如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b55ed6-3312-4e30-89b8-51dc8a4a908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# 练习 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7614337f-f639-42c9-a99b-d33f74fa8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f235d87-be85-4ddf-95a6-af59fca13d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e4e8f0-3272-48bb-96f6-cced5584ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 -> Ak\n",
      "86 -> w\n",
      "343 -> ir\n",
      "86 -> w\n",
      "220 ->  \n",
      "959 -> ier\n"
     ]
    }
   ],
   "source": [
    "for i in integers:\n",
    "    print(f\"{i} -> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33901]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Ak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3664332-e6bb-447e-8b96-203aafde8b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2773c09d-c136-4372-a2be-04b58d292842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[343]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6abd32-1e0a-4038-9dd2-673f47bcdeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ae940a-9841-4e27-a1df-b83fc8a488b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a606c39a-6747-4cd8-bb38-e3183f80908d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[959]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"ier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c7268d-8fdc-4957-bc68-5be6113f45a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([33901, 86, 343, 86, 220, 959])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5034a-95ed-46d8-9972-589354dc9fd4",
   "metadata": {},
   "source": [
    "# 练习 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d50af16-937b-49e0-8ffd-42d30cbb41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 4\n",
    "context_length = max_len\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0128eefa-d7c8-4f76-9851-566dfa7c3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  367],\n",
       "        [2885, 1464],\n",
       "        [1807, 3619],\n",
       "        [ 402,  271]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=4, max_length=2, stride=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5c1e90-c6de-4a87-adf6-7e19f603291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
       "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n",
       "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
       "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=4, max_length=8, stride=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c503e5ef-6bb4-45c3-ac49-0e016cedd8d0",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e554f-58e3-4787-832d-d149add1b857",
   "metadata": {},
   "source": [
    "- 通过取消注释并运行以下单元格来安装此Notebook所需的额外软件包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70bae22-b540-4a13-ab01-e748cb9d55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c59bb-5922-46fc-a787-1369d70925b4",
   "metadata": {},
   "source": [
    "# 比较各种字节对编码（BPE）实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adc3bf-353c-411e-a471-0e92786e7103",
   "metadata": {},
   "source": [
    "## 使用来自 `tiktoken` 的BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c490fca-a48a-47fa-a299-322d1a08ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0952667c-ce84-4f21-87db-59f52b44cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b039c350-18ad-48fb-8e6a-085702dfc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b152ba4-04d3-41cc-849f-adedcfb8cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world. Is this-- a test?\n"
     ]
    }
   ],
   "source": [
    "strings = tik_tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf148a1a-316b-43ec-b7ba-1b6d409ce837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tik_tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b5d4f-2af9-40de-828c-063c4243e771",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 使用GPT-2中使用的原始BPE实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0903108c-65cb-4ae1-967a-2155e25349c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_openai_gpt2 import get_encoder, download_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35dd8d7c-8c12-4b68-941a-0fd05882dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching encoder.json: 1.04Mit [00:00, 3.14Mit/s]                                                   \n",
      "Fetching vocab.bpe: 457kit [00:00, 1.67Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "download_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1888a7a9-9c40-4fe0-99b4-ebd20aa1ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_tokenizer = get_encoder(model_name=\"gpt2_model\", models_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2740510c-a78a-4fba-ae18-2b156ba2dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = orig_tokenizer.encode(text)\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434d115e-990d-42ad-88dd-31323a96e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world. Is this-- a test?\n"
     ]
    }
   ],
   "source": [
    "strings = orig_tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e8c6-707c-4d66-bcf8-dd790647cc86",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 通过Hugging Face transformers使用BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9077bf4-f91f-42ad-ab76-f3d89128510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.34.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9839137-b8ea-4a2c-85fc-9a63064cf8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4df871bb797435787143a3abe6b0231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11b27a4aabf43af9bf57f929683def6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aa9a24aacc43108ef2ed72e7bacd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9341bc23b594bb68dcf8954bff6d9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f55f2f1dbc4152acc9b2061167ee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222cbd69-6a3d-4868-9c1f-421ffc9d5fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer(strings)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a1ade-3401-4f2e-9017-7f58a60cbd98",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 快速性能基准测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61bb445-b151-4a2f-8180-d4004c503754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../01_main-chapter-code/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f7c0a3-c1fd-4313-af34-68e78eb33653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29 ms ± 46.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit orig_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "036dd628-3591-46c9-a5ce-b20b105a8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 ms ± 9.71 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tik_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c85b58-bfbc-465e-9a7e-477e53d55c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46 ms ± 48.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer(raw_text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7117107f-22a6-46b4-a442-712d50b3ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.36 ms ± 184 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer(raw_text, max_length=5145, truncation=True)[\"input_ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7488a4-2d2a-48eb-ad8c-534a2974154b",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063850ab-22b0-4838-b53a-9bb11757d9d0",
   "metadata": {},
   "source": [
    "# 理解嵌入层和线性层之间的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315c598-701f-46ff-8806-15813cad0e51",
   "metadata": {},
   "source": [
    "- PyTorch中的嵌入层与执行矩阵乘法的线性层实现相同的功能；我们使用嵌入层的原因是计算效率\n",
    "- 我们将通过PyTorch中的代码示例一步步查看这种关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061720f4-f025-4640-82a0-15098fa94cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7895a66-7f69-4f62-9361-5c9da2eb76ef",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## Using nn.Embedding(这真的不知道怎么翻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc489ea5-73db-40b9-959e-0d70cae25f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设我们有以下三个训练样本，\n",
    "# 它们可能表示LLM上下文中的token ID\n",
    "idx = torch.tensor([2, 3, 1])\n",
    "\n",
    "# 嵌入矩阵的行数可以通过获取最大token ID + 1来确定。\n",
    "# 如果最高的token ID是3，那么我们需要4行，对应可能的\n",
    "# token ID 0、1、2、3\n",
    "num_idx = max(idx)+1\n",
    "\n",
    "# 所需的嵌入维度是一个超参数\n",
    "out_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d83a6e-8543-40af-b253-fe647640bf36",
   "metadata": {},
   "source": [
    "- 如果用简单的embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a7c104-36e1-4b28-bd02-a24a1099dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用随机种子来保证可重复性，因为\n",
    "# 嵌入层中的权重是用小的随机值初始化的\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding = torch.nn.Embedding(num_idx, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96c00a-3297-4a50-8bfc-247aaea7e761",
   "metadata": {},
   "source": [
    "我们可以快速地查看下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595f603e-8d2a-4171-8f94-eac8106b2e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n",
       "        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eb562-61e2-4171-ab6e-b410a1fd5c18",
   "metadata": {},
   "source": [
    "- 然后，我们可以使用嵌入层来获取ID为1的训练样本的向量表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbc0255-4805-4be9-9f4c-1d0d967ef9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d47f2-4691-47b8-9855-2528b6c285c9",
   "metadata": {},
   "source": [
    "- 可视化处理过的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ffd155-7190-44b1-b6b6-45b11d6fe83b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/1.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1311b-cfb2-4afc-9e25-e4ecf35370d9",
   "metadata": {},
   "source": [
    "- 类似地，我们可以使用嵌入层来获取ID为2的训练样本的向量表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c309266a-c601-4633-9404-2e10b1cdde8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3b601-f68c-41b1-a28d-b624b94ef383",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/2.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd54bd-85ae-4887-9c5e-3139da361cf4",
   "metadata": {},
   "source": [
    "- 现在，让我们转换之前定义的所有训练样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0191aa4b-f6a8-4b0d-9c36-65e82b81d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([2, 3, 1])\n",
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cf8eb-c517-4cd4-aa91-0e818fed7651",
   "metadata": {},
   "source": [
    "- 底层实现本质上仍然是相同的查找机制："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392eb43-0bda-4821-b446-b8dcbee8ae00",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/3.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe863b-d6a3-48f3-ace5-09ecd0eb7b59",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## Using nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138de6a4-2689-4c1f-96af-7899b2d82a4e",
   "metadata": {},
   "source": [
    "- 现在，我们将演示上面的嵌入层与PyTorch中`nn.Linear`层在one-hot编码表示上的效果完全相同。\n",
    "- 首先，我们将token ID转换为独热编码表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bb56cf-bc73-41ab-b107-91a43f77bdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45dfdf-fb26-4514-a176-75224f5f179b",
   "metadata": {},
   "source": [
    "- 接下来，我们初始化一个`Linear`层，它执行矩阵乘法$X W^\\top$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae04c1ed-242e-4dd7-b8f7-4b7e4caae383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2039,  0.0166, -0.2483,  0.1886],\n",
       "        [-0.4260,  0.3665, -0.3634, -0.3975],\n",
       "        [-0.3159,  0.2264, -0.1847,  0.1871],\n",
       "        [-0.4244, -0.3034, -0.1836, -0.0983],\n",
       "        [-0.3814,  0.3274, -0.1179,  0.1605]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "linear = torch.nn.Linear(num_idx, out_dim, bias=False)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efb98e-5cc4-4e8d-9fe6-ef0ad29ae2d7",
   "metadata": {},
   "source": [
    "- PyTorch中的线性层也是用小的随机权重进行初始化的\n",
    "- 为了与上面的`Embedding`层进行直接对比，我们需要使用相同的小随机权重，这就是为什么我们在这里对他们重新赋值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b90d69-761c-486e-bd19-b38a2988fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116482d-f1f9-45e2-9bf3-7ef5e9003898",
   "metadata": {},
   "source": [
    "- 现在，我们可以在输入的one-hot编码表示上使用线性层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d2b0dd-9f1d-4c0f-bb16-1f6ce6b8ac2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(onehot.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6204bc8-92e2-4546-9cda-574fe1360fa2",
   "metadata": {},
   "source": [
    "- 如我们所见，这与使用嵌入层时得到的结果完全相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b057649-3176-4a54-b58c-fd8fbf818c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e447639-8952-460e-8c8f-cf9e23c368c9",
   "metadata": {},
   "source": [
    "- 实际上，底层对第一个训练样本的token ID进行的计算如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830eccf-a707-4753-a24a-9b103f55594a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/4.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5211a-14e6-46aa-a3a8-14609f086e97",
   "metadata": {},
   "source": [
    "- 对于第二个训练样本的token ID："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f6026-a461-44da-b9c5-f571f8ec8bf3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/5.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2608049-f5d1-49a9-a14b-82695fc32e6a",
   "metadata": {},
   "source": [
    "- 由于每个one-hot编码行中只有一个索引为1，其余的索引都为0（这是设计的结果），因此这个矩阵乘法实际上与one-hot元素的查找相同。\n",
    "- 在one-hot编码上使用矩阵乘法与嵌入层的查找等价，但当处理较大的嵌入矩阵时，这种做法可能效率较低，因为存在大量无意义的零乘法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dec0dfb-3d60-41d0-a63a-b010dce67e32",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e475425-8300-43f2-a5e8-6b5d2de59925",
   "metadata": {},
   "source": [
    "# 从零开始构建字节对编码（BPE）分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc3f3-8ec1-4fd3-b378-d9a3d7807a54",
   "metadata": {},
   "source": [
    "- 这是一个独立的notebook，旨在从零开始实现字节对编码（BPE）分词算法，该算法广泛应用于 GPT-2 至 GPT-4、Llama 3 等大模型。\n",
    "- 分词的具体目的请参见[第二章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)；这里的代码为额外材料，用于解释 BPE 算法。\n",
    "- OpenAI 为训练原始 GPT 模型所实现的 BPE 分词器可以在[此处](https://github.com/openai/gpt-2/blob/master/src/encoder.py)。\n",
    "- BPE 算法最早由 Philip Gage 于 1994 年提出，详见：“[一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)”。\n",
    "- 目前，大多数项目，包括 Llama 3，使用 OpenAI 开源的 [tiktoken 库](https://github.com/openai/tiktoken)，因为其卓越的计算性能；它支持加载预训练的 GPT-2 和 GPT-4 分词器（Llama 3 模型同样是使用 GPT-4 分词器训练的）。\n",
    "- 上述案例与我在此notebook中提供有所区别，其一，我们的分词器主要是用于教学目的；其二，我们构建了一个用于训练分词器的函数。\n",
    "- 还有一个[minBPE](https://github.com/karpathy/minbpe)，可能有更好的性能；与 `minbpe` 不同，书中的代码还支持加载 OpenAI 原始分词器的词汇表与自己的单词表合并。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62336db-f45c-4894-9167-7583095dbdf1",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 1. 字节对编码（BPE）的主要思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f1231-bd42-41b5-a017-974b8c660a44",
   "metadata": {},
   "source": [
    "- BPE 的主要思想是将文本转换为整数表示（token ID）用于大规模语言模型（LLM）的训练（详见[第二章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)）。\n",
    "\n",
    "<img src=\"../image/bpe-overview.webp\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c625d-26a1-4896-98a2-0fdcd1591256",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.1 Bits 和 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddaa35-0ed7-4012-827e-911de11c266c",
   "metadata": {},
   "source": [
    "- 在介绍 BPE 算法之前，我们先来了解一下字节的概念。\n",
    "- 我们将文本转换为字节数组（毕竟 BPE 代表的是“字节”对编码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9bc9e4-120f-4bac-8fa6-6523c568d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd92a2a-9d74-4dc7-bb53-ac33d6cf2fab",
   "metadata": {},
   "source": [
    "- 当我们对 `bytearray` 对象调用 `list()` 时，每个字节会被视为一个独立的元素，结果是一个包含字节值对应整数的列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c586945-d459-4f9a-855d-bf73438ef0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efea37-f4c3-4cb8-bfa5-9299175faf9a",
   "metadata": {},
   "source": [
    "- 这种方式是将文本转换为 LLM 嵌入层中所需的token ID的一种有效方法。\n",
    "- 然而，这种方法的缺点是，它为每个字符分配一个 ID（对于短文本来说，这将产生大量的 ID！）。\n",
    "- 也就是说，对于一个包含 17 个字符的输入文本，我们需要使用 17 个token ID 作为 LLM 的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5b61d9-79a0-48b4-9b3e-64ab595c5b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc833a-c0d4-4d46-9180-c0042fd6addc",
   "metadata": {},
   "source": [
    "- 如果你之前使用过 LLM，你可能知道 BPE 分词器有一个词汇表，其中我们按照整个词或子词分配ID，按照单词。\n",
    "- 例如，GPT-2 分词器将相同的文本（\"This is some text\"）分词为 4 个令牌，而不是 17 个：`1212, 318, 617, 2420`\n",
    "- 你可以使用互动式的 [tiktoken 应用](https://tiktokenizer.vercel.app/?model=gpt2) 或 [tiktoken 库](https://github.com/openai/tiktoken)进行验证：\n",
    "\n",
    "<img src=\"../image/tiktokenizer.webp\" width=\"600px\">\n",
    "\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b99de-cbfc-441c-8b3e-296a5dd7bb27",
   "metadata": {},
   "source": [
    "- 由于一个字节由 8 位组成，因此单个字节可以表示 2<sup>8</sup> = 256 个可能的值，范围从 0 到 255。\n",
    "- 你可以通过执行代码 `bytearray(range(0, 257))` 来确认这一点，代码会提醒你 `ValueError: byte must be in range(0, 256)`。\n",
    "- 一个 BPE 分词器通常将这 256 个值作为其前 256 个单字符令牌；你可以通过运行以下代码来直观地查看这一点：\n",
    "\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �  # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff0207-7f8e-44fa-9381-2a4bd83daab3",
   "metadata": {},
   "source": [
    "- 上述代码中，注意到条目 256 和 257 不是单字符值，而是双字符值（一个空格 + 一个字母），这是原始 GPT-2 BPE 分词器的一个小缺点（在 GPT-4 分词器中已有改进）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241c23a-d487-488d-bded-cdf054e24920",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.2 创建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2ceb7-0b3f-4a62-8dcc-07810cd8886e",
   "metadata": {},
   "source": [
    "- BPE 分词算法的目标是构建一个包含常见子词的词汇表，例如 `298: ent`（这个子词可以在 *entangle, entertain, enter, entrance, entity,* 等词中找到），甚至是完整的单词。\n",
    "\n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d4420-a4c7-4813-916a-06f4f46bc3f0",
   "metadata": {},
   "source": [
    "- BPE 算法最早在 1994 年由 Philip Gage 提出，详见：“[一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)”。\n",
    "- 在我们进入实际代码实现之前，今天用于 LLM 分词器的形式可以总结如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc71db9-b070-48c4-8412-81f45b308ab3",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.3 BPE算法大览\n",
    "\n",
    "**1. 识别出现次数最多的字节对**\n",
    "- 在每次迭代中，扫描文本，找到最常出现的字节对（或字符对）。\n",
    "\n",
    "**2. 替换并记录**\n",
    "\n",
    "- 用一个新的占位符 ID 替换该字节对（该 ID 尚未被使用，例如，如果我们从 0...255 开始，第一个占位符将是 256）。\n",
    "- 将该映射记录在查找表中。\n",
    "- 查找表的大小是一个超参数，称为“词汇表大小”（对于 GPT-2，这个值为 50,257）。\n",
    "\n",
    "**3. 重复直到没有进一步优化**\n",
    "\n",
    "- 持续执行步骤 1 和 2，反复合并最频繁的字节对。\n",
    "- 当无法再进行压缩时停止（例如，没有字节对出现超过一次）。\n",
    "\n",
    "**解压缩（解码）**\n",
    "\n",
    "- 为恢复原始文本，通过查找表将每个 ID 替换为其对应的字节对，逆向执行此过程。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ac9a-3528-4186-9468-8420c7b2ac00",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.4 举个例子\n",
    "### 1.4.1 Concrete example of the encoding part (steps 1 & 2)\n",
    "\n",
    "- 假设训练数据集`the cat in the hat`，我们希望为 BPE 分词器构建词汇表。\n",
    "\n",
    "**第一次迭代**\n",
    "\n",
    "1. 识别最多出现字节对\n",
    "  - 在这段文本中，\"th\" 出现了两次（分别位于开头和第二个 \"e\" 前）。\n",
    "\n",
    "2. 替换并记录\n",
    "  - 将 \"th\" 替换为一个尚未使用的新的令牌 ID，例如 256。\n",
    "  - 新的文本变为：`<256>e cat in <256>e hat`。\n",
    "  - 新的词汇表如下：\n",
    "\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "**第二次迭代**\n",
    "\n",
    "1. **识别最多出现字节对**  \n",
    "   - 在文本 `<256>e cat in <256>e hat` 中，字节对 `<256>e` 出现了两次。\n",
    "\n",
    "2. **替换并记录**  \n",
    "   - 将 `<256>e` 替换为一个尚未使用的新的token ID，例如 257。  \n",
    "   - 更新后的文本为：`<257> cat in <257> hat`。\n",
    "\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   - 更新后的将会是\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "**第三次迭代**\n",
    "\n",
    "1. **识别最多出现字节对**  \n",
    "   - 在文本 `<257> cat in <257> hat` 中，字节对 `<257> ` 出现了两次（一次在开头，一次在“hat”之前）。\n",
    "\n",
    "2. **替换并记录**  \n",
    "   - 将 `<257> ` 替换为一个尚未使用的新的token ID，例如 258。  \n",
    "   - 更新后的文本为：`<258>cat in <258>hat`。\n",
    "\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "- 如此循环\n",
    "\n",
    "### 1.4.2 解码部分的具体示例（步骤 3）\n",
    "\n",
    "- 为了恢复原始文本，我们通过逆向执行过程，将每个token ID 替换为它对应的字节对，按它们引入的逆序进行替换。\n",
    "- 从最终压缩后的文本开始：`<258>cat in <258>hat`\n",
    "- 替换 `<258>` → `<257> `：`<257> cat in <257> hat`\n",
    "- 替换 `<257>` → `<256>e`：`<256>e cat in <256>e hat`\n",
    "- 替换 `<256>` → \"th\"：`the cat in the hat`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324948-ddd0-45d1-8ba8-e8eda9fc6677",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 2. BPE的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca709-40d7-4e3d-bf3e-4f5687a2e19b",
   "metadata": {},
   "source": [
    "- 以下是上述算法的具体代码，作为一个 Python 类，其模仿了 `tiktoken` Python 接口。\n",
    "- 注意，上面的编码部分描述了通过 `train()` 进行的原始训练步骤；然而，`encode()` 方法的工作原理相似：\n",
    "\n",
    "1. 将输入文本拆分为单个字节\n",
    "2. 反复查找并替换（合并）相邻的令牌（字节对），当它们与学习到的 BPE 合并中的任何对匹配时（从最高到最低的“排名”，即按它们被学习的顺序）\n",
    "3. 继续合并，直到不能再应用任何合并\n",
    "4. 最终的token ID 列表就是编码后的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e4a15ec-2667-4f56-b7c1-34e8071b621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # 映射 token_id 到 token_str（例如：{11246: \"some\"}）\n",
    "        self.vocab = {}\n",
    "        # 映射 token_str 到 token_id（例如：{\"some\": 11246}）\n",
    "        self.inverse_vocab = {}\n",
    "        # BPE 合并字典：{(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        从头开始训练 BPE 分词器。\n",
    "\n",
    "        参数：\n",
    "            text (str): 训练文本。\n",
    "            vocab_size (int): 目标词汇表大小。\n",
    "            allowed_special (set): 要包含的特殊令牌集。\n",
    "        \"\"\"\n",
    "\n",
    "        # 预处理：将空格替换为 'Ġ'\n",
    "        # 注意，Ġ 是 GPT-2 BPE 实现的一个特性\n",
    "        # 例如，\"Hello world\" 可能被标记为 [\"Hello\", \"Ġworld\"]\n",
    "        # （GPT-4 BPE 会将其标记为 [\"Hello\", \" world\"]）\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # 初始化词汇表，包含唯一字符，包括 'Ġ'（如果存在）\n",
    "        # 从前 256 个 ASCII 字符开始\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # 扩展 unique_chars，包含处理后的文本中未包含的字符\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # 可选：确保 'Ġ' 包含在内（如果它对文本处理相关）\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # 创建词汇表和逆词汇表\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # 添加允许的特殊令牌\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # 将处理后的文本标记化为令牌 ID\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE 步骤 1-3：反复查找并替换频繁的字节对\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # 无更多字节对可合并，停止训练\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # 使用合并后的令牌构建词汇表\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 OpenAI 的 GPT-2 文件加载预训练的词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇文件路径（GPT-2 称其为 'encoder.json'）。\n",
    "            bpe_merges_path (str): BPE 合并文件路径（GPT-2 称其为 'vocab.bpe'）。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab 将 token_str 映射到 token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}  # token_id: token_str\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}  # token_str: token_id\n",
    "\n",
    "        # 加载 BPE 合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # 如果有头部注释行，跳过它\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) != 2:\n",
    "                    print(f\"第 {rank+1} 行有超过 2 个条目：{line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                    else:\n",
    "                        print(f\"合并的令牌 '{merged_token}' 未在词汇表中找到，跳过。\")\n",
    "                else:\n",
    "                    print(f\"跳过配对 {pair}，因为其中一个令牌不在词汇表中。\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将输入文本编码为令牌 ID 列表。\n",
    "\n",
    "        参数：\n",
    "            text (str): 要编码的文本。\n",
    "\n",
    "        返回：\n",
    "            List[int]: 令牌 ID 列表。\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # 将文本拆分为令牌，确保换行符 intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # 确保 '\\n' 被视为独立的令牌\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # 如果单词前有空格或换行符，添加 'Ġ'\n",
    "            else:\n",
    "                tokens.append(word)  # 处理第一个单词或独立的 '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token 已经存在于词汇表中\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # 尝试通过 BPE 处理子词分词\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        使用 BPE 合并对单个令牌进行分词。\n",
    "\n",
    "        参数：\n",
    "            token (str): 要分词的令牌。\n",
    "\n",
    "        返回：\n",
    "            List[int]: 应用 BPE 后的令牌 ID 列表。\n",
    "        \"\"\"\n",
    "        # 将令牌拆分为单个字符（作为初始令牌 ID）\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"未在词汇表中找到的字符：{missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    i += 2  # 跳过下一个令牌，因为它已被合并\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        将令牌 ID 列表解码回字符串。\n",
    "\n",
    "        参数：\n",
    "            token_ids (List[int]): 要解码的令牌 ID 列表。\n",
    "\n",
    "        返回：\n",
    "            str: 解码后的字符串。\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"未在词汇表中找到令牌 ID {token_id}。\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # 用空格替换 'Ġ'\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        将词汇表和 BPE 合并保存到 JSON 文件。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 保存词汇表的路径。\n",
    "            bpe_merges_path (str): 保存 BPE 合并的路径。\n",
    "        \"\"\"\n",
    "        # 保存词汇表\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k: v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # 保存 BPE 合并作为字典列表\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        从 JSON 文件加载词汇表和 BPE 合并。\n",
    "\n",
    "        参数：\n",
    "            vocab_path (str): 词汇表文件路径。\n",
    "            bpe_merges_path (str): BPE 合并文件路径。\n",
    "        \"\"\"\n",
    "        # 加载词汇表\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # 加载 BPE 合并\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge['pair'])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"无效模式。选择 'most' 或 'least'。\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db7310-79c7-4ee0-b5fa-d760c6e1aa67",
   "metadata": {},
   "source": [
    "- 上面 `BPETokenizerSimple` 类的代码比较多，详细的内容超出了本笔记本的范围，但下一节提供了一个简短的使用概述，用来帮助您更好地理解该类的方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe1836-eed4-40dc-860b-2d23074d067e",
   "metadata": {},
   "source": [
    "## 3. BPE演示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c996c-fd34-484f-a877-13d977214cf7",
   "metadata": {},
   "source": [
    "- 实际上，我强烈推荐使用 [tiktoken](https://github.com/openai/tiktoken)，因为我上面的实现更注重可读性和教育目的，无法保证性能。\n",
    "- 然而，使用方式与 tiktoken 基本相似，不同之处在于 tiktoken 没有训练方法。\n",
    "- 让我们通过以下几个示例来看看我上面的 `BPETokenizerSimple` Python 代码是如何工作的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82acaf6-7ed5-4d3b-81c0-ae4d3559d2c7",
   "metadata": {},
   "source": [
    "### 3.1 训练、编码与解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d197cad-ed10-4a42-b01c-a763859781fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1b6ac-71d3-4817-956a-9bc7e463a84a",
   "metadata": {},
   "source": [
    "- 接下来，我们将训练一个 BPE 分词器，词汇表大小为 1,000。\n",
    "- 词汇表大小默认已经为 255，因此我们实际上只“学习”了 745 个词汇条目。\n",
    "- 作为对比，GPT-2 的词汇表包含 50,257 个token，GPT-4 的词汇表包含 100,256 个令牌（在 tiktoken 中为 `cl100k_base`），GPT-4o 使用 199,997 个token（在 tiktoken 中为 `o200k_base`）；与我们简单的示例文本相比，它们拥有更大的训练集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "027348fd-d52f-4396-93dd-38eed142df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474ff05-5629-4f13-9e03-a47b1e713850",
   "metadata": {},
   "source": [
    "- 查看下vocab里的东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f705a283-355e-4460-b940-06bbc2ae4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9da0f-8a18-41cd-91ea-9ccc2bb5febb",
   "metadata": {},
   "source": [
    "- 合并了 742 次 (~ `1000 - len(range(0, 256))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3da42d1c-f75c-4ba7-a6c5-4cb8543d4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac69c9-8413-482a-8148-6b2afbf1fb89",
   "metadata": {},
   "source": [
    "- 这代表前256是单token单次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a4108-7c8b-4b98-9c67-d622e9cdf250",
   "metadata": {},
   "source": [
    "- 接下来我们要`编码`点内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1db5cce-e015-412b-ad56-060b8b638078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ed1b344-f7d4-4e9e-ac34-2a04b5c5b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 42\n",
      "Number of token IDs: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1cfb9-402a-4e1e-9678-0b7547406248",
   "metadata": {},
   "source": [
    "- 从上述长度可以看出，一个 42 字符的句子被编码为 20 个令牌 ID，相比于基于字符字节的编码，这有效地将输入长度缩短了大约一半。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252693ee-e806-4dac-ab76-2c69086360f4",
   "metadata": {},
   "source": [
    "- 请注意，词汇表本身在 `decoder()` 方法中被使用，该方法允许我们将令牌 ID 映射回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da0e1faf-1933-43d9-b681-916c282a8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b690e83-5d6b-409a-804e-321c287c24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea5d09-e5ef-4721-994b-b9b25662fa0a",
   "metadata": {},
   "source": [
    "- 通过遍历每个令牌 ID，我们可以更好地理解令牌 ID 是如何通过词汇表解码的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b9e6289-92cb-4d88-b3c8-e836d7c8095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea41c6c-5538-4fd5-8b5f-195960853b71",
   "metadata": {},
   "source": [
    "- 如我们所见，大多数token ID 表示 2 字符的子词；这是因为训练数据文本非常简短，重复的单词并不多，而且我们使用了相对较小的词汇表大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600055a3-7ec8-4abf-b88a-c4186fb71463",
   "metadata": {},
   "source": [
    "- 总结来说，调用 `decode(encode())` 应该能够重现任意输入文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7056cb1-a9a3-4cf6-8364-29fb493ae240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This is some text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b42bb-55bc-4c9d-b859-457a28b76302",
   "metadata": {},
   "source": [
    "### 3.2 储存tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86210925-06dc-4e8c-87bd-821569cd7142",
   "metadata": {},
   "source": [
    "- 接下来，让我们看看如何保存训练好的分词器，以便稍后重新使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "955181cb-0910-4c6a-9c22-d8292a3ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e5ccfe7-ac67-42f3-b727-87886a8867f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9bcc2-3b27-4473-b75e-4f289d52a7cc",
   "metadata": {},
   "source": [
    "- 加载的分词器应该能够产生与之前相同的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00d9bf8f-756f-48bf-81b8-b890e2c2ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d10b2-1ab8-44ee-b51a-14248e30d662",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### 3.3 加载来自 OpenAI 的原始 GPT-2 BPE 分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07e031-9495-4af1-929f-3f16cbde82a5",
   "metadata": {},
   "source": [
    "- 最后，让我们加载 OpenAI 的 GPT-2 分词器文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b45b4366-2c2b-4309-9a14-febf3add8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.bpe already exists\n",
      "encoder.json already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_file_if_absent(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response, open(filename, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "for url, filename in files_to_download.items():\n",
    "    download_file_if_absent(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe260a0-1d5f-4bbd-9934-5117052764d1",
   "metadata": {},
   "source": [
    "- 接下来，我们通过 `load_vocab_and_merges_from_openai` 方法加载文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74306e6c-47d3-45a3-9e0f-93f7303ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=\"encoder.json\", bpe_merges_path=\"vocab.bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d012ce-9e87-47d7-8a1b-b6d6294d76c0",
   "metadata": {},
   "source": [
    "- 词汇表大小应为 `50257`，我们可以通过下面的代码进行确认："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bb722b4-dbf5-4a0c-9120-efda3293f132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea44b45-f524-44b5-a53a-f6d7f483fc19",
   "metadata": {},
   "source": [
    "- 现在我们可以通过我们的 `BPETokenizerSimple` 对象使用 GPT-2 分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e4866de7-fb32-4dd6-a878-469ec734641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This is some text\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3da8d9b2-af55-4b09-95d7-fabd983e919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "460deb85-8de7-40c7-ba18-3c17831fa8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tik_tokenizer.encode(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e2dc-f69b-4533-87ef-549e6fb9b5a0",
   "metadata": {},
   "source": [
    "- 你可以通过互动式的 [tiktoken 应用](https://tiktokenizer.vercel.app/?model=gpt2) 或 [tiktoken 库](https://github.com/openai/tiktoken)来确认这是否生成了正确的token：\n",
    "\n",
    "<img src=\"../image/tiktokenizer.webp\" width=\"600px\">\n",
    "\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558af04-483c-4f6b-88f5-a534f37316cd",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ed0e6-ad06-4bb3-bb39-6b8110c1caa4",
   "metadata": {},
   "source": [
    "- 这就是 BPE 的基本原理，包括用于创建新分词器的训练方法，或者从原始 OpenAI GPT-2 模型加载 GPT-2 分词器的词汇表和合并。\n",
    "- 希望这个简短的教程对教学目的有所帮助；如果你有任何问题，请随时在 [这里](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a) 开启一个新的讨论。\n",
    "- 关于与其他分词器实现的性能对比，请参见 [这个笔记本](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb)。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch03/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78224549-3637-44b0-aed1-8ff889c65192",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# 第三章 课后练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513b627b-c197-44bd-99a2-756391c8a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import torch\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfa199-9aee-41d4-a64b-7e3811b9a616",
   "metadata": {},
   "source": [
    "# 练习 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fee2cf5-61c3-4167-81b5-44ea155bbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ea289c-41cd-4416-89dd-dde6383a6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b035143-f4e8-45fb-b398-dec1bd5153d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7591d79c-c30e-406d-adfd-20c12eb448f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0f54f-6bce-46cc-a428-17c2a56557d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340908f8-1144-4ddd-a9e1-a1c5c3d592f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33543edb-46b5-4b01-8704-f7f101230544",
   "metadata": {},
   "source": [
    "# 练习 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588e209-1644-496a-8dae-7630b4ef9083",
   "metadata": {},
   "source": [
    "如果我们想要输出维度为 2，可以像之前的单头注意力那样，将投影维度 `d_out` 更改为 1："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e748ef-3106-4e11-a781-b230b74a0cef",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_out = 1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78234544-d989-4f71-ac28-85a7ec1e6b7b",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[[-9.1476e-02,  3.4164e-02],\n",
    "         [-2.6796e-01, -1.3427e-03],\n",
    "         [-4.8421e-01, -4.8909e-02],\n",
    "         [-6.4808e-01, -1.0625e-01],\n",
    "         [-8.8380e-01, -1.7140e-01],\n",
    "         [-1.4744e+00, -3.4327e-01]],\n",
    "\n",
    "        [[-9.1476e-02,  3.4164e-02],\n",
    "         [-2.6796e-01, -1.3427e-03],\n",
    "         [-4.8421e-01, -4.8909e-02],\n",
    "         [-6.4808e-01, -1.0625e-01],\n",
    "         [-8.8380e-01, -1.7140e-01],\n",
    "         [-1.4744e+00, -3.4327e-01]]], grad_fn=<CatBackward0>)\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdabcb-06cf-4576-b810-d883bbd313ba",
   "metadata": {},
   "source": [
    "# 练习 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9b963-d01f-46e6-96bf-8eb2a54c5e42",
   "metadata": {},
   "source": [
    "```python\n",
    "context_length = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d5290-8e8b-4149-958e-1efb58a69191",
   "metadata": {},
   "source": [
    "可选的参数的数量如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e603c-1658-4da9-9c0b-ef4bc72832b4",
   "metadata": {},
   "source": [
    "```python\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mha)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba00bd-feb0-4424-84cb-7c2b1f908779",
   "metadata": {},
   "source": [
    "```\n",
    "2360064  # (2.36 M)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c1d47-9b95-4bd1-a517-580a6f779c52",
   "metadata": {},
   "source": [
    "GPT-2 模型总共有 1.17 亿个参数，但正如我们所看到的，大部分参数并不在多头注意力模块中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch03/01_main-chapter-code/multihead-attention.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16f748-e12a-44a9-ad2b-81e320efdac4",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# 多头注意力在数据载入的运用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9b5847-0515-45cd-87b0-46541f6a1f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070000fc-a7b7-4c56-a2c0-a938d413a790",
   "metadata": {},
   "source": [
    "完整的章节代码位于[ch03.ipynb](./ch03.ipynb)。\n",
    "\n",
    "该笔记本包含了本章的核心内容——多头注意力实现（以及第二章中的数据加载pipeline）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60dc93-281d-447e-941f-aede0c7ff7fc",
   "metadata": {},
   "source": [
    "## 第二章的数据载入器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed4b7db-3b47-4fd3-a4a6-5f4ed5dd166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []  # 输入ID列表\n",
    "        self.target_ids = []  # 目标ID列表\n",
    "\n",
    "        # 对整个文本进行分词\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        # 使用滑动窗口将文本分割成重叠的最大长度序列\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]  # 输入片段\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]  # 目标片段（右移一个位置）\n",
    "            self.input_ids.append(torch.tensor(input_chunk))  # 将输入片段转换为张量\n",
    "            self.target_ids.append(torch.tensor(target_chunk))  # 将目标片段转换为张量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)  # 返回数据集的大小\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]  # 获取特定索引的输入和目标\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True):\n",
    "    # 初始化分词器\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 创建数据集\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader  # 返回数据加载器\n",
    "\n",
    "\n",
    "with open(\"small-text-sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()  # 读取文本文件\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")  # 初始化分词器\n",
    "encoded_text = tokenizer.encode(raw_text)  # 对文本进行编码\n",
    "\n",
    "vocab_size = 50257  # 词汇表大小\n",
    "output_dim = 256  # 输出维度\n",
    "max_len = 1024  # 最大序列长度\n",
    "context_length = max_len  # 上下文长度\n",
    "\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)  # 创建词嵌入层\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  # 创建位置嵌入层\n",
    "\n",
    "max_length = 4  # 每个输入片段的最大长度\n",
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length)  # 创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3664332-e6bb-447e-8b96-203aafde8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd298bf4-e320-40c1-9084-6526d07e6d5d",
   "metadata": {},
   "source": [
    "# 第三章的多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2297b-a001-49fd-994c-b1700866cd01",
   "metadata": {},
   "source": [
    "## 一种变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44e682d-1c3c-445d-85fa-b142f89f8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    该类实现了因果自注意力机制（Causal Self Attention），\n",
    "    用于自回归模型（例如GPT模型中的注意力层）。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        初始化因果自注意力层。\n",
    "        \n",
    "        参数：\n",
    "        - d_in: 输入维度\n",
    "        - d_out: 输出维度\n",
    "        - context_length: 上下文长度（即注意力机制能“看到”的最大令牌数）\n",
    "        - dropout: Dropout率\n",
    "        - qkv_bias: 是否为查询、键和值使用偏置（默认为False）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        # 定义查询、键、值的线性变换\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)  # 新增的Dropout层\n",
    "\n",
    "        # 注册一个buffer，用于存储因果掩码\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))  # 新增掩码，禁止未来的信息\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数计算因果自注意力输出。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入的张量，形状为 (batch_size, num_tokens, d_in)\n",
    "        \n",
    "        返回：\n",
    "        - context_vec: 自注意力机制的输出，形状为 (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, n_tokens, d_in = x.shape  # 获取输入张量的维度\n",
    "        keys = self.W_key(x)  # 键（K）\n",
    "        queries = self.W_query(x)  # 查询（Q）\n",
    "        values = self.W_value(x)  # 值（V）\n",
    "\n",
    "        # 计算注意力分数（查询和键的点积）\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # 这里的转置（transpose）是为了匹配维度\n",
    "        \n",
    "        # 使用掩码阻止未来的tokens看到当前token\n",
    "        attn_scores.masked_fill_(  # 这里的操作是原地修改\n",
    "            self.mask.bool()[:n_tokens, :n_tokens], -torch.inf)  # 将掩码区域填充为负无穷\n",
    "\n",
    "        # 计算注意力权重并进行softmax归一化\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # 使用Dropout层\n",
    "\n",
    "        # 计算上下文向量（加权和）\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    该类实现了多头注意力机制（Multi-Head Attention）包装器。\n",
    "    它包含多个因果自注意力头，并在输出时将多个头的结果合并。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        初始化多头注意力层。\n",
    "        \n",
    "        参数：\n",
    "        - d_in: 输入维度\n",
    "        - d_out: 输出维度\n",
    "        - context_length: 上下文长度\n",
    "        - dropout: Dropout率\n",
    "        - num_heads: 注意力头的数量\n",
    "        - qkv_bias: 是否为查询、键和值使用偏置（默认为False）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 定义多个因果自注意力头\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalSelfAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]  # 为每个头创建一个CausalSelfAttention实例\n",
    "        )\n",
    "        \n",
    "        # 定义最终的线性变换，用于将多个头的输出合并\n",
    "        self.out_proj = nn.Linear(d_out * num_heads, d_out * num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数，计算多头注意力输出。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入的张量，形状为 (batch_size, num_tokens, d_in)\n",
    "        \n",
    "        返回：\n",
    "        - out: 多头注意力的输出，形状为 (batch_size, num_tokens, d_out * num_heads)\n",
    "        \"\"\"\n",
    "        # 将多个头的输出拼接在一起\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # 通过线性变换得到最终输出\n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7898551e-f582-48ac-9f66-3632abe2a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = max_length\n",
    "d_in = output_dim\n",
    "\n",
    "num_heads=2\n",
    "d_out = d_in // num_heads\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads)\n",
    "\n",
    "batch = input_embeddings\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e288239-5146-424d-97fe-74024ae711b9",
   "metadata": {},
   "source": [
    "## 另一种变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2773c09d-c136-4372-a2be-04b58d292842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    该类实现了多头自注意力机制（Multi-Head Attention），\n",
    "    用于自回归模型（如Transformer和GPT中的注意力层）。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        初始化多头自注意力层。\n",
    "        \n",
    "        参数：\n",
    "        - d_in: 输入维度\n",
    "        - d_out: 输出维度\n",
    "        - context_length: 上下文长度（即注意力机制能“看到”的最大令牌数）\n",
    "        - dropout: Dropout率\n",
    "        - num_heads: 注意力头的数量\n",
    "        - qkv_bias: 是否为查询、键和值使用偏置（默认为False）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 检查输出维度是否能被头数整除\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # 将输出维度除以头数，得到每个头的维度\n",
    "\n",
    "        # 定义查询、键、值的线性变换\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # 定义输出的线性变换层，用于合并多个头的输出\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 注册一个buffer，用于存储因果掩码\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))  # 新增掩码，禁止未来的信息\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数，计算多头自注意力输出。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入张量，形状为 (batch_size, num_tokens, d_in)\n",
    "        \n",
    "        返回：\n",
    "        - context_vec: 多头自注意力的输出，形状为 (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape  # 获取输入张量的维度\n",
    "\n",
    "        # 计算键、查询和值的表示\n",
    "        keys = self.W_key(x)  # 键（K）\n",
    "        queries = self.W_query(x)  # 查询（Q）\n",
    "        values = self.W_value(x)  # 值（V）\n",
    "\n",
    "        # 将最后的维度按头数进行拆分：\n",
    "        # 将 (b, num_tokens, d_out) 转换为 (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 转置以适配矩阵相乘：\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 计算缩放点积注意力（self-attention）\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # 点积计算每个头的注意力分数\n",
    "\n",
    "        # 通过掩码将未来的信息遮掩（变成负无穷）\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # 将掩码转换为布尔类型\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # 使用掩码将未来的信息填充为负无穷\n",
    "\n",
    "        # 计算注意力权重并进行softmax归一化\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # 使用Dropout层\n",
    "\n",
    "        # 计算上下文向量（加权和）\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # 恢复维度 (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # 合并头部的输出，并进行线性变换\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)  # 合并头的输出\n",
    "        context_vec = self.out_proj(context_vec)  # 可选的投影层\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779fdd04-0152-4308-af08-840800a7f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = max_length\n",
    "d_in = output_dim\n",
    "d_out = d_in\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "batch = input_embeddings\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch03/03_understanding-buffers/understanding-buffers.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dlv8N4uWtXcN"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6BXGeEJ_s-8"
   },
   "source": [
    "# 理解PyTorch缓冲区作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQt9Ob1Y_8EH"
   },
   "source": [
    "本质上，PyTorch缓冲区是与PyTorch模块或模型相关联的张量属性，与参数类似.\n",
    "\n",
    "但不同于参数的是，缓冲区在训练过程中不会被更新。\n",
    "\n",
    "在处理GPU计算时，PyTorch缓冲区尤其重要，因为它们需要与模型的参数一起在设备间传输（如从CPU到GPU）。与参数不同，缓冲区不需要计算梯度，但仍需位于正确的设备上，以确保计算的准确性。\n",
    "\n",
    "在第三章中，我们通过`self.register_buffer`使用了PyTorch缓冲区，书中对此仅做了简要介绍。由于其概念和作用并不十分直观，本代码笔记本提供了更为详细的解释和实操示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAwGo_gYLY45"
   },
   "source": [
    "## 无缓存区"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qBQC9IPAJVZ"
   },
   "source": [
    "假设我们有以下代码，基于第三章的代码，并已修改以排除缓冲区。该代码实现了LLM中使用的因果自注意力机制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7wx-_rokAN04"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithoutBuffers(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNrK-wLaNSi7"
   },
   "source": [
    "我们可以按照如下形式初始化模型并在在测试样例上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1MZiIsPA0Py",
    "outputId": "ce1407c6-c082-4755-b8ad-d9adcc9f153a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_hqz6AgCCc1"
   },
   "source": [
    "到目前为止，一切都运行良好。\n",
    "\n",
    "然而，在训练LLM时，我们通常使用GPU来加速这一过程。因此，我们将把`CausalAttentionWithoutBuffers`模块转移到GPU设备上。\n",
    "\n",
    "这需要在配备GPU的环境中运行代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYwn44HWCPJS",
    "outputId": "d7236e0c-2a43-4770-ccc1-03c9d5d11421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine has GPU: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Machine has GPU:\", torch.cuda.is_available())\n",
    "\n",
    "batch = batch.to(\"cuda\")\n",
    "ca_without_buffer.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_lMki2_CoIR"
   },
   "source": [
    "再一次运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "KE9iLcjGC1V1",
    "outputId": "ab6921c7-d7dd-44ea-9b92-1911037e3dcc"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e0d2e6638f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca_without_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cf1dad0dd611>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         attn_scores.masked_fill_(\n\u001b[0m\u001b[1;32m     24\u001b[0m             self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n\u001b[1;32m     25\u001b[0m         attn_weights = torch.softmax(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7V26PLrC2gk"
   },
   "source": [
    "运行代码时出现了错误。发生了什么呢？\n",
    "看起来我们尝试在GPU上的张量和CPU上的张量之间进行矩阵乘法。\n",
    "但我们已经将这些模块移到了GPU上！\n",
    "\n",
    "让我们再检查一下某些张量的设备位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvYDPBRIDHfU",
    "outputId": "4b9703a8-7035-4a2d-8643-c64d37b7abd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.device: cuda:0\n",
      "mask.device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"W_query.device:\", ca_without_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d11nX-FFOJ3C",
    "outputId": "1e92b0e8-dbc6-41f9-e88f-5d06e0726050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ca_without_buffer.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojay-KY-DL5M"
   },
   "source": [
    "如我们所见，`mask`没有被移到GPU上。原因是它不像权重（例如`W_query.weight`）那样是PyTorch的参数。\n",
    "\n",
    "因此，我们需要通过`.to(\"cuda\")`手动将其移到GPU上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYirQ63zDYsW",
    "outputId": "304628ac-bc4c-49c2-a0e1-ecf9385ddcd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ca_without_buffer.mask = ca_without_buffer.mask.to(\"cuda\")\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OoTqzkpDfAm"
   },
   "source": [
    "再一次运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfF0yBZODdAZ",
    "outputId": "291cfb54-86e6-45f9-99d1-fa145319f379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUrVgWuuD7UE"
   },
   "source": [
    "这次，它成功了！\n",
    "\n",
    "然而，记得将每个张量手动移到GPU可能会很繁琐。正如我们将在下一节中看到的，使用`register_buffer`将`mask`注册为缓冲区会更为简便。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StS2wUrBLeuW"
   },
   "source": [
    "## 有了缓冲区的运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEqD2NFzPO6l"
   },
   "source": [
    "现在，让我们修改因果注意力类，将因果`mask`注册为缓冲区："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ndsYj3Zf6N8U"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithBuffer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Old:\n",
    "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "        # New:\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AL1X6y3Eb7S"
   },
   "source": [
    "十分方便的是，如果我们将模块移到GPU，`mask`也会自动被放置到GPU上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_VCxEa76j00",
    "outputId": "4d1af501-5a9e-46aa-b1ac-63bf0c68e02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.device: cuda:0\n",
      "mask.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
    "ca_with_buffer.to(\"cuda\")\n",
    "\n",
    "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_with_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBWvKlMe7bbB",
    "outputId": "e43bf8ab-3fb9-417e-d087-560858332d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4772, 0.1063],\n",
      "         [0.5891, 0.3257],\n",
      "         [0.6202, 0.3860],\n",
      "         [0.5478, 0.3589],\n",
      "         [0.5321, 0.3428],\n",
      "         [0.5077, 0.3493]],\n",
      "\n",
      "        [[0.4772, 0.1063],\n",
      "         [0.5891, 0.3257],\n",
      "         [0.6202, 0.3860],\n",
      "         [0.5478, 0.3589],\n",
      "         [0.5321, 0.3428],\n",
      "         [0.5077, 0.3493]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_with_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvOTh4NNPjef"
   },
   "source": [
    "As we can see above, registering a tensor as a buffer can make our lives a lot easier: We don't have to remember to move tensors to a target device like a GPU manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-5YYKmJte3h"
   },
   "source": [
    "## 缓冲区与`state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIHHawPbtjfp"
   },
   "source": [
    "- PyTorch缓冲区相较于普通张量的另一个优点是，它们会被包含在模型的`state_dict`中。\n",
    "- 例如，考虑没有缓冲区的因果注意力对象的`state_dict`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c217juzqtxsS",
    "outputId": "dbae3c3d-f4f8-4c70-a64f-90906561d8d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W_query.weight',\n",
       "              tensor([[-0.2354,  0.0191, -0.2867],\n",
       "                      [ 0.2177, -0.4919,  0.4232]], device='cuda:0')),\n",
       "             ('W_key.weight',\n",
       "              tensor([[-0.4196, -0.4590, -0.3648],\n",
       "                      [ 0.2615, -0.2133,  0.2161]], device='cuda:0')),\n",
       "             ('W_value.weight',\n",
       "              tensor([[-0.4900, -0.3503, -0.2120],\n",
       "                      [-0.1135, -0.4404,  0.3780]], device='cuda:0'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_without_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdmZuPaqt6aO"
   },
   "source": [
    "- 上面的`state_dict`中没有包含`mask`。\n",
    "- 然而，由于将其注册为缓冲区，下面的`state_dict`中包含了`mask`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGIGQAwPt1Pl",
    "outputId": "00f9bc44-63f9-4ebc-87ea-d4b8cafd81c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mask',\n",
       "              tensor([[0., 1., 1., 1., 1., 1.],\n",
       "                      [0., 0., 1., 1., 1., 1.],\n",
       "                      [0., 0., 0., 1., 1., 1.],\n",
       "                      [0., 0., 0., 0., 1., 1.],\n",
       "                      [0., 0., 0., 0., 0., 1.],\n",
       "                      [0., 0., 0., 0., 0., 0.]], device='cuda:0')),\n",
       "             ('W_query.weight',\n",
       "              tensor([[-0.1362,  0.1853,  0.4083],\n",
       "                      [ 0.1076,  0.1579,  0.5573]], device='cuda:0')),\n",
       "             ('W_key.weight',\n",
       "              tensor([[-0.2604,  0.1829, -0.2569],\n",
       "                      [ 0.4126,  0.4611, -0.5323]], device='cuda:0')),\n",
       "             ('W_value.weight',\n",
       "              tensor([[ 0.4929,  0.2757,  0.2516],\n",
       "                      [ 0.2377,  0.4800, -0.0762]], device='cuda:0'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_with_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACC-a1Hnt4Zv"
   },
   "source": [
    "- `state_dict`在保存和加载训练好的PyTorch模型时非常有用，例如。\n",
    "- 在这个特定的情况下，保存和加载`mask`可能并不是特别有用，因为它在训练过程中保持不变；因此，出于演示目的，我们假设它被修改了，将所有的`1`改为`2`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLm1Sw0cuhvy",
    "outputId": "4b2cc70f-1709-44e4-aa17-4e01353b86f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_with_buffer.mask[ca_with_buffer.mask == 1.] = 2.\n",
    "ca_with_buffer.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIkGgGqqvp4S"
   },
   "source": [
    "- 然后，如果我们保存并加载模型，可以看到`mask`已经恢复为修改后的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8g0QHUhuVBw",
    "outputId": "cc7ee348-7f94-4117-e5cc-e0e01a94e906"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(ca_with_buffer.state_dict(), \"model.pth\")\n",
    "\n",
    "new_ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
    "new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "new_ca_with_buffer.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pPaJk7bvBD7"
   },
   "source": [
    "- 如果我们不使用缓冲区，情况就不一样了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D03w8vDyvBRS",
    "outputId": "28071601-120c-42da-b327-bb293793839f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_without_buffer.mask[ca_without_buffer.mask == 1.] = 2.\n",
    "\n",
    "torch.save(ca_without_buffer.state_dict(), \"model.pth\")\n",
    "\n",
    "new_ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
    "new_ca_without_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "new_ca_without_buffer.mask"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="ch05/01_main-chapter-code/ch05.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# 第五章: 在无标签数据集上预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "#同样导入库并检查版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- 在本章中，我们将实现循环训练和基本模型评价的代码，用于预训练大语言模型。\n",
    "- 在本章的最后，我们还将从 OpenAI 加载公开可用的预训练权重到我们的模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"../image/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214765-7a73-42d5-95e9-302154b29db9",
   "metadata": {},
   "source": [
    "- 本章节涉及的主题如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"../image/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "## 5.1 评估文本生成大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- 本节开始时，我们简要回顾了如何使用上一章的代码初始化 GPT 模型。\n",
    "- 然后，我们讨论了大语言模型的基本评估指标。\n",
    "- 最后，在本节中，我们将这些评估指标应用于训练和验证数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "### 5.1.1 用GPT来生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- 我们首先与前几章一样初始化GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "#导入模型, 设定一系列参数, 设定随机种子确保可复现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- 我们在上述代码中使用了 0.1 的 dropout 率，但如今训练大语言模型时通常不使用 dropout。\n",
    "- 现代的大语言模型不在 `nn.Linear` 层的查询、键和值矩阵中使用偏置向量（与早期的 GPT 模型不同），而是通过设置 `\"qkv_bias\": False` 实现。\n",
    "- 我们将上下文长度（`context_length`）减少到仅 256 个 token，以减少训练模型时的计算资源需求，而原始的 1.24 亿参数的 GPT-2 模型使用了 1024 个token。\n",
    "  - 这是为了让更多读者可以在他们的笔记本电脑上运行并跟随代码示例。\n",
    "  - 然而，您可以自由将 `context_length` 增加到 1024 个 token（这不需要更改任何代码）。\n",
    "  - 我们稍后也将从预训练权重中加载一个具有 1024 `context_length` 的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- 接下来，我们使用上一章中的 `generate_text_simple` 函数生成文本。\n",
    "- 此外，我们定义了两个便利函数，`text_to_token_ids` 和 `token_ids_to_text`，用于在 token ID 和文本表示之间进行转换，这两个函数将在本章中多次使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"../image/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "#给输入的字符进行编码并实现一个Batch维度的向量,符合模型的输入形式\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "#反向编码,去掉移除张量中的批次维度, 变成普通的链表\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "#举个例子\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    #初始上下文的Token ID张量，是上一步 text_to_token_ids 的输出\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "#输出最长单词度为10的句子\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- 如上所示，由于模型尚未训练，它生成的文本并不理想。\n",
    "- 我们如何衡量或设定“好文本”的标准，并将其转化为数值，以便在训练过程中进行跟踪？\n",
    "- 下一小节介绍了计算生成输出的损失指标的度量标准，我们可以用它来衡量训练进度。\n",
    "- 后续关于微调大语言模型的章节还将介绍其他衡量模型质量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
   },
   "source": [
    "### 5.1.2 计算文本生成的损失：交叉熵(cross- entropy)和困惑度(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- 假设我们有一个 `inputs` 张量，其中包含 2 个训练示例（行）的 token ID。\n",
    "- 与 `inputs` 对应，`targets` 包含我们希望模型生成的目标 token ID。\n",
    "- 请注意，`targets` 是将 `inputs` 向右移动 1 个位置后的结果，正如我们在第二章实现数据加载器时所解释的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "#用向量的形式展现输入的文本\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "#用向量的形式展现要输出的东西"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- 将 `inputs` 输入给模型后，我们将得到 2 个输入示例的 logits 向量，每个输入示例包含 3 个token。\n",
    "- 每个 token 都是一个 50,257 维的向量，对应于词汇表的大小。\n",
    "- 通过应用 softmax 函数，我们可以将 logits 张量转换为一个相同维度的张量，其中包含概率得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "#不用梯度计算的计算inputes并储存\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "#用soft Max整理logits\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- 下图展示了我们如何将概率得分转换为文本，示例使用了一个非常小的词汇表，这一内容已在上一章的结尾讨论过。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"../image/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- 如上一章所讨论的，我们可以应用 `argmax` 函数将概率得分转换为预测的 token ID。\n",
    "- 上面的 softmax 函数为每个 token 生成了一个 50,257 维的向量；`argmax` 函数返回该向量中概率得分最高的位置，即给定 token 的预测 token ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- 由于我们有 2 个输入批次，每个批次包含 3 个 token，因此我们得到 2 行 3 列的预测 token ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "#相当于用贪心算法给出最有可能的答案\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- 如果解码这些 token，我们会发现它们与希望模型预测的 token (目标 token)有很大不同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "#给出答案\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "#给出事实上的结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- 这是因为模型还没有经过训练。\n",
    "- 要训练模型，我们需要知道它与正确预测（目标）之间的差距有多大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"../image/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- 对应于目标索引的 token 概率如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- 我们希望最大化这些值，使它们的概率接近 1。\n",
    "- 在数学优化中，最大化概率得分的对数比直接最大化概率得分更为简单；虽然这超出了本书的讨论范围，但我录制了一节更详细的讲座，您可以在这里查看：[L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "#用对数输出他最大的可能数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- 接下来，我们计算平均对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "#对数概率平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- 目标是通过优化模型权重，使得平均对数概率尽可能大。\n",
    "- 由于对数的性质，最大的可能值是 0，而我们当前距离 0 还有很大差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- 在深度学习中，通常的做法是最小化 \"负\" 的平均对数概率值，而不是最大化平均对数概率值；在我们的例子中，深度学习中我们会最小化 10.7722 使其接近 0，而不是最大化 -10.7722 使其接近 0。\n",
    "- 值 -10.7722 的负数，即 10.7722，在深度学习中也被称为交叉熵损失（cross-entropy loss）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "#最大化对数等价为最小化负对数\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch 中的`cross_entropy` 已经能实现这些功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"../image/cross-entropy.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- 在使用`cross_entropy` 之前, 我们可以看一下loggias跟target是怎样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- 有了PyTorch 中的 `cross_entropy` 函数，我们希望通过在批次维度上合并这些张量来将其展平："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "#将张量 logits 的 第0维和第1维合并为一个维度，展平成一个二维张量\n",
    "targets_flat = targets.flatten()\n",
    "#将张量 targets 展平为一维张量\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- 请注意，目标是 token ID，这些 ID 也代表我们希望最大化的 logits 张量中的索引位置。\n",
    "- PyTorch 中的 `cross_entropy` 函数会自动处理对这些token索引的 softmax 和对数概率计算，确保它们被最大化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "#封装函数出马,一个代替好几行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- 与交叉熵损失相关的一个概念是大语言模型的困惑度 (perplexity)。\n",
    "- 困惑度就是交叉熵损失的指数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "#指数化loss作为P值\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- 困惑度通常被认为更易解释，因为它可以理解为模型在每一步对词汇表大小的不确定性（在上面的例子中，这相当于 48,725 个单词或 token）。\n",
    "- 换句话说，困惑度提供了一个衡量模型预测的概率分布与数据集中单词实际分布匹配程度的指标。\n",
    "- 类似于损失值，较低的困惑度表示模型预测与实际分布的差距较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "### 5.1.3 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- 我们使用一个相对较小的数据集来训练大语言模型（训练内容只有一个短篇故事）。\n",
    "- 选择小故事的原因包括：\n",
    "  - 无独立显卡的电脑也可以快速完成\n",
    "  - 训练时间较短（以分钟计算，而非数周）\n",
    "  - 我们使用了无版权文本，可以将其包含在这个 GitHub 仓库中，而不会违反任何使用权或显著增加仓库大小。\n",
    "\n",
    "- 例如，Llama 2 7B 模型在 2 万亿 token 上训练时需要 184,320 个 GPU 小时（使用 A100 GPU）。\n",
    "  - 截至本文撰写时，AWS 上 8xA100 云服务器的每小时成本约为 30 美元。\n",
    "  - 因此，通过简单计算，训练这个 LLM 的成本为 184,320 / 8 * 30 美元 = 690,000 美元。\n",
    "\n",
    "- 下面，我们使用了第二章中使用的同一数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "#引入数据集\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "#一系列经典的读取数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- 通过前100个词与后100个词快速检查文本是否加载正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "#统计一下文本的长度,编码文本内容并输出文本个数\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- 为了教学,我们选取了这个短文本作为样例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- 接下来，我们将数据集划分为训练集和验证集，并使用第二章中的data loader为大语言模型（LLM）训练准备数据。\n",
    "- 出于可视化的目的，下图假设 `max_length=6`，但对于训练加载器，我们将 `max_length` 设置为 LLM 支持的上下文长度。\n",
    "- 为了简化，下图仅展示了输入token：\n",
    "    - 由于我们训练 LLM 预测文本中的下一个单词，因此目标 token 与输入 token 相同，只是目标 token 向右移动了一个位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"../image/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "#从一个库导入之前的文章\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "#这边可以手动定义训练集跟测试剂的比例\n",
    "\n",
    "torch.manual_seed(123)\n",
    "#依旧保持可复现\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "#初始化输入训练模型,给出批处理的大小、给出最大文本容量防止溢出\n",
    "#给出不畅,丢弃最后一批不足的文本,打开随机防止拟合过度\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "#验证数据集仅仅修改了是否丢弃跟随抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "# 神圣性,看一下一批次够了没\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- 小的批处理数据集很适合用来小试牛刀\n",
    "- 例如, Llama 27B就是用每次1024的批处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- 另一种确认数据正常导入的方法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- 还有一个方法来确认是否导入成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch. numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "#每次加一下训练数据集所有元素的种类\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)\n",
    "\n",
    "# 在 PyTorch 中，调用 .numel() 方法会返回张量中所有元素的总数，无论张量的形状或维度如何"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- 我们用了预分装函数来计算交叉熵\n",
    "- 我们还调用另一个辅助函数，用于计算数据加载器中由用户指定的批次数Loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    #呼唤GPU\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    #用交叉熵函数对于logits进行计算并且拉伸到二维长度\n",
    "    return loss\n",
    "#一个计算批损失的函数\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果指定的批次数超过数据加载器中的总批次数，则将批次数减少到与数据加载器的总批次数匹配。\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        #减少需要处理的数量,同时也是防止溢出\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "        #一点点加上去的损失\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- 如果你的电脑有支持 CUDA 的 GPU，大预言模型将无需更改代码即可在 GPU 上进行训练。\n",
    "- 通过 `device` 设置，我们确保数据加载到与大语言模型相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 如果支持，则调用 GPU\n",
    "\n",
    "# 注意：\n",
    "# 如果取消注释以下代码块，代码可以在 Apple Silicon 芯片上运行（如果适用），\n",
    "# 在 M3 MacBook Air 上测量速度大约是 Apple CPU 的两倍。\n",
    "# 然而，计算得到的损失值可能会略有不同。\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)  # 对于 nn.Module 类，不需要赋值 model = model.to(device)\n",
    "\n",
    "torch.manual_seed(123)  # 固定随机种子，保证数据加载器打乱数据的结果可复现\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度跟踪以提高效率，因为此时尚未开始训练\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "# 推理阶段不计算梯度\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"../image/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "## 5.2 训练大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- 在本节中，我们最终实现了用于训练大语言模型（LLM）的代码。\n",
    "- 我们想要于一个简单的训练函数（如果您对通过更高级的技术增强此训练函数感兴趣，例如学习率预热(rate warmup)、余弦退火(cosine annealing)和梯度裁剪(gradient clipping)，请参阅[附录D](../../appendix-D/01_main-chapter-code)）。\n",
    "\n",
    "<img src=\"../image/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    #初始化训练模型而且给了空的队列\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):#训练次数\n",
    "        model.train()  # Set model to training mode\n",
    "        #转移到训练模块\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            #从loader里面调出输入跟目标\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            #清空所有函数的梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            #计算损失函数\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            #反向传播优化\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            #更新权重\n",
    "            tokens_seen += input_batch.numel()\n",
    "            #加一下一共有多少\n",
    "            global_step += 1\n",
    "            #看一下一共训练了多少步\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                #按照一定的步数进行记录\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                #计算损失函数\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                #加到list中\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    #评价模块\n",
    "    model.eval()\n",
    "    #检验模式\n",
    "    with torch.no_grad():\n",
    "        #我认为的双保险,防止梯度更新\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    #\t在评估结束后切换回训练模式，确保模型能继续用于训练。\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252ffa4-8162-466e-b347-27cb89b9a5ee",
   "metadata": {},
   "source": [
    "(GPT的解释)\n",
    "- global_step 是训练循环中的重要计数器，主要用于控制学习率调度、记录日志、保存模型检查点和控制终止条件等任务。\n",
    "- 在分批训练中，它提供了一个统一的参考点，有助于管理复杂的训练流程。\n",
    "- epoch 是按完整数据集的迭代单位，而 global_step 是按 batch 单位，粒度更细，用于管理更精确的任务。例如：\n",
    "    1.\t动态学习率调整,某些学习率调度器需要以 batch 为单位进行调整，而不是每个 epoch。例如，WarmUp 会在固定的前 N 步逐渐升高学习率。\n",
    "\t2.\t频繁日志记录,记录训练日志时，通常是每隔 log_interval 步输出一次，而不是每个 epoch。\n",
    "\t3.\t检查点保存,保存模型状态通常是按步数完成，尤其是当训练需要中断和恢复时，global_step 是更精确的 token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- 我们用上述的定义训练一下这个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "#下面可以看一下计算了多久\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "#经典操作\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "#用Adam进行优化,其中学习rate为0.004,动量衰减是0.1\n",
    "num_epochs = 10\n",
    "#10论学习\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "#记录了开始文本、检验的频率\n",
    "# 注意：\n",
    "# 如果需要显示执行时间，请取消注释以下代码\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"训练完成耗时 {execution_time_minutes:.2f} 分钟。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "#一个经典的plot画图函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- 从以上结果可以看出，模型在开始阶段生成的是难以理解的字符串，但是在后期能够生成基本符合语法的句子。\n",
    "- 从训练集和验证集的损失值可以看出，模型开始出现过拟合现象。\n",
    "- 如果检查后期它生成的某些段落，会发现它们与训练集中的内容完全相同(模型只是简单地记住了训练数据,背住答案罢了)。\n",
    "- 之后的部分，我们将讨论一些解码策略，这些策略可以一定程度缓解这种“背答案”的问题。\n",
    "- 请注意，这里的过拟合是由于训练集非常非常小，并且我们对其进行了多次迭代。\n",
    "  - 本次 LLM 训练主要用于教学目的；我们的目标是观察模型是否能够学会生成连贯的文本。\n",
    "  - 为了避免花费数周或数月时间在大量昂贵硬件上训练模型，我们将在后续加载预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"../image/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**如果您对通过更高深的技术增强此训练函数感兴趣，例如学习率预热、余弦退火和梯度裁剪，请参阅[附录D](../../appendix-D/01_main-chapter-code)。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**更大的数据集跟更深度的训练,可以在以下找到链接 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 控制随机性的解码策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- 对于像我们训练的这种规模相对较小的GPT模型（LLM），推理阶段的计算成本较低。因此，如果在训练时使用了GPU，推理阶段则无需依赖GPU资源。\n",
    "- 我们可以利用第5章介绍的`generate_text_simple`函数（该函数已在简单训练函数中被调用）来逐步生成新文本，每次生成一个单词（或 token）。\n",
    "- 正如5.1.2节所提到的，下一个生成的 token 是从词汇表中选取概率得分最高的 token 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "#经典的载入\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- 即使我们多次调用 `generate_text_simple` 函数，大语言模型（LLM）生成的输出也始终是确定性的，即每次结果相同。\n",
    "- 为了增强生成文本的灵活性，我们引入了两种解码策略来改进 `generate_text_simple`：**温度缩放** 和 **top-k 采样**。\n",
    "- 这些方法能够调节模型生成文本的随机性和多样性，从而满足不同的应用需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 温度缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- 在之前的实现中，我们始终使用 `torch.argmax` 来选择概率最高的 token 作为下一个生成的 token。\n",
    "- 为了增加生成文本的多样性，我们可以改用 `torch.multinomial(probs, num_samples=1)`，从概率分布中随机采样下一个 token。\n",
    "- 在这种方法中，每个索引被选中的概率与其在输入张量中对应的概率值成正比，从而实现基于概率的随机采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- 以下是对生成下一个 token 过程的简单回顾，假设我们使用一个非常小的词汇表来说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "#插入\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "#softmax归一化\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "#选个可能性最大\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- 我们不再依赖 `torch.argmax` 来选择最可能的 token ，而是通过 `torch.multinomial(probas, num_samples=1)` 从 softmax 分布中采样来确定下一个 token 。\n",
    "- 为了直观地理解这一过程，我们可以使用原始的 softmax 概率对下一个 token 进行 1,000 次采样，并观察结果分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    #从概率分布 probas 中按照权重进行一次采样,并生成索引\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    #然后变成单词\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "#统计采样过程中每个词的出现频率\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- 我们可以通过一种称为**温度缩放**的技术来调节概率分布和 token 选择的过程。\n",
    "- 温度缩放的核心操作是将 logits 除以一个大于 0 的数值（即温度值），然后再应用 softmax 函数。\n",
    "- 当温度值大于 1 时，softmax 输出的概率分布会更加均匀，从而增加生成文本的多样性。\n",
    "- 当温度值小于 1 时，softmax 输出的概率分布会更加集中（更陡峭或更尖锐），从而倾向于选择概率更高的 token，减少随机性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e24fe1-3c4e-4ebe-877e-d5d8b703d148",
   "metadata": {},
   "source": [
    "模型的预测概率往往过于自信或低估某些类别的概率，尤其在分类任务中。\n",
    "温度缩放通过引入一个参数  T > 0  来重新调整 logits，改善预测概率的校准性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "#温度校正\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "#初始校正系数\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()\n",
    "#一套经典的画图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- 从结果中可以看出，当温度设置为 0.1 时，概率分布变得更加陡峭，接近于 `torch.argmax` 的行为，因此最可能的 token 几乎总是被选中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- 当温度设置为 5 时，概率分布变得更加均匀，从而增加了生成文本的多样性和随机性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- 假设大语言模型（LLM）的输入是“every effort moves you”，上述方法有时可能会生成无意义的文本，例如“every effort moves you pizza”，其出现的概率为 3.2%（即在 1000 次采样中出现了 32 次）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k 取样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- 为了在使用更高温度增加输出多样性的同时减少生成无意义句子的概率，我们可以将采样限制在前 k 个最可能的 token 中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"../image/topk.webp\" width=500px>\n",
    "\n",
    "- （请注意，此图中的数值已截取到小数点后两位，以减少视觉干扰。Softmax 行中的值总和应为 1.0。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- 我们可以按照下述建议补充代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f110a-8aa7-4c84-8d71-5cfd28468b48",
   "metadata": {},
   "source": [
    "-\t控制输出质量： 减少低概率、无意义的词被选中的机会。\n",
    "-\t保持多样性： 允许模型在概率较高的几个候选词中随机选择，而不是总是选择最高概率的词（这会导致输出缺乏变化）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "#topK采样\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "#不是前K遮蔽掉\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6fa49-6e99-459d-a517-d7d0f51c4f00",
   "metadata": {},
   "source": [
    "> NOTE:  \n",
    ">\n",
    ">  一种稍微更高效的实现方式可以通过以下代码实现：\n",
    ">\n",
    "> ```python\n",
    "> new_logits = torch.full_like( # create tensor containing -inf values\n",
    ">    next_token_logits, -torch.inf\n",
    ">)   \n",
    "> new_logits[top_pos] = next_token_logits[top_pos] # copy top k values into the -inf tensor\n",
    "> ```\n",
    ">\n",
    "> For more details, see https://github.com/rasbt/LLMs-from-scratch/discussions/326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 优化文本更新功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- 在前两小节中，我们介绍了**温度采样**和**top-k 采样**的概念。\n",
    "- 现在，我们将结合这两种方法，对之前用于生成大语言模型（LLM）文本的 `generate_simple` 函数进行改进，创建一个新的 `generate` 函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9c09a-41fb-465e-9f60-eb4fd2a7230d",
   "metadata": {},
   "source": [
    "    - (译者):用自己的话总结下\n",
    "    - 温度校正是更加平滑,防止数据差之毫厘以谬以千里 \n",
    "    - topK是防止臭鱼烂虾进入筛选范围提高质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "#生成模块\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        #计算预测值,但是切最后一个\n",
    "        # New: Filter logits with top_k sampling\n",
    "        #top K采样\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "            \n",
    "        # New: Apply temperature scaling\n",
    "        #温度校正\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "            #从概率分布中采样下一个 token \n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "            #如果未启用采样，选择概率最高的 token 作为下一个 token \n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "#经典的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 在Pytorch中加载并保留权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- 大模型的训练是很贵的, 所以导入已训练好的参数是很有必要的\n",
    "<img src=\"../image/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- 在Pytorch中我们所推荐的保存方式是所谓的 `state_dict` ,这玩意通过调用 `torch.save` 的子模块 `.state_dict()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "#训练完的数据保存一下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- 之后我们可以对新的 `GPTModel` 导入已经训练好的参数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- 自适应的Adam跟AdamW相较于SGD更好!\n",
    "- 但是这些算法需要另外的参数, 所以保存训练好的参数就更有必要了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n",
    "#全家整整齐齐地保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "#保存检查点\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "#调整到训练模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 从OpenAI导入超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- 在之前的实验中，我们仅使用了一本非常短的小故事书来训练一个小型 GPT-2 模型，这主要是为了教学目的。\n",
    "- 对此感兴趣的读者可以在[../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)中找到基于完整古登堡计划图书语料库的更长时间预训练记录。\n",
    "- 幸运的是，我们无需花费数万到数十万美元在大型预训练语料库上预训练模型，而是可以直接加载 OpenAI 提供的预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "- 有关从Hugging Face中加载权重的另一种方法请参阅 [../02_alternative_weight_loading](../02_alternative_weight_loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- 首先，我们需要一些基础代码来从 OpenAI 下载文件并将权重加载到 Python 中。\n",
    "- 由于 OpenAI 使用了 [TensorFlow](https://www.tensorflow.org/)，我们需要安装并使用 TensorFlow 来加载权重；同时，[tqdm](https://github.com/tqdm/tqdm) 是一个用于显示进度条的库。\n",
    "- 取消注释并运行下一个代码单元以安装所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))\n",
    "#tensorflow他到底还是来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2\n",
    "#召唤神仙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "- 通过如下代码下载124M的模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- 此外，`model_size` 参数还支持 \"355M\"、\"774M\" 和 \"1558M\" 等选项。\n",
    "- 下图总结了这些不同规模模型之间的主要差异："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"../image/gpt-sizes.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- 在上述操作中，我们已经成功将 124M 的 GPT-2 模型权重加载到 Python 中，但仍需将这些权重传输到我们的 `GPTModel` 实例中。\n",
    "- 首先，我们需要初始化一个新的 `GPTModel` 实例。\n",
    "- 需要注意的是，原始的 GPT 模型在多头注意力模块中为查询、键和值矩阵的线性层初始化了带有偏置向量的权重，这种做法既不必要也不推荐。然而，为了正确加载权重，我们在实现中必须将 `qkv_bias` 参数设置为 `True`。\n",
    "- 此外，我们使用了原始 GPT-2 模型所支持的 `1024`  token 的上下文长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "#把每个大小的模型都与先载入并确定好\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- 接下来的任务是将 OpenAI 的权重分配到我们 `GPTModel` 实例中对应的权重张量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    #位置权重\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    #单词全中\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        #三个参数的输入\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "#主要目的是将预训练的模型参数加载到一个gpt中\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- 如果模型正确加载了,我们可以用先前的`generate` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
   "metadata": {},
   "source": [
    "- 我们可以确认模型权重已正确加载，因为模型能够生成连贯的文本；如果我们在加载过程中出现任何错误，模型将无法实现这一点。\n",
    "\n",
    "- 如果您想了解另一种从 Hugging Face Hub 加载权重的方法，请参阅 [../02_alternative_weight_loading](../02_alternative_weight_loading)。\n",
    "\n",
    "- 如果您对 GPT 架构与 Llama 架构（Meta AI 开发的一种流行大语言模型）之间的比较感兴趣，请查看附加内容：[../07_gpt_to_llama](../07_gpt_to_llama)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
   "metadata": {},
   "source": [
    "## 总结与收获"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
   "metadata": {},
   "source": [
    "- 请参考 [./gpt_train.py](./gpt_train.py) 脚本，这是一个独立的训练脚本。\n",
    "- [./gpt_generate.py](./gpt_generate.py) 脚本会加载 OpenAI 提供的预训练权重，并根据提示生成文本。\n",
    "- 您可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习的解答。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# 第五章 课后练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37aa4692-2357-4d88-b072-6d2d988d7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# 练习 5.1：基于温度缩放(Temperature Scaling)的 Softmax 得分与采样概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    "- 我们可以使用本节中定义的 `print_sampled_tokens` 函数来打印“pizza”这个词被采样的次数。\n",
    "- 我们从第 5.3.1 节中定义的代码开始。\n",
    "\n",
    "- 当温度为 0 或 0.1 时，它的采样次数为 0x，而当温度被调高到 5 时，它的采样次数为 32x。估计的概率是 32/1000 * 100% = 3.2%。\n",
    "\n",
    "- 实际的概率是 4.3%，并包含在重新缩放的 softmax 概率张量（`scaled_probas[2][6]`）中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba59c2-a8a3-4af3-add4-70230795225e",
   "metadata": {},
   "source": [
    "- 以下是一个完整的示例，使用了第 5 章中的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42dda298-3014-4c36-8d63-97c210bcf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0f9f3-4132-42c7-8324-252fd8f59145",
   "metadata": {},
   "source": [
    "- 现在，我们可以遍历 `scaled_probas` 并打印每种情况下的采样频率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5605236-e300-4844-aea7-509d868efbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf88c97-19c4-462c-924a-411c8c765d2c",
   "metadata": {},
   "source": [
    "- 请注意，采样提供了对单词“pizza”进行采样时的实际概率的近似值。\n",
    "- 例如，如果它被采样了 32/1000 次，估计的概率是 3.2%。\n",
    "- 要获得实际的概率，我们可以通过访问 `scaled_probas` 中对应的条目来直接查看概率。\n",
    "\n",
    "- 由于“pizza”是词汇表中的第 7 个条目，在温度为 5 时，我们可以按如下方式获取它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4163c0-22ad-4f5b-8e20-b7420e9dbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcb438-5f18-4332-9627-66009f30a1a4",
   "metadata": {},
   "source": [
    "如果温度设置为 5，\"pizza\" 这个词被采样的概率为 4.3%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
   "metadata": {},
   "source": [
    "# 练习 5.2：不同温度系数和 top-k 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10263328",
   "metadata": {},
   "source": [
    "- 译者的理解:\n",
    "  - 温度”（temperature）通常控制采样的多样性\n",
    "  - “top-k”则是指从概率最高的前 k 个词中采样,保证确定性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
   "metadata": {},
   "source": [
    "- 温度和 top-k 参数设置需要根据具体的 LLM 调整（这是一种试错过程，直到生成理想的输出）。\n",
    "- 然而，理想的结果也是应用特定的：\n",
    "  - 较低的 top-k 和温度会导致较少随机的输出，这在创建教育内容、技术写作、问答、数据分析、代码生成等任务中是更为理想的。\n",
    "  - 较高的 top-k 和温度会导致更具多样性和随机性的输出，这在头脑风暴、创意写作等任务中更为理想。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
   "metadata": {},
   "source": [
    "# 练习 5.3：解码函数中的确定性行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
   "metadata": {},
   "source": [
    "有多种方式可以强制 `generate` 函数实现确定性行为：\n",
    "\n",
    "1. 将 `top_k=None`，并且不应用温度缩放；\n",
    "2. 设置 `top_k=1`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c5dc8-8dd7-4a0a-90bd-519b72f528c7",
   "metadata": {},
   "source": [
    "- 以下是一个完整的示例，使用了第 5 章中的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61a4034-797a-4635-bf42-ddfff1b07125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # 词汇表大小\n",
    "    \"context_length\": 256,       # 缩短的上下文长度（原始值：1024）\n",
    "    \"emb_dim\": 768,       # 嵌入维度\n",
    "    \"n_heads\": 12,        # 注意力头数\n",
    "    \"n_layers\": 12,       # 层数\n",
    "    \"drop_rate\": 0.1,     # 丢弃率\n",
    "    \"qkv_bias\": False     # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee95a272-b852-43b4-9827-ea7e1dbd5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_chapters import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb22d06-393a-42d3-ab64-66646d33b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# 使用torch.argmax的确定性函数\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"生成的文本:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b1f11-37a5-477d-9c2d-170a6865e669",
   "metadata": {},
   "source": [
    "- 重新执行前一个代码单元将生成完全相同的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75469f24-47cc-458d-a200-fe64c648131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# 确定性行为：没有top_k，没有温度缩放\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"生成的文本:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
   "metadata": {},
   "source": [
    "# 练习 5.4: 后续预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
   "metadata": {},
   "source": [
    "- 如果我们仍然在第5章首次训练模型的Python会话中，要继续预训练一个轮次，我们只需要加载在主章节中保存的模型和优化器，并再次调用`train_model_simple`函数\n",
    "\n",
    "-   在新的代码环境中确保结果可复现需要多几个步骤\n",
    "-   首先，我们加载分词器、模型和优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94eae6ba-d9fd-417a-8e31-fc39e9299870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度（原始值：1024）\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # 丢弃率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fce4a-9ab2-4d97-a95c-fef02c32b4f3",
   "metadata": {},
   "source": [
    "- 接下来初始化载入器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a78470-0652-4abd-875a-664e23c07c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# 训练/验证集比例\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76598ef8-165c-4bcc-af5e-b6fe72398365",
   "metadata": {},
   "source": [
    " 最后，我们使用`train_model_simple`函数来训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab4693dc-1359-47a7-8110-1e90f514a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.271, Val loss 6.545\n",
      "Ep 1 (Step 000005): Train loss 0.244, Val loss 6.614\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "from gpt_train import train_model_simple\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
   "metadata": {},
   "source": [
    "# 练习 5.5: 预训练模型的训练和验证集Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
   "metadata": {},
   "source": [
    "我们可以使用以下代码计算GPT模型的训练集和验证集损失：\n",
    "\n",
    "```python\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "```\n",
    "\n",
    "- 124M模型的Loss如下\n",
    "\n",
    "```\n",
    "Training loss: 3.754748503367106\n",
    "Validation loss: 3.559617757797241\n",
    "```\n",
    "- 主要观察结果:训练集和验证集的表现差不多\n",
    "- 这可能有多种解释：\n",
    "\n",
    "1. The Verdict并不是OpenAI在训练GPT-2时使用的预训练数据集的一部分。因此，模型并没有显式地对训练集发生过拟合，并且在The Verdict的训练集和验证集部分表现相似。（在深度学习中，验证集的损失略低于训练集的损失，这并不常见。由于数据集相对较小，这种现象可能是由随机噪声引起的。在实际应用中，如果没有发生过拟合，训练集和验证集的表现通常是大致相同的）。\n",
    "\n",
    "2. The Verdict是GPT-2训练数据集的一部分。在这种情况下，我们无法判断模型是否在训练数据上发生了过拟合，因为验证集也会用于训练。为了评估过拟合的程度，我们需要一个在OpenAI完成训练GPT-2后生成的新数据集，以确保该数据集不可能是预训练数据的一部分。\n",
    "主要观察结果是训练集和验证集的表现差不多\n",
    "这可能有多种解释：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb4316-a57c-437f-9a01-fe99b1678524",
   "metadata": {},
   "source": [
    "下面的代码是一个可复现的独立示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d162d6-bbb9-4d6d-82ee-1c410694f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度（原始值：1024）\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # 丢弃率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8373461-7dad-47da-a489-3e23f0799b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd44873-d6c2-4471-a20f-f639b09fdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在字典中定义模型配置以简化表示\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# 复制基础配置并根据特定模型设置进行更新\n",
    "model_name = \"gpt2-small (124M)\"  # 示例模型名称\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d562e4-33f6-4611-9b75-6ad1cb441d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46eda9ea-ccb0-46ee-931b-3c07502b2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# 比例\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3574a2-687d-47a2-a2f6-457fe9d595f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7547486888037787\n",
      "Validation loss: 3.5596182346343994\n"
     ]
    }
   ],
   "source": [
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123) # 确保可复现\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96485d6b-bf1f-4bc0-a53f-73b08d85726e",
   "metadata": {},
   "source": [
    "我们也可以对最大的GPT-2模型执行相同操作，但不要忘记更新上下文长度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a79a4b6-fe8f-40c2-a018-e731dcf391b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 43.5kiB/s]\n",
      "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 2.75MiB/s]\n",
      "hparams.json: 100%|█████████████████████████| 91.0/91.0 [00:00<00:00, 60.2kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|█████| 6.23G/6.23G [06:02<00:00, 17.2MiB/s]\n",
      "model.ckpt.index: 100%|████████████████████| 20.7k/20.7k [00:00<00:00, 171kiB/s]\n",
      "model.ckpt.meta: 100%|████████████████████| 1.84M/1.84M [00:00<00:00, 4.27MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.3046312861972384\n",
      "Validation loss: 3.1195147037506104\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
   "metadata": {},
   "source": [
    "# 练习5.6 用更大的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
   "metadata": {},
   "source": [
    "- 在主章节中，我们尝试了最小的GPT-2模型，只有124M参数\n",
    "- 这样做的原因是为了将资源需求保持在最低限度\n",
    "- 然而，您可以通过最少的代码更改轻松地尝试更大的模型\n",
    "- 例如，在第5章中，加载1558M模型而不是124M模型，我们只需更改以下两行代码：\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "```\n",
    "\n",
    "- 最新的代码如下\n",
    "\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e0972b-e85e-4904-a0f5-24c3eacd5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度（原始值：1024）\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # 丢弃率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b641ee88-f9d4-43ec-a787-e34199eed356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/1558M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/1558M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "load_weights_into_gpt(gpt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c98f56f4-98fc-43b4-9ee5-726e9d17c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f7853c-6e81-4f1f-a1d0-61e2c7d33a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal life. You don't have to accept your current one at once, because if you do you'll never\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/02_alternative_weight_loading/README.md">
# **加载预训练权重的替代方法（Alternative Approaches to Loading Pretrained Weights）**  

本目录包含 **不同的模型权重加载策略**，以防 **OpenAI 官方权重不可用**。  

- **[weight-loading-hf-transformers.ipynb](weight-loading-hf-transformers.ipynb)**：  
  该笔记本展示了如何使用 **Hugging Face `transformers` 库**，从 **Hugging Face Model Hub** 加载模型权重。
</file>

<file path="ch05/04_learning_rate_schedulers/README.md">
# **为训练循环添加高级优化策略**

在主章节中，我们使用了 **较为简洁的训练函数**，以保持代码的可读性，并使 **第 5 章** 的内容符合篇幅要求。然而，为了 **提高训练的稳定性和收敛速度**，可以额外添加以下优化策略：  

- **线性预热（Linear Warmup）**  
- **余弦衰减调度（Cosine Decay Schedule）**  
- **梯度裁剪（Gradient Clipping）**  

如果希望使用 **更高级的训练函数**，请参考 [附录 D: 训练循环优化](../../appendix-D/01_main-chapter-code/appendix-D.ipynb)，其中包含完整的实现代码。
</file>

<file path="ch05/05_bonus_hparam_tuning/README.md">
# **预训练的超参数优化（Optimizing Hyperparameters for Pretraining）**  

[`hparam_search.py`](hparam_search.py) 脚本基于 **[附录 D: 训练循环优化](../../appendix-D/01_main-chapter-code/appendix-D.ipynb)** 中扩展的训练函数，  
用于 **通过网格搜索（Grid Search）寻找最佳超参数配置**。  

> **💡 注意**  
> 该脚本运行时间较长，建议在 `HPARAM_GRID` 字典中减少搜索的超参数组合，以加快实验速度。
</file>

<file path="ch05/06_user_interface/README.md">
# **构建用户界面，与预训练 LLM 进行交互**  

本目录提供 **ChatGPT 风格的用户界面**，用于与 **第 5 章 训练的 LLM 进行交互**，示例如下：  

![Chainlit UI 示例](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/chainlit/chainlit-orig.webp)  

为了实现此用户界面，我们使用了 **开源的 [Chainlit Python 库](https://github.com/Chainlit/chainlit)**。  

---

## **步骤 1：安装依赖**  

首先，通过以下命令安装 `chainlit` 包：

```bash
pip install chainlit
```

（或者执行 `pip install -r requirements-extra.txt` 进行安装。）  

---

## **步骤 2：运行 `app` 代码**  

本目录包含以下两个文件：  

1. **[`app_orig.py`](app_orig.py)**：  
   - 该文件 **加载并使用 OpenAI 提供的原始 GPT-2 权重**。  

2. **[`app_own.py`](app_own.py)**：  
   - 该文件 **加载并使用我们在第 5 章生成的 GPT-2 权重**。  
   - 运行该文件前，需 **先执行** [`../01_main-chapter-code/ch05.ipynb`](../01_main-chapter-code/ch05.ipynb)。  

（建议打开并阅读这些文件，以了解其具体实现。）  

---

从终端运行以下命令之一，即可启动 UI 服务器：

```bash
chainlit run app_orig.py
```

or

```bash
chainlit run app_own.py
```

运行上述命令后，系统应 **自动在浏览器中打开新标签页**，让你直接与模型交互。  

如果浏览器 **未自动打开**，请检查终端输出，并 **复制本地访问地址**（通常为 **`http://localhost:8000`**）到浏览器地址栏手动访问。
</file>

<file path="ch05/07_gpt_to_llama/README.md">
# **从 GPT 转换为 Llama（Converting GPT to Llama）**  

本目录包含代码示例，演示如何 **将第 4 章和第 5 章的 GPT 实现转换为 Meta AI 的 Llama 体系结构**。推荐按照以下顺序阅读：  

- **[converting-gpt-to-llama2.ipynb](converting-gpt-to-llama2.ipynb)**：  
  逐步将 **GPT 转换为 Llama 2 7B**，并加载 **Meta AI 预训练权重**。  

- **[converting-llama2-to-llama3.ipynb](converting-llama2-to-llama3.ipynb)**：  
  讲解如何将 **Llama 2 迁移至 Llama 3**，并支持 **Llama 3.1 和 Llama 3.2** 版本。  

- **[standalone-llama32.ipynb](standalone-llama32.ipynb)**：  
  **Llama 3.2** 的独立实现，提供完整的代码示例。  

---

### **Llama 体系结构演进**  

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt-and-all-llamas.webp">
</file>

<file path="ch05/08_memory_efficient_weight_loading/README.md">
# **高效加载模型权重（Memory-efficient Model Weight Loading）**

本目录包含示例代码，演示如何 **更高效地加载模型权重**。  

- **[memory-efficient-state-dict.ipynb](memory-efficient-state-dict.ipynb)**：  
  该笔记本展示了如何使用 PyTorch 的 **`load_state_dict`** 方法 **优化模型权重加载**，以减少 **内存占用** 并提高 **加载速度**。
</file>

<file path="ch05/09_extending-tokenizers/extend-tiktoken.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbc1fe3-bff1-4631-bf35-342e19c54cc0",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b022374-e3f6-4437-b86f-e6f8f94cbebc",
   "metadata": {},
   "source": [
    "# **扩展 Tiktoken BPE 分词器，添加新 Token**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd624b1-2060-49af-bbf6-40517a58c128",
   "metadata": {},
   "source": [
    "- 本笔记本介绍 **如何扩展现有的 BPE 分词器**，并重点讲解 **如何在 OpenAI 的 [Tiktoken](https://github.com/openai/tiktoken) 实现中添加新 Token**。  \n",
    "- 如果需要 **分词的基础知识**，请参考 [第 2 章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) 和 **BPE from Scratch** [教程](link)。  \n",
    "- 例如，假设我们有一个 **GPT-2 分词器**，并希望对以下文本进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798d4355-a146-48a8-a1a5-c5cec91edf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 2011, 3791, 30642, 62, 16, 318, 257, 649, 11241, 13, 220, 50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "base_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "sample_text = \"Hello, MyNewToken_1 is a new token. <|endoftext|>\"\n",
    "\n",
    "token_ids = base_tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09b19b-772d-4449-971b-8ab052ee726d",
   "metadata": {},
   "source": [
    "- **遍历每个 Token ID**，可以帮助我们更好地理解 **如何通过词汇表解码 Token ID**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fd634b-bb4c-4ba3-8b69-9322b727bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15496 -> Hello\n",
      "11 -> ,\n",
      "2011 ->  My\n",
      "3791 -> New\n",
      "30642 -> Token\n",
      "62 -> _\n",
      "16 -> 1\n",
      "318 ->  is\n",
      "257 ->  a\n",
      "649 ->  new\n",
      "11241 ->  token\n",
      "13 -> .\n",
      "220 ->  \n",
      "50256 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {base_tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b1b9b-b1a9-489e-9711-c15a8e081813",
   "metadata": {},
   "source": [
    "- 如上所示，**\"MyNewToken_1\" 被拆分为 5 个子词 Token**，这对于 BPE 处理 **未知词汇** 时是正常行为。  \n",
    "- 但如果 **\"MyNewToken_1\" 是一个特殊 Token**，我们希望它像 **`\"<|endoftext|>\"`** 一样 **作为单个 Token 进行编码**，本笔记本将讲解如何实现该功能。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f62ab6-df96-4f88-ab9a-37702cd30f5f",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1. 添加特殊的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4379fdb-57ba-4a75-9183-0aee0836c391",
   "metadata": {},
   "source": [
    "- 需要注意，我们必须 **将新 Token 作为特殊 Token 添加**。原因在于：  \n",
    "  - **新 Token 在原始分词器训练过程中并未出现**，因此 **没有对应的“合并规则”（merges）**。  \n",
    "  - 即使我们手动创建这些合并规则，**也很难在不破坏现有分词体系的情况下，将其正确整合**（详情请参考 **BPE from Scratch** 笔记本 [链接] 了解“合并规则”）。  \n",
    "\n",
    "- 例如，假设我们希望 **添加 2 个新 Token**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265f1bba-c478-497d-b7fc-f4bd191b7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom tokens and their token IDs\n",
    "custom_tokens = [\"MyNewToken_1\", \"MyNewToken_2\"]\n",
    "custom_token_ids = {\n",
    "    token: base_tokenizer.n_vocab + i for i, token in enumerate(custom_tokens)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f3d98-1ab6-43cf-9ae2-2bf53860f99e",
   "metadata": {},
   "source": [
    "- 接下来，我们创建一个自定义的 **`Encoding`** 对象，用于存储 **特殊 Token**，具体如下：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f519852-59ea-4069-a8c7-0f647bfaea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Encoding object with extended tokens\n",
    "extended_tokenizer = tiktoken.Encoding(\n",
    "    name=\"gpt2_custom\",\n",
    "    pat_str=base_tokenizer._pat_str,\n",
    "    mergeable_ranks=base_tokenizer._mergeable_ranks,\n",
    "    special_tokens={**base_tokenizer._special_tokens, **custom_token_ids},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af6cfa-e0cc-4c80-89dc-3a824e7bdeb2",
   "metadata": {},
   "source": [
    "- 就这样！现在我们可以验证 **分词器是否能够正确编码示例文本**：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e8e1d-c4cb-41ff-9c55-1701e9bcae1c",
   "metadata": {},
   "source": [
    "- 如我们所见，**新添加的 Token**（`50257` 和 `50258`）**已成功编码到输出中**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eccc78a4-1fd4-47ba-a114-83ee0a3aec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36674, 2420, 351, 220, 50257, 290, 220, 50258, 13, 220, 50256]\n"
     ]
    }
   ],
   "source": [
    "special_tokens_set = set(custom_tokens) | {\"<|endoftext|>\"}\n",
    "\n",
    "token_ids = extended_tokenizer.encode(\n",
    "    \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\",\n",
    "    allowed_special=special_tokens_set\n",
    ")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0547c1-bbb5-4915-8cf4-caaebcf922eb",
   "metadata": {},
   "source": [
    "- 同样，我们还可以 **逐个 Token 检查编码结果**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7583eff9-b10d-4e3d-802c-f0464e1ef030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36674 -> Sample\n",
      "2420 ->  text\n",
      "351 ->  with\n",
      "220 ->  \n",
      "50257 -> MyNewToken_1\n",
      "290 ->  and\n",
      "220 ->  \n",
      "50258 -> MyNewToken_2\n",
      "13 -> .\n",
      "220 ->  \n",
      "50256 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {extended_tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0764e-e5a9-4226-a384-18c11bd5fec3",
   "metadata": {},
   "source": [
    "- 如上所示，我们已成功 **更新分词器**。  \n",
    "- 但如果要将其用于 **预训练的 LLM**，还需要 **更新 LLM 的嵌入层（embedding layer）和输出层（output layer）**，具体方法将在 **下一节** 进行讲解。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7f98d-8f09-4386-83f0-9bec68ef7f66",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 2. 更新预训练的LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f68b-04e9-4524-8df4-8718c7b566f2",
   "metadata": {},
   "source": [
    "- 本节将讲解 **如何在更新分词器后，对现有的预训练 LLM 进行相应调整**。  \n",
    "- 我们将使用 **书中主章节所采用的原始预训练 GPT-2 模型** 进行演示。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b252e-1d1d-4ddf-b9f3-95bd6ba505a9",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### 2.1 加载预训练的GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded29b4e-9b39-4191-b61c-29d6b2360bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 34.4kiB/s]\n",
      "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 4.78MiB/s]\n",
      "hparams.json: 100%|█████████████████████████| 90.0/90.0 [00:00<00:00, 24.7kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [00:33<00:00, 14.7MiB/s]\n",
      "model.ckpt.index: 100%|███████████████████| 5.21k/5.21k [00:00<00:00, 1.05MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 2.33MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 2.45MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93dc0d8e-b549-415b-840e-a00023bddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f898c0-18f4-49ce-9b1f-3203a277b29e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### 2.2 使用预训练过的GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1f5e1-e806-4c60-abaa-42ae8564908c",
   "metadata": {},
   "source": [
    "- 接下来，我们使用 **原始分词器** 和 **更新后的分词器** 对以下示例文本进行编码，并进行对比：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a88017d-cc8f-4ba1-bba9-38161a30f673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\"\n",
    "\n",
    "original_token_ids = base_tokenizer.encode(\n",
    "    sample_text, allowed_special={\"<|endoftext|>\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee01bc3-ca24-497b-b540-3d13c52c29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_ids = extended_tokenizer.encode(\n",
    "    \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\",\n",
    "    allowed_special=special_tokens_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143106b-68fe-4234-98ad-eaff420a4d08",
   "metadata": {},
   "source": [
    "- 现在，我们将 **原始 Token ID 输入到 GPT 模型中**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b06827f-b411-42cc-b978-5c1d568a3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2204,  0.8901,  1.0138,  ...,  0.2585, -0.9192, -0.2298],\n",
      "         [ 0.6745, -0.0726,  0.8218,  ..., -0.1768, -0.4217,  0.0703],\n",
      "         [-0.2009,  0.0814,  0.2417,  ...,  0.3166,  0.3629,  1.3400],\n",
      "         ...,\n",
      "         [ 0.1137, -0.1258,  2.0193,  ..., -0.0314, -0.4288, -0.1487],\n",
      "         [-1.1983, -0.2050, -0.1337,  ..., -0.0849, -0.4863, -0.1076],\n",
      "         [-1.0675, -0.5905,  0.2873,  ..., -0.0979, -0.8713,  0.8415]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = gpt(torch.tensor([original_token_ids]))\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c7a78-35a8-473e-a08d-b099a6348a74",
   "metadata": {},
   "source": [
    "- 如上所示，模型能够正常运行 **（为简洁起见，代码仅显示原始输出，未将其转换回文本）**。  \n",
    "- 若需了解 **如何将模型输出转换回文本**，请参考 **第 5 章 [链接] 的 `generate` 函数（5.3.3 节）**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628265b5-3dde-44e7-bde2-8fc594a2547d",
   "metadata": {},
   "source": [
    "- 如果我们 **使用更新后的分词器生成的 Token ID** 再次输入模型，会发生什么情况？  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796ad09-787c-4c25-a7f5-6d1dfe048ac3",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.no_grad():\n",
    "    gpt(torch.tensor([new_token_ids]))\n",
    "\n",
    "print(out)\n",
    "\n",
    "...\n",
    "# IndexError: index out of range in self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d00244-7e40-4de0-942e-e15cdd8e3b18",
   "metadata": {},
   "source": [
    "- 如我们所见，这会导致 **索引错误（Index Error）**。  \n",
    "- 这是因为 **GPT 模型的输入嵌入层（Embedding Layer）和输出层（Output Layer）** 预设了固定的 **词汇表大小（Vocabulary Size）**，而更新后的分词器可能已超出该范围：\n",
    "\n",
    "<img src=\"../image/gpt-updates.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec38b24-c845-4090-96a4-0d3c4ec241d6",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### **2.3 更新嵌入层（Updating the Embedding Layer）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1328726-8297-4162-878b-a5daff7de742",
   "metadata": {},
   "source": [
    "- 我们首先 **更新模型的嵌入层（Embedding Layer）**。  \n",
    "- 首先，需要注意 **嵌入层包含 50,257 个条目**，这正好对应于 **原始词汇表的大小**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ecab6e-1232-47c7-a318-042f90e1dff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.tok_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760c683-d082-470a-bff8-5a08b30d3b61",
   "metadata": {},
   "source": [
    "- 我们希望 **扩展嵌入层**，**增加 2 个新 Token**。  \n",
    "- 简而言之，我们 **创建一个更大的嵌入层**，然后 **将原始嵌入层的权重复制到新嵌入层中**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec5c48e-c6fe-4e84-b290-04bd4da9483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50259, 768)\n"
     ]
    }
   ],
   "source": [
    "num_tokens, emb_size = gpt.tok_emb.weight.shape\n",
    "new_num_tokens = num_tokens + 2\n",
    "\n",
    "# Create a new embedding layer\n",
    "new_embedding = torch.nn.Embedding(new_num_tokens, emb_size)\n",
    "\n",
    "# Copy weights from the old embedding layer\n",
    "new_embedding.weight.data[:num_tokens] = gpt.tok_emb.weight.data\n",
    "\n",
    "# Replace the old embedding layer with the new one in the model\n",
    "gpt.tok_emb = new_embedding\n",
    "\n",
    "print(gpt.tok_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63954928-31a5-4e7e-9688-2e0c156b7302",
   "metadata": {},
   "source": [
    "- 如上所示，我们的 **嵌入层（Embedding Layer）已成功扩展**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68bea5-255b-47bb-b352-09ea9539bc25",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### **2.4 更新输出层（Updating the Output Layer）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4a519-bf0f-4502-912d-ef0ac7a9deab",
   "metadata": {},
   "source": [
    "- 接下来，我们需要 **扩展输出层（Output Layer）**，该层当前包含 **50,257 个输出特征**，其大小与嵌入层的词汇表大小相同。  \n",
    "- **（顺带一提，你可能会对额外的学习资料感兴趣，其中探讨了 PyTorch 中 `Linear` 层与 `Embedding` 层的相似性。）**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6105922f-d889-423e-bbcc-bc49156d78df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.out_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1ff24-9c00-40f6-a94f-82d03aaf0890",
   "metadata": {},
   "source": [
    "- **扩展输出层（Output Layer）的过程** 与 **扩展嵌入层（Embedding Layer）** 类似：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354589db-b148-4dae-8068-62132e3fb38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=50259, bias=True)\n"
     ]
    }
   ],
   "source": [
    "original_out_features, original_in_features = gpt.out_head.weight.shape\n",
    "\n",
    "# Define the new number of output features (e.g., adding 2 new tokens)\n",
    "new_out_features = original_out_features + 2\n",
    "\n",
    "# Create a new linear layer with the extended output size\n",
    "new_linear = torch.nn.Linear(original_in_features, new_out_features)\n",
    "\n",
    "# Copy the weights and biases from the original linear layer\n",
    "with torch.no_grad():\n",
    "    new_linear.weight[:original_out_features] = gpt.out_head.weight\n",
    "    if gpt.out_head.bias is not None:\n",
    "        new_linear.bias[:original_out_features] = gpt.out_head.bias\n",
    "\n",
    "# Replace the original linear layer with the new one\n",
    "gpt.out_head = new_linear\n",
    "\n",
    "print(gpt.out_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d2205-1fae-4a4f-a7bd-fa8fc37eeec2",
   "metadata": {},
   "source": [
    "- 首先，我们先 **使用原始 Token ID 测试模型**，观察其是否仍能正常运行： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df604bbc-6c13-4792-8ba8-ecb692117c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2267,  0.9132,  1.0494,  ..., -0.2330, -0.3008, -1.1458],\n",
      "         [ 0.6808, -0.0495,  0.8574,  ...,  0.0671,  0.5572, -0.7873],\n",
      "         [-0.1947,  0.1045,  0.2773,  ...,  1.3368,  0.8479, -0.9660],\n",
      "         ...,\n",
      "         [ 0.1200, -0.1027,  2.0549,  ..., -0.1519, -0.2096,  0.5651],\n",
      "         [-1.1920, -0.1819, -0.0981,  ..., -0.1108,  0.8435, -0.3771],\n",
      "         [-1.0612, -0.5674,  0.3229,  ...,  0.8383, -0.7121, -0.4850]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = gpt(torch.tensor([original_token_ids]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80717e-50e6-4927-8129-0aadfa2628f5",
   "metadata": {},
   "source": [
    "- 接下来，让我们 **测试更新后的模型在新增 Token 上的表现**。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f11ec9-bdd2-440f-b8c8-6646b75891c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2267,  0.9132,  1.0494,  ..., -0.2330, -0.3008, -1.1458],\n",
      "         [ 0.6808, -0.0495,  0.8574,  ...,  0.0671,  0.5572, -0.7873],\n",
      "         [-0.1947,  0.1045,  0.2773,  ...,  1.3368,  0.8479, -0.9660],\n",
      "         ...,\n",
      "         [-0.0656, -1.2451,  0.7957,  ..., -1.2124,  0.1044,  0.5088],\n",
      "         [-1.1561, -0.7380, -0.0645,  ..., -0.4373,  1.1401, -0.3903],\n",
      "         [-0.8961, -0.6437, -0.1667,  ...,  0.5663, -0.5862, -0.4020]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = gpt(torch.tensor([new_token_ids]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a1bba-db01-4090-97e4-25dfc23ed54c",
   "metadata": {},
   "source": [
    "- 如我们所见，**模型已成功支持扩展后的 Token 集**。  \n",
    "- 实际应用中，我们通常需要 **对模型进行微调（Fine-tuning）或持续预训练（Continual Pretraining）**，特别是 **新扩展的嵌入层（Embedding Layer）和输出层（Output Layer）**，以确保模型能够有效学习新 Token 的表示。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de573ad-0338-40d9-9dad-de60ae349c4f",
   "metadata": {},
   "source": [
    "### **关于权重共享（Weight Tying）**  \n",
    "\n",
    "- **如果模型使用了权重共享（Weight Tying）**，即 **嵌入层（Embedding Layer）与输出层（Output Layer）共享相同的权重**（类似于 **Llama 3** [链接]），那么 **扩展输出层的过程将更为简单**。  \n",
    "- 在这种情况下，我们 **只需直接将嵌入层的权重复制到输出层**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbc5f51-c7a8-49d0-b87f-d3d87510953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.out_head.weight = gpt.tok_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d553a8-edff-40f0-bdc4-dff900e16caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = gpt(torch.tensor([new_token_ids]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/09_extending-tokenizers/README.md">
## **扩展 Tiktoken BPE 分词器以支持新标记**

在本教程中，我们将探讨如何在 `tiktoken` 实现的分词器（Tokenizer）中添加特殊标记（Special Tokens），并相应地更新 LLM（大语言模型）。

- [📜 extend-tiktoken.ipynb](extend-tiktoken.ipynb)  
  该 Jupyter Notebook 文件包含可选（额外）代码，详细说明了如何向 `tiktoken` 分词器添加特殊标记，并调整 LLM 以支持这些标记。
</file>

<file path="ch05/README.md">
# 第五章：在无标签数据上进行预训练

&nbsp;
## 章节主要代码

- [01_main-chapter-code](01_main-chapter-code) 包含了章节的主要代码。

&nbsp;
## 额外材料

- [02_alternative_weight_loading](02_alternative_weight_loading) 包含了从替代来源加载 GPT 模型权重的代码，以防 OpenAI 提供的模型权重不可用。
- [03_bonus_pretraining_on_gutenberg](03_bonus_pretraining_on_gutenberg) 包含了使用 Project Gutenberg 的全部书籍语料库上对大语言模型进行更长时间预训练的代码。
- [04_learning_rate_schedulers](04_learning_rate_schedulers) 包含了实现更复杂训练功能的代码，包括学习率调度器和梯度裁剪。
- [05_bonus_hparam_tuning](05_bonus_hparam_tuning) 包含了一个可选的超参数调优脚本。
- [06_user_interface](06_user_interface) 实现了一个交互式用户界面，用于与预训练的大语言模型进行互动。
- [07_gpt_to_llama](07_gpt_to_llama) 包含了将 GPT 架构实现转换为 Llama 3.2 的逐步指南，并加载来自 Meta AI 的预训练权重。
- [08_memory_efficient_weight_loading](08_memory_efficient_weight_loading) 包含了一个额外的笔记本，展示如何通过 PyTorch 的 `load_state_dict` 方法更高效地加载模型权重。
</file>

<file path="ch07/02_dataset-utilities/create-passive-voice-entries.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 为指令数据集创建“被动语态”数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- 如下所示,我们用gpt-4来创建被动语态\n",
    "\n",
    "```python\n",
    "{  \n",
    "   'instruction': 'Identify the verb in the following sentence',\n",
    "   'input': 'The cat sleeps on the couch.',\n",
    "   'output': 'The verb in the sentence is \"sleeps.\"',\n",
    "   'output_2': 'The sentence is \"sleeps.\"'   #  <---- 新创建的条目\n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 1.30.3\n",
      "tqdm version: 4.65.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"openai\",  # OpenAI API\n",
    "        \"tqdm\",    # 进度条\n",
    "       ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## Test OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- 首先，让我们测试一下OpenAI API是否已正确设置\n",
    "- 如果您还没有账户，需要在 https://platform.openai.com/ 创建一个\n",
    "- 请注意，您还需要向账户中转账，因为GPT-4 API不是免费的（请参见 https://platform.openai.com/settings/organization/billing/overview）\n",
    "- 使用本笔记本中的代码创建约200个被动语态条目大约需要花费0.13美元（13美分）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": [
    "- 首先，我们需要提供OpenAI API的密钥，您可以在 https://platform.openai.com/api-keys 找到该密钥\n",
    "- 请确保不要与任何人共享此密钥\n",
    "- 将此密钥（`\"sk-...\"`）添加到此文件夹中的 `config.json` 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26900564-aba7-48ba-8ee8-6cc9a505a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# 从 JSON 文件中加载 API 密钥. \n",
    "# 请确保将 \"sk-...\" 替换为您在 https://platform.openai.com/api-keys 上的实际 API 密钥\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- 首先，让我们通过一个简单的示例来测试API，确保它按预期工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e9ef2e-e816-4283-840e-43625791ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breakfast was eaten by me.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# 准备输入\n",
    "sentence = \"I ate breakfast\"\n",
    "prompt = f\"Convert the following sentence to passive voice: '{sentence}'\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 创建json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": [
    "- 导入想要优化的json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file = \"instruction-examples.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "    \n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 小试牛刀,确保API可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      ">> The verb in the sentence is \"sleeps.\"\n",
      "\n",
      "Output:\n",
      ">> The sentence is \"sleeps.\"\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Input:\n",
      ">> The plural form of \"goose\" is \"geese.\"\n",
      "\n",
      "Output:\n",
      ">> The plural form of \"goose\" is referred to as \"geese.\"\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Input:\n",
      ">> The three primary colors are red, blue, and yellow.\n",
      "\n",
      "Output:\n",
      ">> Red, blue, and yellow are considered the three primary colors.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Input:\n",
      ">> They had finished the game.\n",
      "\n",
      "Output:\n",
      ">> The game had been finished by them.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Input:\n",
      ">> The abbreviation for \"Doctor of Philosophy\" is Ph.D.\n",
      "\n",
      "Output:\n",
      ">> The abbreviation \"Ph.D.\" is used for \"Doctor of Philosophy\".\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[:5]:\n",
    "    text = entry[\"output\"]\n",
    "    prompt = f\"Without adding any response or explanation, convert the following text to passive voice: {text}\"\n",
    "    \n",
    "    print(\"\\nInput:\")\n",
    "    print(\">>\", text)\n",
    "    print(\"\\nOutput:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- 拓展代码也拓展数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 一个进度条工具\n",
    "\n",
    "\n",
    "for i, entry in tqdm(enumerate(json_data[:5]), total=len(json_data[:5])):\n",
    "    text = entry[\"output\"]\n",
    "    prompt = f\"Without adding any response or explanation, convert the following text to passive voice: {text}\"\n",
    "    json_data[i][\"output_2\"] = run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd144282-0596-4e9b-9815-322cff34b400",
   "metadata": {},
   "source": [
    "- 确保输出合理有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6eaa87-a86d-42a1-a20a-b764b0d559d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Identify the verb in the following sentence: The cat sleeps on the couch.',\n",
       " 'input': '',\n",
       " 'output': 'The verb in the sentence is \"sleeps.\"',\n",
       " 'output_2': 'The sentence is \"sleeps.\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970e8cf-2b18-4e3d-9f25-e6a4489c39a7",
   "metadata": {},
   "source": [
    "- 万事俱备,运行下程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef99407-8ffd-4a63-b7ab-ffe30c0f0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 200/200 [03:43<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, entry in tqdm(enumerate(json_data), total=len(json_data)):\n",
    "    text = entry[\"output\"]\n",
    "    prompt = f\"Without adding any response or explanation, convert the following text to passive voice: {text}\"\n",
    "    json_data[i][\"output_2\"] = run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91ae85-2f0e-456a-be1d-56e1958f30d8",
   "metadata": {},
   "source": [
    "- 测试结束,保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330cc30a-b08e-4bf0-bee2-bec0da4208de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_file = json_file.replace(\".json\", \"-modified.json\")\n",
    "\n",
    "\n",
    "with open(new_json_file, \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)  # \"indent\"设置为了更美观的展示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/03_model-evaluation/README.md">
# 第 7 章：指令微调（Finetuning to Follow Instructions）

本文件夹包含用于 **模型评估** 的工具代码。

&nbsp;

## 使用 OpenAI API 评估指令响应

- [llm-instruction-eval-openai.ipynb](llm-instruction-eval-openai.ipynb) 笔记本使用 **OpenAI 的 GPT-4** 来评估 **指令微调模型** 生成的响应。  
- 该笔记本适用于以下格式的 **JSON 文件**：

```python
{
    "instruction": "What is the atomic number of helium?",
    "input": "",
    "output": "The atomic number of helium is 2.",               # <-- The target given in the test set
    "model 1 response": "\nThe atomic number of helium is 2.0.", # <-- Response by an LLM
    "model 2 response": "\nThe atomic number of helium is 3."    # <-- Response by a 2nd LLM
},
```

&nbsp;

## 本地使用 Ollama 评估指令响应

- [llm-instruction-eval-ollama.ipynb](llm-instruction-eval-ollama.ipynb) 笔记本提供了 **另一种评估方法**，使用 **Ollama 本地下载的 LLaMA 3 模型** 进行评估。
</file>

<file path="ch07/04_preference-tuning-with-dpo/README.md">
# 第 7 章：指令微调（Finetuning to Follow Instructions）

- [create-preference-data-ollama.ipynb](create-preference-data-ollama.ipynb)：使用 **LLaMA 3.1** 和 **Ollama** 生成 **偏好微调数据集（Preference Finetuning Dataset）** 的笔记本。  
- [dpo-from-scratch.ipynb](dpo-from-scratch.ipynb)：实现 **直接偏好优化（Direct Preference Optimization, DPO）** 以对 **LLM 进行对齐（Alignment）** 的笔记本。
</file>

<file path="ch07/05_dataset-generation/README.md">
# 生成指令微调数据集

本文件夹包含用于 **生成指令微调数据集** 的工具代码。

- [llama3-ollama.ipynb](llama3-ollama.ipynb)：使用 **LLaMA 3** 和 **Ollama** 生成 **合成指令微调数据集** 的笔记本。  
- [reflection-gpt4.ipynb](reflection-gpt4.ipynb)：基于 **反思微调（Reflection-Tuning）** 进行 **指令数据集优化** 的笔记本。
</file>

<file path="ch07/06_user_interface/README.md">
# 构建用户界面以交互指令微调的 GPT 模型

本 **附加文件夹（bonus folder）** 包含 **运行 ChatGPT 风格用户界面** 的代码，以便与 **第 7 章** 训练的 **指令微调 GPT** 进行交互，如下所示：

![Chainlit UI 示例](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/chainlit/chainlit-sft.webp?2)

为了实现该用户界面，我们使用了 **开源 Python 库** [Chainlit](https://github.com/Chainlit/chainlit)。

&nbsp;

## 步骤 1：安装依赖项

首先，我们通过以下命令安装 `chainlit` 库：


```bash
pip install chainlit
```

（或者，执行 `pip install -r requirements-extra.txt`。）

&nbsp;

## 步骤 2：运行 `app` 代码

[`app.py`](app.py) 文件包含 **基于 UI 的代码**，您可以打开并检查这些文件，以了解其具体实现。  

该文件 **加载并使用** 我们在 **第 7 章** 生成的 **GPT-2 权重**，因此需要 **先执行** [`../01_main-chapter-code/ch07.ipynb`](../01_main-chapter-code/ch07.ipynb)。  

在终端中执行以下命令，以启动 **UI 服务器**：


```bash
chainlit run app.py
```

- 运行上述命令后，应会 **自动打开一个新的浏览器标签页**，您可以在其中与模型进行交互。  
- 如果浏览器未自动打开，请检查 **终端输出**，然后 **手动复制本地地址** 到浏览器地址栏（通常是 `http://localhost:8000`）。
</file>

<file path="setup/01_optional-python-setup-preferences/README.md">
# Python 环境设置指南

以下介绍了几种安装 Python 并设置计算环境的方法。这是我的个人推荐方法。

（我使用的是运行 macOS 的计算机，但此工作流程与 Linux 机器类似，可能也适用于其他操作系统。）

<br>
<br>

## 1. 下载并安装 Miniforge

从 [GitHub 仓库](https://github.com/conda-forge/miniforge) 下载 Miniforge。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/download.png" alt="download" width="600px">

根据操作系统的不同，这会下载一个 `.sh` 文件（适用于 macOS 和 Linux）或 `.exe` 文件（适用于 Windows）。

对于 `.sh` 文件，打开命令行终端并执行以下命令：

```bash
sh ~/Desktop/Miniforge3-MacOSX-arm64.sh
```

其中，`Desktop/` 是下载 Miniforge 安装程序的文件夹。在你的计算机上，可能需要将其替换为 `Downloads/`。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/miniforge-install.png" alt="miniforge-install" width="600px">

接下来，按照下载说明逐步操作，按 "Enter" 键确认。


<br>
<br>


## 2. 创建新的虚拟环境

安装成功后，我建议创建一个名为 `LLMs` 的新虚拟环境，你可以通过执行以下命令完成：

```bash
conda create -n LLMs python=3.10
```

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/new-env.png" alt="new-env" width="600px">

> 许多科学计算库并不立即支持最新版本的 Python。因此，在安装 PyTorch 时，建议使用比最新版本低一到两个版本的 Python。例如，如果最新版本是 Python 3.13，推荐使用 Python 3.10 或 3.11。

接下来，激活你新创建的虚拟环境（每次打开新的终端窗口或标签页时都需要激活）： 

```bash
conda activate LLMs
```

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/activate-env.png" alt="activate-env" width="600px">

<br>
<br>

## 可选：美化你的terminal

如果你希望将终端样式设置得与我的类似，以便显示当前激活的虚拟环境，可以查看 [Oh My Zsh](https://github.com/ohmyzsh/ohmyzsh) 项目。

<br>
<br>

## 3. 安装新的 Python 库

现在，你可以使用 `conda` 包管理器来安装新的 Python 库。例如，可以通过以下命令安装 [JupyterLab](https://jupyter.org/install) 和 [watermark](https://github.com/rasbt/watermark)：

```bash
conda install jupyterlab watermark
```

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/conda-install.png" alt="conda-install" width="600px">



你也可以继续使用 `pip` 安装库。默认情况下，`pip` 应已链接到你的新 `LLMs` conda 环境：

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/check-pip.png" alt="check-pip" width="600px">

<br>
<br>

## 4. 安装 PyTorch

PyTorch 可以像其他 Python 库或包一样通过 pip 安装。例如：

```bash
pip install torch
```

然而，由于 PyTorch 是一个功能全面的库，支持 CPU 和 GPU 代码，其安装可能需要额外的设置和说明（更多信息请参阅书中 *A.1.3 Installing PyTorch* 部分）。

同时，强烈建议参考 PyTorch 官方网站的安装指南：[https://pytorch.org](https://pytorch.org)。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/01_optional-python-setup-preferences/pytorch-installer.jpg" width="600px">

## 5. 安装本书使用的 Python 包和库

请参考 [Installing Python packages and libraries used in this book](../02_installing-python-libraries/README.md) 文档，了解如何安装所需库。

<br>

---

有任何问题？欢迎在 [讨论论坛](https://github.com/rasbt/LLMs-from-scratch/discussions) 中提问。
</file>

<file path="setup/02_installing-python-libraries/README.md">
# 安装本书所需的 Python 包和库

本文档提供了关于如何检查 Python 版本和已安装包的详细信息。（如需了解如何安装 Python 和 Python 包，请参阅 [../01_optional-python-setup-preferences](../01_optional-python-setup-preferences) 目录。）

本书使用了[此处](https://github.com/rasbt/LLMs-from-scratch/blob/main/requirements.txt)列出的库。这些库的较新版本通常也是兼容的，但如果您在运行代码时遇到问题，可以尝试使用这些指定版本作为备用方案。

为了更方便地安装这些依赖项，您可以使用代码库根目录中的 `requirements.txt` 文件，并运行以下命令：

```bash
pip install -r requirements.txt
```

或者，您也可以通过 GitHub URL 安装，命令如下：

```bash
pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/requirements.txt
```

安装完成后，请使用以下命令检查所有包是否已安装并更新至最新版本：

```bash
python python_environment_check.py
```
![检查安装结果](https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/02_installing-python-libraries/check_1.jpg)

建议在 JupyterLab 中运行本目录下的 `python_environment_check.ipynb` 文件来检查版本，结果应与上图一致。

![JupyterLab 检查结果](https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/02_installing-python-libraries/check_2.jpg)

如果您看到以下问题，可能是您的 JupyterLab 实例连接到了错误的 conda 环境：

![JupyterLab 环境问题](https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/02_installing-python-libraries/jupyter-issues.jpg)

此时，您可以使用 `watermark` 工具，并通过 `--conda` 标志检查是否在正确的 conda 环境中打开了 JupyterLab 实例：

![使用 watermark 检查环境](https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/02_installing-python-libraries/watermark.jpg)

<br>
<br>

## 安装 PyTorch

PyTorch 可以像其他 Python 库一样使用 pip 安装。例如：

```bash
pip install torch
```

由于 PyTorch 是一个功能全面的库，支持 CPU 和 GPU 兼容的代码，安装时可能需要额外的设置和说明（更多信息请参阅本书的 *A.1.3 安装 PyTorch* 部分）。

强烈建议参考 PyTorch 官方网站 [https://pytorch.org](https://pytorch.org) 上的安装指南。

![PyTorch 安装界面](https://sebastianraschka.com/images/LLMs-from-scratch-images/setup/02_installing-python-libraries/pytorch-installer.jpg)

<br>

---

如有任何问题，欢迎在 [讨论区](https://github.com/rasbt/LLMs-from-scratch/discussions) 中提问。
</file>

<file path="setup/03_optional-docker-environment/README.md">
# Docker 环境设置指南

如果你更喜欢隔离项目的依赖项和配置，使用 Docker 是一种非常有效的解决方案。通过这种方法，可以避免手动安装软件包和库，同时确保开发环境的一致性。

本指南将引导你为本书设置一个可选的 Docker 环境，如果你不想使用 [../01_optional-python-setup-preferences](../01_optional-python-setup-preferences) 和 [../02_installing-python-libraries](../02_installing-python-libraries) 中解释的 conda 方法，可以参考此方法。

<br>

## 下载并安装 Docker

使用 Docker 的最简单方法是为你的平台安装 [Docker Desktop](https://docs.docker.com/desktop/)。

Linux（Ubuntu）用户可以选择安装 [Docker Engine](https://docs.docker.com/engine/install/ubuntu/) 并按照 [安装后配置步骤](https://docs.docker.com/engine/install/linux-postinstall/) 进行操作。

<br>

## 在 Visual Studio Code 中使用 Docker DevContainer

Docker DevContainer（开发容器）是一种工具，它允许开发者将 Docker 容器作为功能完整的开发环境使用。这种方法可以确保用户快速搭建一个一致的开发环境，而无需依赖本地机器的配置。

虽然 DevContainer 也可以与其他 IDE 一起使用，但一个常用的 IDE/编辑器是 Visual Studio Code（VS Code）。以下指南说明了如何在 VS Code 环境中使用本书的 DevContainer，但类似的过程也适用于 PyCharm。如果你尚未安装 VS Code，并且想使用它，请 [下载](https://code.visualstudio.com/download)。

1. 克隆此 GitHub 仓库并使用 `cd` 命令进入项目根目录。

```bash
git clone https://github.com/rasbt/LLMs-from-scratch.git
cd LLMs-from-scratch
```

2. 将 `.devcontainer` 文件夹从 `setup/03_optional-docker-environment/` 移动到当前目录（项目根目录）。

```bash
mv setup/03_optional-docker-environment/.devcontainer ./
```

3. 在 Docker Desktop 中，确保 **_desktop-linux_ builder** 正在运行并将用于构建 Docker 容器（路径：_Docker Desktop_ -> _Change settings_ -> _Builders_ -> _desktop-linux_ -> _..._ -> _Use_）。

4. 
   1. 安装 NVIDIA Container Toolkit

    按照 [此处](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt) 的说明安装 **NVIDIA Container Toolkit**。根据 [此处](https://docs.nvidia.com/cuda/wsl-user-guide/index.html#nvidia-compute-software-support-on-wsl-2) 的说明，NVIDIA Container Toolkit 在 WSL 2 上受支持。

    2. 在 Docker Engine 配置中添加 NVIDIA 运行时

        在 Docker Engine 守护程序（daemon）配置中添加 `_nvidia_` 作为运行时（路径：_Docker Desktop_ -> _更改设置_ -> _Docker Engine_）。在配置文件中添加以下内容：

    ```json
    "runtimes": {
        "nvidia": {
        "path": "nvidia-container-runtime",
        "runtimeArgs": []
    ```

    例如，完整的 Docker Engine 守护程序配置 JSON 代码应如下所示：

    ```json
    {
      "builder": {
        "gc": {
          "defaultKeepStorage": "20GB",
          "enabled": true
        }
      },
      "experimental": false,
      "runtimes": {
        "nvidia": {
          "path": "nvidia-container-runtime",
          "runtimeArgs": []
        }
      }
    }
    ```

    然后重启 Docker Desktop。


5. 在终端中输入 `code .` 以在 VS Code 中打开项目。或者，你也可以启动 VS Code，并从界面中选择项目打开。

6. 从 VS Code 左侧的 _Extensions_ 菜单中安装 **Remote Development** 扩展。

7. 打开 DevContainer。

由于 `.devcontainer` 文件夹位于主目录 `LLMs-from-scratch` 中（根据系统设置，可能无法直接看到以 `.` 开头的文件夹），VS Code 应自动检测到它，并询问你是否希望在 DevContainer 中打开项目。如果没有弹出提示，可以按 `Ctrl + Shift + P` 打开命令面板，输入 `dev containers` 查看所有 DevContainer 相关选项。

8. 选择 **Reopen in Container**。

如果尚未构建 Docker 镜像，Docker 将开始根据 `.devcontainer` 配置文件构建镜像；如果镜像已在注册表中，则会拉取镜像。

整个过程是自动化的，可能需要几分钟，具体取决于你的系统和网络速度。可以选择点击 VS Code 右下角的 "Starting Dev Container (show log)" 查看当前构建进度。

完成后，VS Code 将自动连接到容器并在新创建的 Docker 开发环境中重新打开项目。你可以像在本地机器上一样编写、运行和调试代码，同时享受 Docker 带来的隔离性和一致性。

> [!警告]
> 如果在构建过程中遇到错误，这可能是由于你的机器不支持 NVIDIA 容器工具包，因为你的 GPU 不兼容。在这种情况下，编辑 `devcontainer.json` 文件，删除 `"runArgs": ["--runtime=nvidia", "--gpus=all"],` 行，然后重新运行 "Reopen Dev Container" 步骤。

9. 完成。

镜像拉取和构建完成后，你的项目将挂载到容器内，并安装好所有必要的包，随时可以进行开发。

<br>

## 卸载 Docker 镜像

以下是卸载或移除 Docker 容器和镜像的说明，如果你不再计划使用它们。这一过程不会从系统中移除 Docker 本身，而是清理与项目相关的 Docker 文件。

1. 列出所有 Docker 镜像，以找到与 DevContainer 相关的镜像：
```bash
docker image ls
```

2. 使用镜像 ID 或名称删除 Docker 镜像：

```bash
docker image rm [IMAGE_ID_OR_NAME]
```

<br>

## 卸载 Docker

如果你决定不再使用 Docker 并希望卸载它，请参阅官方文档 [这里](https://docs.docker.com/desktop/uninstall/)，其中详细说明了针对不同操作系统的卸载步骤。
</file>

<file path="appendix-A/01_main-chapter-code/code-part1.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896245e-57c4-48fd-854f-9e43f22e10c9",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fc8a0-280c-4979-b0c7-fc3a99b3b785",
   "metadata": {},
   "source": [
    "# 附录A: Pytorch介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf13d2-8fc2-483e-88cc-6b4310221e68",
   "metadata": {},
   "source": [
    "## A.1 什么是PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ee5660-5327-48e2-9104-a882b3b2afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73ad4e4-7ec6-4467-a9e9-0cdf6d195264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ba1ab-3306-4965-8618-1ed5f24fb939",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/1.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c0555-88f6-4515-8c99-aa56b0769d54",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/2.png\" width=\"300px\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/3.png\" width=\"300px\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/4.png\" width=\"500px\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/5.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100cf2e-7459-4ab3-92a8-43e86ab35a9b",
   "metadata": {},
   "source": [
    "## A.2 理解张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c484e87-bfc9-4105-b0a7-1e23b2a72a30",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A2/1.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7f785-e048-42bc-9182-a556af6bb7f4",
   "metadata": {},
   "source": [
    "### A.2.1 标量、向量、矩阵和张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a464d6-cec8-4363-87bd-ea4f900baced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 从Python整数创建一个零维张量（标量）\n",
    "tensor0d = torch.tensor(1)\n",
    "\n",
    "# 从Python列表创建一个一维张量（向量）\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 从嵌套的Python列表创建一个二维张量\n",
    "tensor2d = torch.tensor([[1, 2], \n",
    "                         [3, 4]])\n",
    "\n",
    "# 从嵌套的Python列表创建一个三维张量\n",
    "tensor3d_1 = torch.tensor([[[1, 2], [3, 4]], \n",
    "                           [[5, 6], [7, 8]]])\n",
    "\n",
    "# 从NumPy数组创建一个张量\n",
    "ary3d = np.array([[[1, 2], [3, 4]], \n",
    "                  [[5, 6], [7, 8]]])\n",
    "tensor3d_2 = torch.tensor(ary3d)  # 复制 NumPy 数组\n",
    "tensor3d_3 = torch.from_numpy(ary3d)  # 与 NumPy 数组共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe14c47-499a-4d48-b354-a0e6fd957872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "ary3d[0, 0, 0] = 999\n",
    "print(tensor3d_2) # 保持不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e4c23a-cdba-46f5-a2dc-5fb32bf9117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[999,   2],\n",
      "         [  3,   4]],\n",
      "\n",
      "        [[  5,   6],\n",
      "         [  7,   8]]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor3d_3) # 由于共享内存而发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dec48d-2b60-41a2-ac06-fef7e718605a",
   "metadata": {},
   "source": [
    "### A.2.2 张量数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f48c014-e1a2-4a53-b5c5-125812d4034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5429a086-9de2-4ac7-9f14-d087a7507394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a438d1-49bb-481c-8442-7cc2bb3dd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020deb5-aa02-4524-b311-c010f4ad27ff",
   "metadata": {},
   "source": [
    "### A.2.3 常见的PyTorch张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02095f2-8a48-4953-b3c9-5313d4362ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d = torch.tensor([[1, 2, 3], \n",
    "                         [4, 5, 6]])\n",
    "tensor2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33e1d45-5b2c-4afe-b4b2-66ac4099fd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a4129d-f870-4e03-9c32-cd8521cb83fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.reshape(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589ac0a7-adc7-41f3-b721-155f580e9369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.view(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344e307f-ba5d-4f9a-a791-2c75a3d1417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a75030-6a41-4ca8-9aae-c507ae79225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.matmul(tensor2d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c950bc-d640-4203-b210-3ac8932fe4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d @ tensor2d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15bdeb-78e2-4870-8a4f-a9f591666f38",
   "metadata": {},
   "source": [
    "## A.3 将模型视为计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e16c3-07df-44b6-9106-a42fb24452a9",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A3/1.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22af61e9-0443-4705-94d7-24c21add09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])  # 真实标签\n",
    "x1 = torch.tensor([1.1]) # 输入特征\n",
    "w1 = torch.tensor([2.2]) # 权重参数\n",
    "b = torch.tensor([0.0])  # 偏置单元\n",
    "\n",
    "z = x1 * w1 + b          # 净输入\n",
    "a = torch.sigmoid(z)     # 激活和输出\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9424f26-2bac-47e7-b834-92ece802247c",
   "metadata": {},
   "source": [
    "## A.4 轻松实现自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa2ee4-6f1d-448d-8707-67cd5278233c",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A4/1.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf5cef7-48d6-4d2a-8ab0-0fb10bdd7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b \n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c5875d-f6b2-492c-b5ef-7e132f93a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53bdd7d-44e6-40ab-8a5a-4eef74ef35dc",
   "metadata": {},
   "source": [
    "## A.5 实现多层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb9787-2bc8-4379-9e8c-a3401ac63c51",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A5/1.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b749e1-7768-4cfe-94d6-a08c7feff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b59e2e-1930-456d-93b9-f69263e3adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d02a21-33e7-4879-8fd2-d6309faf2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94535738-de02-4c2a-9b44-1cd186fa990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c394106-ad71-4ccb-a3c9-9b60af3fa748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1182,  0.0606, -0.1292,  ..., -0.1126,  0.0735, -0.0597],\n",
      "        [-0.0249,  0.0154, -0.0476,  ..., -0.1001, -0.1288,  0.1295],\n",
      "        [ 0.0641,  0.0018, -0.0367,  ..., -0.0990, -0.0424, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0618,  0.0867,  0.1361,  ..., -0.0254,  0.0399,  0.1006],\n",
      "        [ 0.0842, -0.0512, -0.0960,  ..., -0.1091,  0.1242, -0.0428],\n",
      "        [ 0.0518, -0.1390, -0.0923,  ..., -0.0954, -0.0668, -0.0037]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b201882b-9285-4db9-bb63-43afe6a2ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da9a35e-44f3-460c-90fe-304519736fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57eadbae-90fe-43a3-a33f-c23a095ba42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "X = torch.rand((1, 50))\n",
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d720cb-ef73-4b7b-92e0-8198a072defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10df3640-83c3-4061-a74d-08f07a5cc6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19858180-0f26-43a8-b2c3-7ed40abf9f85",
   "metadata": {},
   "source": [
    "## A.6 设置高效的数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98d8fc-5618-47a2-bc72-153818972a24",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A6/1.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9dc2745-8be8-4344-80ef-325f02cda7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88283948-5fca-461a-98a1-788b6be191d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edf323e2-1789-41a0-8e44-f3cab16e5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]        \n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7014705-1fdc-4f72-b892-d8db8bebc331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ec6627a-4c3f-481a-b794-d2131be95eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c9446de-5e4b-44fa-bf9a-a63e2661027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d4404c-9884-419f-979c-f659742d86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d003f7e-7a80-40bf-a7fb-7a0d7dbba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4db4d7f4-82da-44a4-b94e-ee04665d9c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03ed57-df38-4ee0-a553-0863450df39b",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A6/2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904ca82-e50f-4f3d-a3ac-fc6ca53dd00e",
   "metadata": {},
   "source": [
    "## A.7 典型的训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f1791a-d887-4fc5-a307-5e5bde9e06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels) # 损失函数\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        ### 日志\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # 插入可选的模型评估代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00dcf57f-6a7e-4af7-aa5a-df2cb0866fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19be7390-18b8-43f9-9841-d7fb1919f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)\n",
    "\n",
    "predictions = torch.argmax(probas, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07e7e530-f8d3-429c-9f5e-cf8078078c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f756f0d-63c8-41b5-a5d8-01baa847e026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da274bb0-f11c-4c81-a880-7a031fbf2943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16d62314-8dee-45b0-8f55-9e5aae2b24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f6c9c17-2a5f-46c0-804b-873f169b729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311ed864-e21e-4aac-97c7-c6086caef27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cd469-3a45-4394-944b-3ce543f41dac",
   "metadata": {},
   "source": [
    "## A.8 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b013127d-a2c3-4b04-9fb3-a6a7c88d83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2b428c2-3a44-4d91-97c4-8298cf2b51eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(2, 2) # 需要与最初保存的模型完全匹配\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891c013-43da-4a05-973d-997be313d2d8",
   "metadata": {},
   "source": [
    "## A.9 使用 GPU 优化训练性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ae888-cabf-49c9-bad6-ecdce774db57",
   "metadata": {},
   "source": [
    "### A.9.1 在GPU设备运行PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c845f-efe3-4614-b376-b8b7a9a2c887",
   "metadata": {},
   "source": [
    "见 [code-part2.ipynb](code-part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99811829-b817-42ea-b03e-d35374debcc0",
   "metadata": {},
   "source": [
    "### A.9.2 单个GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21456c-4af7-440f-9e78-37770277b5bc",
   "metadata": {},
   "source": [
    "见 [code-part2.ipynb](code-part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6eb2d1-a341-4489-b04b-635c26945333",
   "metadata": {},
   "source": [
    "### A.9.3 使用多个GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d049a81-5fb0-49b5-9d6a-17a9976d8520",
   "metadata": {},
   "source": [
    "见 [DDP-script.py](DDP-script.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="appendix-A/01_main-chapter-code/code-part2.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAAnDw04iAm4"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9i6kzBsZVaZ"
   },
   "source": [
    "# Appendix A: Pytorch介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppbG5d-NZezH"
   },
   "source": [
    "## A.9 用GPU优化训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jH0J_DPZhbn"
   },
   "source": [
    "### A.9.1 在GPU设备运行PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RM7kGhwMF_nO",
    "outputId": "b1872617-aacd-46fa-e5f3-f130fd81b246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXLCKXhiUkZt",
    "outputId": "e9ca3c58-d92c-4c8b-a9c9-cd7fcc1fedb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTTlfh53Va-T",
    "outputId": "bae76cb5-d1d3-441f-a7c5-93a161e2e86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.tensor([1., 2., 3.])\n",
    "tensor_2 = torch.tensor([4., 5., 6.])\n",
    "\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4LwTNw7Vmmb",
    "outputId": "9ad97923-bc8e-4c49-88bf-48dc1de56804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = tensor_1.to(\"cuda\")\n",
    "tensor_2 = tensor_2.to(\"cuda\")\n",
    "\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "tKT6URN1Vuft",
    "outputId": "8396eb18-47c8-47a1-c1b6-8bcb9480fb52"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2321/2079609735.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtensor_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtensor_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "tensor_1 = tensor_1.to(\"cpu\")\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8j1cWDcWAMf"
   },
   "source": [
    "### A.9.2 单个GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GyY59cjieitv"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v41gKqEJempa"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UPGVRuylep8Y"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "drhg6IXofAXh"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "\n",
    "            # 第一隐藏层\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 第二隐藏层\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 输出层\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jaS5sqPWCY0",
    "outputId": "8a5cd93d-671c-4abf-d5cd-97845f300ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 新增代码\n",
    "model = model.to(device) # 新增代码\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        features, labels = features.to(device), labels.to(device) # 新增代码\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels) # 损失函数\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### 日志\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # 插入可用的模型评估代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4qrlmnPPe7FO"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, device):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        features, labels = features.to(device), labels.to(device) # 新增代码\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_-BfkfEf4HX",
    "outputId": "9453154f-0a5b-4a44-a3c9-f010e08d5a2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, train_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYtXKBGEgKss",
    "outputId": "d6cc870a-34de-490e-e5d3-23e6956744bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc2LGFVbiAnB"
   },
   "source": [
    "### A.9.3 Training with multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOUza9iQiAnC"
   },
   "source": [
    "See [DDP-script.py](DDP-script.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOYk5Fh7iAnC"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A9/1.png\" width=\"600px\">\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A9/2.png\" width=\"600px\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="ch02/01_main-chapter-code/dataloader.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2a4891-c257-4d6b-afb3-e8fef39d0437",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# 主数据加载流程概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070000fc-a7b7-4c56-a2c0-a938d413a790",
   "metadata": {},
   "source": [
    "完整的章节代码位于[ch02.ipynb](./ch02.ipynb)。\n",
    "\n",
    "该笔记本包含了数据加载流程的主要要点，但不包括中间步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e8f2d-cb81-41a3-8780-a70b382e18ae",
   "metadata": {},
   "source": [
    "本章所需的包如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ed6fbe-45ac-40ce-8ea5-4edb212565e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_SKIP\n",
    "# 仅用于打印依赖版本，方便复现；不影响后续数据加载与嵌入示例\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed4b7db-3b47-4fd3-a4a6-5f4ed5dd166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 这个笔记本展示一个最小可运行的数据加载流程：\n",
    "# 1) 原始文本 -> token id 序列（整数列表）\n",
    "# 2) token id 序列 -> (input_ids, target_ids) 训练样本（滑动窗口）\n",
    "# 3) DataLoader 按 batch 产出张量，便于送入模型\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        # 保存每个样本的输入与目标：\n",
    "        # - input_ids: 长度为 max_length 的上下文 token id\n",
    "        # - target_ids: input_ids 向右平移 1 位（next-token prediction 的监督信号）\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # 把整段文本一次性编码成 token id 序列（Python list[int]）\n",
    "        # allowed_special 允许编码结果中包含特殊 token（例如 <|endoftext|>）\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # 用滑动窗口把 token_ids 切成许多训练样本\n",
    "        # - max_length: 每个样本的长度（上下文长度）\n",
    "        # - stride: 窗口每次向右移动多少个 token；stride < max_length 会产生重叠样本\n",
    "        # 例：token_ids=[t0,t1,t2,t3,t4,...], max_length=4\n",
    "        # - i=0: input=[t0,t1,t2,t3], target=[t1,t2,t3,t4]\n",
    "        # - i=1: input=[t1,t2,t3,t4], target=[t2,t3,t4,t5]\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "\n",
    "            # 把 list[int] 转成 tensor；DataLoader 会在 batch 维度上自动堆叠\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据集样本数 = 可以滑动出的窗口数\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回第 idx 个样本：(input_ids, target_ids)\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    # 创建 DataLoader 的便捷函数：把分词器、Dataset、DataLoader 封装起来\n",
    "    # - shuffle: 是否打乱样本顺序（训练通常 True；演示/复现可 False）\n",
    "    # - drop_last: 最后一个 batch 不满 batch_size 时是否丢弃\n",
    "    # - num_workers: 并行加载进程数（本章示例用 0 便于跨平台）\n",
    "\n",
    "    # 初始化 GPT-2 的 BPE 分词器（tiktoken 提供）\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 构造 Dataset：内部会把 raw text 切成一对对 (input_ids, target_ids)\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 构造 DataLoader：每次迭代返回一个 batch\n",
    "    # - x 的 shape: (batch_size, max_length)\n",
    "    # - y 的 shape: (batch_size, max_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# 读取示例文本（本章用它来构造训练样本）\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# GPT-2 (BPE) 词表大小；Embedding 的输出维度通常等于模型的隐藏维度\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "# 位置嵌入表能覆盖的最大位置数（绝对位置：0,1,2,...,context_length-1）\n",
    "context_length = 1024\n",
    "\n",
    "\n",
    "# token embedding: token id -> 向量，权重矩阵形状 (vocab_size, output_dim)\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# position embedding: position id -> 向量，权重矩阵形状 (context_length, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "batch_size = 8\n",
    "# 为了方便观察张量形状，这里用很短的 max_length=4（真实训练会更长）\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=max_length  # stride=max_length 表示相邻样本不重叠\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 DataLoader 取出一个 batch（这里只演示第 1 个 batch）\n",
    "for batch in dataloader:\n",
    "    # x: 输入 token id，y: 目标 token id（均为整型张量）\n",
    "    # x.shape == y.shape == (batch_size, max_length)\n",
    "    x, y = batch\n",
    "\n",
    "    # token_embedding_layer 会把每个 token id 查表变成 output_dim 维向量\n",
    "    # token_embeddings.shape: (batch_size, max_length, output_dim)\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "\n",
    "    # 绝对位置嵌入：先生成位置 id [0, 1, ..., max_length-1]\n",
    "    # torch.arange(max_length).shape: (max_length,)\n",
    "    # pos_embeddings.shape: (max_length, output_dim)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    # 相加时会发生广播：pos_embeddings 会被“复制”到每个 batch 上\n",
    "    # input_embeddings.shape: (batch_size, max_length, output_dim)\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3664332-e6bb-447e-8b96-203aafde8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# 打印最终输入嵌入的张量形状：应为 (batch_size, max_length, output_dim)\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e65c03-36d4-413f-9b23-5cdd816729ab",
   "metadata": {
    "id": "e2e65c03-36d4-413f-9b23-5cdd816729ab"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {
    "id": "6f678e62-7bcb-4405-86ae-dce94f494303"
   },
   "source": [
    "# 比较高效的多头注意力实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742938a-4bfc-4527-a1f1-d5963508967d",
   "metadata": {
    "id": "b742938a-4bfc-4527-a1f1-d5963508967d"
   },
   "source": [
    "本代码笔记本比较: 解码器风格大型语言模型（如GPT、Llama等）实现因果多头注意力的不同方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7898551e-f582-48ac-9f66-3632abe2a93f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7898551e-f582-48ac-9f66-3632abe2a93f",
    "outputId": "1a7d22c1-96d8-4a42-e3ec-ce78abaf18eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LYLcq3403Yq6",
   "metadata": {
    "id": "LYLcq3403Yq6"
   },
   "source": [
    "- 若要运行本笔记本中的所有代码，请确保将PyTorch更新到至少2.5版本（FlexAttention在早期版本的PyTorch中不包含）。\n",
    "- 如果上面的代码单元显示的PyTorch版本低于2.5，您可以通过取消注释并运行以下代码单元来升级PyTorch（请注意，PyTorch 2.5要求Python 3.9或更高版本）。\n",
    "- 有关更具体的说明和CUDA版本，请参考官方安装指南：https://pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db27f43-86f4-478f-89df-fbc2182a129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bb1b6-a1e5-4e0a-884d-0f31b374a8d6",
   "metadata": {
    "id": "2f9bb1b6-a1e5-4e0a-884d-0f31b374a8d6"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 1) 第3章中的CausalAttention多头注意力封装类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297c93ed-aec0-4896-bb89-42c4b294d3d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "297c93ed-aec0-4896-bb89-42c4b294d3d1",
    "outputId": "b6f596e4-b778-496c-bea8-3fe83d873c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # New\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))  # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class Ch03_MHA_Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)\n",
    "\n",
    "\n",
    "mha_ch03_wrapper = Ch03_MHA_Wrapper(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim//12,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03_wrapper(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21930804-b327-40b1-8e63-94dcad39ce7b",
   "metadata": {
    "id": "21930804-b327-40b1-8e63-94dcad39ce7b"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 2) 第三章的多头注意力类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6a61b-d25c-4a0c-8a59-f285544e3710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ee6a61b-d25c-4a0c-8a59-f285544e3710",
    "outputId": "4d9ade55-4710-4ae6-9f00-aa87811bfb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class Ch03_MHA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out 必须能够被 num_heads 整除\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # 将投影维度缩小，以匹配所需的输出维度\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # 线性层，用于结合每个头的输出\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # 形状: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 我们通过添加 `num_heads` 维度来隐式地拆分矩阵\n",
    "        # 展开最后一个维度: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 转置: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 计算缩放点积注意力（即自注意力），并应用因果掩码\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # 对每个头进行点积\n",
    "\n",
    "        # 原始的掩码被截断到令牌数，并转换为布尔值\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # 使用掩码填充注意力分数\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 形状: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # 合并头部输出，其中 self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # 可选的投影\n",
    "\n",
    "        return context_vec\n",
    "        \n",
    "mha_ch03= Ch03_MHA(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd11da-ea3b-4081-b483-c4965dfefbc4",
   "metadata": {
    "id": "73cd11da-ea3b-4081-b483-c4965dfefbc4"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 3) 权重的组合运用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1a5ea-eaff-4d2d-aaf0-b34cdb6fd4dd",
   "metadata": {
    "id": "1fa1a5ea-eaff-4d2d-aaf0-b34cdb6fd4dd"
   },
   "source": [
    "- 以下是 `MultiHeadAttentionCombinedQKV` 类的代码，基于 [Rayed Bin Wahed](https://github.com/rasbt/LLMs-from-scratch/discussions/51) 分享的代码。\n",
    "- `MultiHeadAttentionCombinedQKV` 类与第3章中使用的 `MultiHeadAttention` 类的主要区别在于，`MultiHeadAttentionCombinedQKV` 使用一个单独的权重矩阵，`self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)`，而不是分别使用多个权重矩阵：\n",
    "\n",
    "  - `self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)`\n",
    "  - `self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)`\n",
    "  - `self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)`\n",
    "\n",
    "- 在这里，`self.qkv` 将 `self.W_query`、`self.W_key` 和 `self.W_value` 三个权重矩阵结合起来，执行查询、键和值的计算一步完成。\n",
    "- 使用 `q, k, v = qkv.unbind(0)`，我们获取单独的查询、键和值张量，然后这些张量与第3章中的 `MultiHeadAttention` 类中的查询、键和值张量类似地使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6bd0a2-f27c-4602-afa0-c96cd295c1a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a6bd0a2-f27c-4602-afa0-c96cd295c1a6",
    "outputId": "a0a023ee-3bc7-4a89-cdba-7c97921160ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttentionCombinedQKV(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_head, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv.unbind(0)\n",
    "\n",
    "        # (b, num_heads, num_tokens, head_dim) --> (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**-0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens) --> (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        # (b, num_heads, num_tokens, head_dim) --> (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim) --> (b, num_tokens, embed_dim)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "mha_combined_qkv = MultiHeadAttentionCombinedQKV(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_combined_qkv(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14390d-3e21-43fd-87be-43e7029163ee",
   "metadata": {
    "id": "9b14390d-3e21-43fd-87be-43e7029163ee"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 4) 用了Einsum的多头注意力\n",
    "\n",
    "- 使用爱因斯坦求和法通过 [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) 实现多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92481814-068d-439b-a65c-b1310ebbe0aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92481814-068d-439b-a65c-b1310ebbe0aa",
    "outputId": "59a75f6e-ef06-418f-8e54-d3b368fbed13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class MHAEinsum(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Initialize parameters for Q, K, V\n",
    "        self.W_query = nn.Parameter(torch.randn(d_out, d_in))\n",
    "        self.W_key = nn.Parameter(torch.randn(d_out, d_in))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_out, d_in))\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.bias_q = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_k = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_v = nn.Parameter(torch.zeros(d_out))\n",
    "        else:\n",
    "            self.register_parameter(\"bias_q\", None)\n",
    "            self.register_parameter(\"bias_k\", None)\n",
    "            self.register_parameter(\"bias_v\", None)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_query, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_key, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_value, a=math.sqrt(5))\n",
    "        if self.bias_q is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_query)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_q, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_k, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_v, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Calculate Q, K, V using einsum, first perform linear transformations\n",
    "        Q = torch.einsum(\"bnd,di->bni\", x, self.W_query)\n",
    "        K = torch.einsum(\"bnd,di->bni\", x, self.W_key)\n",
    "        V = torch.einsum(\"bnd,di->bni\", x, self.W_value)\n",
    "\n",
    "        # Add biases if they are used\n",
    "        if self.bias_q is not None:\n",
    "            Q += self.bias_q\n",
    "            K += self.bias_k\n",
    "            V += self.bias_v\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.einsum(\"bhnd,bhmd->bhnm\", Q, K) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask\n",
    "        mask = self.mask[:n, :n].unsqueeze(0).unsqueeze(1).expand(b, self.num_heads, n, n)\n",
    "        scores = scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Aggregate the attended context vectors\n",
    "        context_vec = torch.einsum(\"bhnm,bhmd->bhnd\", attn_weights, V)\n",
    "\n",
    "        # Combine heads and project the output\n",
    "        context_vec = context_vec.transpose(1, 2).reshape(b, n, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "mha_einsum = MHAEinsum(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_einsum(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a042d3-ee78-4c29-bf63-d92fe6706632",
   "metadata": {
    "id": "48a042d3-ee78-4c29-bf63-d92fe6706632"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 5) 使用PyTorch的缩放点积注意力和FlashAttention实现多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e346f-3b85-44e6-9feb-f01131381148",
   "metadata": {
    "id": "f78e346f-3b85-44e6-9feb-f01131381148"
   },
   "source": [
    "- 以下实现使用了PyTorch的 [`scaled_dot_product_attention`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) 函数，该函数实现了一种内存优化版的自注意力机制，称为 [FlashAttention](https://arxiv.org/abs/2205.14135)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8e5a0d-1f65-4a03-bf6e-723f0cc428f5",
   "metadata": {
    "id": "1b8e5a0d-1f65-4a03-bf6e-723f0cc428f5"
   },
   "outputs": [],
   "source": [
    "class MHAPyTorchScaledDotProduct(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            queries, keys, values, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc8ba92-3471-41cb-b1b2-4c0ef5be392b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbc8ba92-3471-41cb-b1b2-4c0ef5be392b",
    "outputId": "087a53e7-86d8-48dc-bf2e-023f0f2104cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_scaled = MHAPyTorchScaledDotProduct(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_scaled(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51492724-6018-49f6-8bf6-ae9e585229c3",
   "metadata": {
    "id": "51492724-6018-49f6-8bf6-ae9e585229c3"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 6) 没有FlashAttention的PyTorch缩放点积注意力\n",
    "\n",
    "- 这与上面类似，不同之处在于我们通过传递显式的因果掩码来禁用FlashAttention。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad53538-e905-4065-ba0c-caacdfec5a0b",
   "metadata": {
    "id": "bad53538-e905-4065-ba0c-caacdfec5a0b"
   },
   "outputs": [],
   "source": [
    "class MHAPyTorchSDPAWithoutFlash(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        # Ensure attn_mask is compatible with expected shape and `batch_first=True`\n",
    "        # No need to manually adjust for num_heads; ensure it's right for the sequence\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.mask[:num_tokens, :num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            queries, keys, values, attn_mask=attn_mask, dropout_p=use_dropout, is_causal=False)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3da7850-e772-47d3-bd51-22d077b01412",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3da7850-e772-47d3-bd51-22d077b01412",
    "outputId": "cc8fc837-8e06-42fc-bad5-b17816f47fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_sdpa_no_flash = MHAPyTorchSDPAWithoutFlash(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_sdpa_no_flash(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c318f-4835-4d74-8d58-a070222447c4",
   "metadata": {
    "id": "351c318f-4835-4d74-8d58-a070222447c4"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 7) 使用PyTorch的torch.nn.MultiheadAttention\n",
    "\n",
    "- 以下是使用PyTorch的 [torch.nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) 实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3799c7ef-3155-42c6-a829-f95656453ae0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3799c7ef-3155-42c6-a829-f95656453ae0",
    "outputId": "78236eea-a0f4-47e4-c846-606e7f8f8768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MHAPyTorchClass(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False, need_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_out,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=qkv_bias,\n",
    "            add_bias_kv=qkv_bias,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.need_weights = need_weights\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, _ = x.shape\n",
    "\n",
    "        # Ensure attn_mask is compatible with expected shape and `batch_first=True`\n",
    "        # No need to manually adjust for num_heads; ensure it's right for the sequence\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.mask[:num_tokens, :num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        # attn_mask broadcasting will handle batch_size dimension implicitly\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            x, x, x, attn_mask=attn_mask, need_weights=self.need_weights\n",
    "        )\n",
    "\n",
    "        output = self.proj(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "mha_pytorch_class_default = MHAPyTorchClass(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_class_default(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3953bff-1056-4de2-bfd1-dfccf659eee4",
   "metadata": {
    "id": "a3953bff-1056-4de2-bfd1-dfccf659eee4"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 8) 使用PyTorch的torch.nn.MultiheadAttention与 `scaled_dot_product_attention`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2164859-31a0-4537-b4fb-27d57675ba77",
   "metadata": {
    "id": "d2164859-31a0-4537-b4fb-27d57675ba77"
   },
   "source": [
    "- 将 `need_weights`（默认值为 `True`）设置为 `False`，这样 `MultiheadAttention` 就会使用 `scaled_dot_product_attention` [根据文档](https://github.com/pytorch/pytorch/blob/71d020262793542974cf13b30f2a9099773f015c/torch/nn/modules/activation.py#L1096)。\n",
    "\n",
    "```markdown\n",
    "need_weights: If specified, returns `attn_output_weights` in addition to `attn_outputs`.\n",
    "           Set `need_weights=False` to use the optimized `scaled_dot_product_attention`\n",
    "           and achieve the best performance for MHA.\n",
    "           Default: `True`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a4c2afe-5e1f-4bd7-a118-67031176f147",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a4c2afe-5e1f-4bd7-a118-67031176f147",
    "outputId": "6359dcff-ddcf-4cf9-eada-c3f0685cced2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_class_noweights = MHAPyTorchClass(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False,\n",
    "    need_weights=False # NEW!\n",
    ").to(device)\n",
    "\n",
    "out = mha_pytorch_class_noweights(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4ff35-651c-4e47-bfa1-016f3de01ecc",
   "metadata": {
    "id": "21f4ff35-651c-4e47-bfa1-016f3de01ecc"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 9) Using PyTorch's FlexAttention\n",
    "\n",
    "- See [FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention](https://pytorch.org/blog/flexattention/) to learn more about FlexAttention\n",
    "- This is supported starting from PyTorch 2.5, which you can install on a CPU machine via\n",
    "\n",
    "    ```bash\n",
    "    pip install torch torchvision torchaudio\n",
    "    ```\n",
    "\n",
    "- To install PyTorch on a GPU machine, use the following (for more information, also see the installation menu on [pytorch.org](https://pytorch.org/))\n",
    "\n",
    "    ```bash\n",
    "    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834318c8-4748-4902-99f0-70ee02bef63e",
   "metadata": {
    "id": "834318c8-4748-4902-99f0-70ee02bef63e"
   },
   "outputs": [],
   "source": [
    "from packaging.version import parse as parse_version\n",
    "\n",
    "def normalize_version(version):\n",
    "    parsed_version = parse_version(version)\n",
    "    return parse_version(f\"{parsed_version.major}.{parsed_version.minor}.{parsed_version.micro}\")\n",
    "\n",
    "current_version = normalize_version(torch.__version__)\n",
    "MIN_TORCH_VERSION = \"2.5.0\"\n",
    "required_version = parse_version(MIN_TORCH_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WYyFRCXndVH9",
   "metadata": {
    "id": "WYyFRCXndVH9"
   },
   "outputs": [],
   "source": [
    "if current_version >= required_version:\n",
    "    from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "\n",
    "class MHAPyTorchFlexAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout\n",
    "        # self.register_buffer(\"block_mask\", create_block_mask(causal, B=None, H=None, Q_LEN=context_length, KV_LEN=context_length))\n",
    "        # `create_block_mask` function does not support buffers, yet\n",
    "        self.block_mask = create_block_mask(causal, B=None, H=None, Q_LEN=context_length, KV_LEN=context_length)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        # Ensure attn_mask is compatible with expected shape and `batch_first=True`\n",
    "        # No need to manually adjust for num_heads; ensure it's right for the sequence\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.block_mask[:num_tokens, :num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.block_mask[:self.context_length, :self.context_length]\n",
    "\n",
    "        context_vec = flex_attention(queries, keys, values, block_mask=attn_mask)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cdaaf8a-f956-44bc-932f-4d33448e8aaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cdaaf8a-f956-44bc-932f-4d33448e8aaf",
    "outputId": "a88a7398-159e-401f-d96c-2fc928908e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "if current_version >= required_version and torch.cuda.is_available():\n",
    "\n",
    "    mha_pytorch_flex = MHAPyTorchFlexAttention(\n",
    "        d_in=embed_dim,\n",
    "        d_out=embed_dim,\n",
    "        context_length=context_len,\n",
    "        dropout=0.0,\n",
    "        num_heads=12,\n",
    "        qkv_bias=False\n",
    "    ).to(device)\n",
    "\n",
    "    out = mha_pytorch_flex(embeddings)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877de71-f84f-4f6d-bc87-7552013b6301",
   "metadata": {
    "id": "8877de71-f84f-4f6d-bc87-7552013b6301"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## Quick speed comparison (M3 Macbook Air CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cf93a-078f-434d-888c-2458d0731285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "219cf93a-078f-434d-888c-2458d0731285",
    "outputId": "a10b52d4-b4e6-43c2-9677-113c41edd3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0\n",
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c0b2e-6593-49d8-98bc-2267b3aa610f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a97c0b2e-6593-49d8-98bc-2267b3aa610f",
    "outputId": "7bcd7da4-d115-4ba6-efba-377a0bd7d3a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 ms ± 7.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "## 1) CausalAttention MHA wrapper class from chapter 3\n",
    "%timeit mha_ch03_wrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db9c2c-8e75-431a-8eef-0b4d8284e6e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19db9c2c-8e75-431a-8eef-0b4d8284e6e6",
    "outputId": "b04b4d0d-71aa-4944-f02b-131bf5a50202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 ms ± 2.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 2) The multi-head attention class from chapter 3\n",
    "%timeit mha_ch03(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa526ee0-7a88-4f34-a49a-f8f97da83779",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa526ee0-7a88-4f34-a49a-f8f97da83779",
    "outputId": "5436928a-7b98-4c40-bf51-97973f13327e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 2.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 3) An alternative multi-head attention with combined weights\n",
    "%timeit mha_combined_qkv(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ca826-35bf-47e5-b497-540aba439ef9",
   "metadata": {
    "id": "131ca826-35bf-47e5-b497-540aba439ef9",
    "outputId": "f5848852-f81b-4e5f-a7ff-e37a8445ad91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 4) Multi-head attention using Einstein summation\n",
    "%timeit mha_einsum(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b4256-16d8-4c34-9fd0-d4b4af0e60fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc2b4256-16d8-4c34-9fd0-d4b4af0e60fa",
    "outputId": "9e07ce73-a2de-4e2c-8276-64626df9450e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 ms ± 423 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 5) Multi-head attention with PyTorch's scaled dot product attention\n",
    "%timeit mha_pytorch_scaled(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44305ce-9f61-451a-b9ef-30caba222357",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c44305ce-9f61-451a-b9ef-30caba222357",
    "outputId": "6bab4a24-5bb4-4ad6-b260-3b442f598950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.5 ms ± 790 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 6) PyTorch's scaled dot product attention without FlashAttention\n",
    "%timeit mha_pytorch_sdpa_no_flash(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f209e70-ebb6-4a1a-b608-1ff42e41c01d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f209e70-ebb6-4a1a-b608-1ff42e41c01d",
    "outputId": "630c49d1-8a06-4148-cd96-a7b2467310a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 ms ± 3.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 7) Using PyTorch's torch.nn.MultiheadAttention\n",
    "%timeit mha_pytorch_class_default(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4968c2-8d40-4ab9-8dba-052b4f77d756",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f4968c2-8d40-4ab9-8dba-052b4f77d756",
    "outputId": "10f6a268-f9cf-446c-aa83-e87b6a0b4f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 ms ± 2.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 8) Using PyTorch's torch.nn.MultiheadAttention disabling `need_weights`\n",
    "%timeit mha_pytorch_class_noweights(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8e0fc-ef24-424c-bccf-c381e73da228",
   "metadata": {
    "id": "bdd8e0fc-ef24-424c-bccf-c381e73da228"
   },
   "outputs": [],
   "source": [
    "## 9) Using PyTorch's FlexAttention\n",
    "\n",
    "# Requires PyTorch 2.5.0 or newer and currently only supports CUDA PyTorch\n",
    "%timeit mha_pytorch_flex(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ff594-6cc2-496d-a302-789fa104c3c9",
   "metadata": {
    "id": "a78ff594-6cc2-496d-a302-789fa104c3c9"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## Quick speed comparison (Nvidia A100 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "RStnI1pEi6Eo",
   "metadata": {
    "id": "RStnI1pEi6Eo"
   },
   "outputs": [],
   "source": [
    "# Enable tensor cores\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8431d75-e1c9-4d9a-b7da-9a1ff391f2bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8431d75-e1c9-4d9a-b7da-9a1ff391f2bf",
    "outputId": "f6356d4c-7a3f-47f5-cf51-5507db3f5748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0.dev20240905+cu121\n",
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707a2a14-a089-48a8-88aa-d328e1e0a9d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "707a2a14-a089-48a8-88aa-d328e1e0a9d0",
    "outputId": "4ea5798b-a590-401b-d049-3fed0716db34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33 ms ± 51.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 1) CausalAttention MHA wrapper class from chapter 3\n",
    "%timeit mha_ch03_wrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8686dd69-3655-40e4-a57b-a2c55532a010",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8686dd69-3655-40e4-a57b-a2c55532a010",
    "outputId": "88094b61-4d87-47bd-8c8b-c9344ab57062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.09 ms ± 363 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 2) The multi-head attention class from chapter 3\n",
    "%timeit mha_ch03(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2209d7df-e54b-4910-ae2b-c78cf684d9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2209d7df-e54b-4910-ae2b-c78cf684d9bf",
    "outputId": "e3d82c53-f75b-425a-ed3e-5e48ea9ef768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.81 ms ± 656 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 3) An alternative multi-head attention with combined weights\n",
    "%timeit mha_combined_qkv(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abee5edf-2585-4f0e-846c-b1c7ca88f545",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abee5edf-2585-4f0e-846c-b1c7ca88f545",
    "outputId": "c9bf17f5-de62-4c39-a328-fe430812b156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12 ms ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 4) Multi-head attention using Einstein summation\n",
    "%timeit mha_einsum(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1075abe2-4839-4fd6-af3e-c09bb3651e26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1075abe2-4839-4fd6-af3e-c09bb3651e26",
    "outputId": "b63f4769-3be5-44df-b8f2-2ac379be1ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25 ms ± 1.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 5) Multi-head attention with PyTorch's scaled dot product attention\n",
    "%timeit mha_pytorch_scaled(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "218adbaf-f17f-47d9-81d5-41c758218df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218adbaf-f17f-47d9-81d5-41c758218df7",
    "outputId": "a30ab365-865d-4175-f148-dc15abc4e07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.03 ms ± 119 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 6) PyTorch's scaled dot product attention without FlashAttention\n",
    "%timeit mha_pytorch_sdpa_no_flash(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868e3670-8edc-47bc-9e06-eb505e44dc9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "868e3670-8edc-47bc-9e06-eb505e44dc9d",
    "outputId": "e20e77ac-6573-4830-82c7-795bd139af4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 ms ± 388 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 7) Using PyTorch's torch.nn.MultiheadAttention\n",
    "%timeit mha_pytorch_class_default(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "944870e6-de54-4e3b-a455-b8f21f6f92c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "944870e6-de54-4e3b-a455-b8f21f6f92c8",
    "outputId": "26df6295-fa5c-4b3f-89be-c7183f079fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 ms ± 6.53 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "## 8) Using PyTorch's torch.nn.MultiheadAttention disabling `need_weights`\n",
    "%timeit mha_pytorch_class_noweights(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "evKtpb5QN_2A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evKtpb5QN_2A",
    "outputId": "23bf5398-c8ec-4463-8af9-17de8f920a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "## 9) Using PyTorch's FlexAttention\n",
    "\n",
    "# Requires PyTorch 2.5.0 or newer\n",
    "%timeit mha_pytorch_flex(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc6575-0316-4640-a729-e616d5c17b73",
   "metadata": {
    "id": "dabc6575-0316-4640-a729-e616d5c17b73"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbb2f729-d3d8-46d0-b249-9249197ea574",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbb2f729-d3d8-46d0-b249-9249197ea574",
    "outputId": "a45fe256-6416-4f43-87d2-27bbf97239e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0.dev20240905+cu121\n",
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0620bf5",
   "metadata": {
    "id": "b0620bf5"
   },
   "outputs": [],
   "source": [
    "functions = {\n",
    "    \"1) MHA wrapper class\": mha_ch03_wrapper,\n",
    "    \"2) MHA Ch03\": mha_ch03,\n",
    "    \"3) MHA with combined QKV weights\": mha_combined_qkv,\n",
    "    \"4) MHA with Einsum\": mha_einsum,\n",
    "    \"5) MHA with PyTorch scaled_dot_product_attention\": mha_pytorch_scaled,\n",
    "    \"6) PyTorch's SDPA, no FlashAttention\": mha_pytorch_sdpa_no_flash,\n",
    "    \"7) PyTorch MHA class defaults\": mha_pytorch_class_default,\n",
    "    \"8) PyTorch MHA with need_weights=False\": mha_pytorch_class_noweights\n",
    "    }\n",
    "\n",
    "if current_version >= required_version:\n",
    "    functions[\"8) PyTorch's FlexAttention\"] =  mha_pytorch_flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "CDJAPZaszaqx",
   "metadata": {
    "id": "CDJAPZaszaqx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Customize further for dark mode aesthetics\n",
    "plt.rcParams[\"figure.facecolor\"] = \"#121212\"\n",
    "plt.rcParams[\"axes.facecolor\"] = \"#121212\"\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"white\"\n",
    "plt.rcParams[\"axes.labelcolor\"] = \"white\"\n",
    "plt.rcParams[\"text.color\"] = \"white\"\n",
    "plt.rcParams[\"xtick.color\"] = \"white\"\n",
    "plt.rcParams[\"ytick.color\"] = \"white\"\n",
    "plt.rcParams[\"grid.color\"] = \"#444444\"\n",
    "plt.rcParams[\"lines.linewidth\"] = 2\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "\n",
    "def plot_execution_times(functions, execution_means, execution_stds, filename):\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.bar(functions.keys(), execution_means, yerr=execution_stds, capsize=5, error_kw={'ecolor': 'grey'})\n",
    "\n",
    "    plt.ylabel(\"Execution time (ms)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Calculate new ylim with a margin\n",
    "    max_execution_time = max(execution_means)\n",
    "    upper_ylim = max_execution_time + 0.4 * max_execution_time  # Adding a 40% margin\n",
    "    plt.ylim(0, upper_ylim)\n",
    "\n",
    "    # Annotate bars with execution times\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + (0.05 * upper_ylim), round(yval, 2), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df834dc",
   "metadata": {
    "id": "4df834dc"
   },
   "source": [
    "## Speed comparison (Nvidia A100 GPU) with warmup (forward pass only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29b63d3d-6d0b-43bb-9c68-d5514dc81000",
   "metadata": {
    "id": "29b63d3d-6d0b-43bb-9c68-d5514dc81000"
   },
   "outputs": [],
   "source": [
    "# CUDA benchmark code shared by Andrei Aksionov\n",
    "# and based on code from\n",
    "# https://github.com/cuda-mode/lectures/blob/main/lecture1/pytorch_square.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def time_pytorch_function(func, *input, num_repeats=1_000):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(*input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(num_repeats):\n",
    "        start.record()\n",
    "        func(*input)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dd07a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "9dd07a09",
    "outputId": "491d06f4-a6bc-431a-a1ca-4db38df57e0c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_stats = [time_pytorch_function(fn, embeddings) for fn in functions.values()]\n",
    "execution_means = [stat[0] for stat in execution_stats]\n",
    "execution_stds = [stat[1] for stat in execution_stats]\n",
    "\n",
    "\n",
    "plot_execution_times(functions, execution_means, execution_stds, filename=\"1_forward-only.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQaSerWCOnYB",
   "metadata": {
    "id": "VQaSerWCOnYB"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "## Speed comparison (Nvidia A100 GPU) with warmup (forward and backward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69e6377b",
   "metadata": {
    "id": "69e6377b"
   },
   "outputs": [],
   "source": [
    "def forward_backward(func, embeddings):\n",
    "    if embeddings.grad is not None:\n",
    "        embeddings.grad.zero_()\n",
    "\n",
    "    output = func(embeddings)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def time_pytorch_function_forward_backward(func, *input, num_repeats = 1_000):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        forward_backward(func, *input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(num_repeats):\n",
    "        start.record()\n",
    "        forward_backward(func, *input)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ReCmeRhCOpm8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "ReCmeRhCOpm8",
    "outputId": "2bcfa909-ba87-4d31-b926-bc66e63736cc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_stats = [time_pytorch_function_forward_backward(fn, embeddings) for fn in functions.values()]\n",
    "execution_means = [stat[0] for stat in execution_stats]\n",
    "execution_stds = [stat[1] for stat in execution_stats]\n",
    "\n",
    "\n",
    "plot_execution_times(functions, execution_means, execution_stds, filename=\"2_forward-and-backward.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1gWX-Ayqia1k",
   "metadata": {
    "id": "1gWX-Ayqia1k"
   },
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "## Speed comparison (Nvidia A100 GPU) with warmup and compilation (forward and backward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "LQDiAPooiYAz",
   "metadata": {
    "id": "LQDiAPooiYAz"
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "def prepare_function(fn):\n",
    "    fn = torch.compile(fn)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aac06ffe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "aac06ffe",
    "outputId": "098c66b4-1201-4bdd-af23-e634f5ade806"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_stats = [time_pytorch_function_forward_backward(prepare_function(fn), embeddings) for fn in functions.values()]\n",
    "execution_means = [stat[0] for stat in execution_stats]\n",
    "execution_stds = [stat[1] for stat in execution_stats]\n",
    "\n",
    "\n",
    "plot_execution_times(functions, execution_means, execution_stds, filename=\"3_forward-and-backward-compiled.pdf\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch04/02_performance-analysis/flops-analysis.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtQYMbLvgzO-"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbrESHKtgzPA"
   },
   "source": [
    "# FLOPS分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS2WjniMgzPB"
   },
   "source": [
    "- FLOPs（每秒浮点运算数）衡量神经网络的计算复杂度。\n",
    "- 高FLOPs计算更加复杂，能耗更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L01-NzkggzPB"
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObzfVatqgzPC",
    "outputId": "3ead6a41-ac38-4db1-9fc3-012fb3ad18cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"thop\",\n",
    "    \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74UpjSLjgzPC"
   },
   "source": [
    "&nbsp;\n",
    "# 固定批次大小的基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90pnCK39gzPD"
   },
   "source": [
    "- 仅有前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GerIdRMXd6g9",
    "outputId": "177c6d00-a817-40fe-badd-95cfa8ac9b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M)  : 5.1e+11 FLOPS\n",
      "gpt-medium (355M) : 1.4e+12 FLOPS\n",
      "gpt-large (774M)  : 3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    : 6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "# 基本配置\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # 丢弃率\n",
    "    \"qkv_bias\": True         # 是否使用查询-键-值偏置\n",
    "}\n",
    "\n",
    "# 不同规模的GPT模型配置\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},  # 小型模型\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16}, # 中型模型\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},  # 大型模型\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},    # 超大模型\n",
    "}\n",
    "\n",
    "# 设置设备（优先使用GPU，如果没有则使用CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 批次大小\n",
    "batch_size = 2\n",
    "# 输入张量：一个大小为(批次大小, 上下文长度)的随机整数张量\n",
    "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
    "\n",
    "# 对每个模型配置进行遍历\n",
    "for size in model_configs:\n",
    "    # 更新基础配置\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "\n",
    "    # 创建GPT模型并转换为bfloat16类型\n",
    "    model = GPTModel(BASE_CONFIG).bfloat16()\n",
    "    model.to(device)\n",
    "\n",
    "    # MACS（乘加操作）= 浮动计算操作的一种\n",
    "    # MACS通常被认为是两个FLOPS（一个乘法和一个加法）\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    flops = 2*macs  # 计算FLOPS：每个MACS算作2个FLOPS\n",
    "    print(f\"{size:18}: {flops:.1e} FLOPS\")  # 输出FLOPS\n",
    "\n",
    "    # 清除模型并释放GPU缓存\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S6V05QmgzPD"
   },
   "source": [
    "&nbsp;\n",
    "# 固定批次大小的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amw4E983gzPD"
   },
   "source": [
    "- 仅有前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h08VOiqpgzPE",
    "outputId": "a6a90ef8-28fb-4b55-9268-6915b0c84c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing gpt-small (124M)\n",
      "  Batch size 256: 6.5e+13 FLOPS\n",
      "  Batch size 384: 9.7e+13 FLOPS\n",
      "  Batch size 388: 9.8e+13 FLOPS\n",
      "  Batch size 389: 9.8e+13 FLOPS\n",
      "\n",
      "Processing gpt-medium (355M)\n",
      "  Batch size 256: 1.9e+14 FLOPS\n",
      "  Batch size 260: 1.9e+14 FLOPS\n",
      "  Batch size 262: 1.9e+14 FLOPS\n",
      "  Batch size 263: 1.9e+14 FLOPS\n",
      "\n",
      "Processing gpt-large (774M)\n",
      "  Batch size 256: 4.0e+14 FLOPS\n",
      "\n",
      "Processing gpt-xl (1558M)\n",
      "  Batch size 128: 4.1e+14 FLOPS\n",
      "  Batch size 136: 4.3e+14 FLOPS\n",
      "  Batch size 140: 4.5e+14 FLOPS\n",
      "  Batch size 142: 4.5e+14 FLOPS\n",
      "  Batch size 143: 4.6e+14 FLOPS\n"
     ]
    }
   ],
   "source": [
    "for size in model_configs:\n",
    "    print(f\"\\n正在处理 {size}\")\n",
    "    config = BASE_CONFIG.copy()  # 复制基础配置\n",
    "    config.update(model_configs[size])  # 更新配置为对应模型的配置\n",
    "\n",
    "    min_batch_size = 1  # 最小批次大小\n",
    "    max_batch_size = None  # 最大批次大小（初始为空）\n",
    "    max_possible_batch_size = 4096  # 设定最大可能的批次大小\n",
    "\n",
    "    # 通过二分法探索适合的批次大小\n",
    "    while min_batch_size <= max_possible_batch_size:\n",
    "        batch_size = (min_batch_size + max_possible_batch_size) // 2  # 计算当前批次大小\n",
    "        try:\n",
    "            # 创建输入张量，大小为（批次大小, 上下文长度）\n",
    "            input_tensor = torch.randint(\n",
    "                0, config[\"vocab_size\"],\n",
    "                (batch_size, config[\"context_length\"]),\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # 创建GPT模型并转换为bfloat16类型\n",
    "            model = GPTModel(config).bfloat16().to(device)\n",
    "\n",
    "            # MACS = 乘加操作（Multiply-Accumulate operations）\n",
    "            # MACS通常计为两个FLOPS（一个乘法操作和一个加法操作）\n",
    "            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "            flops = 2 * macs  # 计算FLOPS：每个MACS算作2个FLOPS\n",
    "            print(f\"  批次大小 {batch_size}: {flops:.1e} FLOPS\")\n",
    "\n",
    "            # 如果成功，则尝试更大的批次大小\n",
    "            min_batch_size = batch_size + 1\n",
    "            max_batch_size = batch_size\n",
    "\n",
    "            # 清理模型和输入张量\n",
    "            del model, input_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                # 如果内存溢出，尝试更小的批次大小\n",
    "                max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                # 清理模型和输入张量\n",
    "                try:\n",
    "                    del model, input_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "                except NameError:\n",
    "                    pass\n",
    "            else:\n",
    "                raise e  # 其他错误，重新抛出异常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4lD7tfcgzPE"
   },
   "source": [
    "&nbsp;\n",
    "# 自动批量大小调整与模型FLOP利用率（MFU）基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Y2mblVgzPE"
   },
   "source": [
    "•\t**模型FLOP利用率（MFU）**的解释来源于PaLM论文\n",
    "\n",
    "> 我们提出了一种新的效率度量标准，它与实现方式无关，并允许更清晰地比较系统效率，称为模型FLOP利用率（MFU）。这是观察到的吞吐量（每秒处理的tokens）与理论最大吞吐量（系统在峰值FLOP时的处理能力）的比率。重要的是，“理论最大”吞吐量只考虑了计算前向和反向传递所需的操作，而不包括重计算操作。\n",
    "\n",
    "$$\\text{MFU} = \\frac{\\text{Observed Tokens per Second}}{\\text{Theoretical Max Tokens per Second}}$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\\text{Theoretical Max Tokens per Second} = \\frac{\\text{Max FLOPs per Second}}{\\text{Total FLOPs per Token}}$$\n",
    "\n",
    "并且\n",
    "\n",
    "$$\\text{Tokens per Second} = \\frac{\\text{Batch Size} \\times \\text{Sequence Length}}{\\text{Total Time}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKttjC8xgzPF"
   },
   "source": [
    "- 前向传播与反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aO4rjtNgzPF"
   },
   "outputs": [],
   "source": [
    "flops_per_second = {\n",
    "    # https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899\n",
    "    \"H100\": {\n",
    "        torch.float32: 51.22e12,  # NVIDIA H100在FP32模式下的51.22 TFLOPs\n",
    "        torch.float16: 204.9e12,  # NVIDIA H100在FP16模式下的204.9 TFLOPs\n",
    "        torch.bfloat16: 204.9e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/l4.c4091\n",
    "    \"L4\": {\n",
    "        torch.float32: 30.29e12,  # NVIDIA L4在FP32模式下的30.29 TFLOPs\n",
    "        torch.float16: 30.29e12,  # NVIDIA L4在FP16模式下的30.29 TFLOPs\n",
    "        torch.bfloat16: 30.29e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/tesla-t4.c3316\n",
    "    \"T4\": {\n",
    "        torch.float32: 8.1e12,  # NVIDIA T4在FP32模式下的8.1 TFLOPs\n",
    "        torch.float16: 65.13e12,  # NVIDIA T4在FP16模式下的65.13 TFLOPs\n",
    "        torch.bfloat16: 65.13e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a10g.c3798\n",
    "    \"A10G\": {\n",
    "        torch.float32: 31.52e12,  # NVIDIA A10G在FP32模式下的31.52 TFLOPs\n",
    "        torch.float16: 31.52e12,  # NVIDIA A10G在FP16模式下的31.52 TFLOPs\n",
    "        torch.bfloat16: 31.52e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623\n",
    "    \"A100\": {\n",
    "        torch.float32: 19.49e12,  # NVIDIA A100在FP32模式下的19.49 TFLOPs\n",
    "        torch.float16: 77.97e12,  # NVIDIA A100在FP16模式下的77.97 TFLOPs\n",
    "        torch.bfloat16: 77.97e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621\n",
    "    \"RTX_3080\": {\n",
    "        torch.float32: 29.77e12,  # NVIDIA RTX 3080在FP32模式下的29.77 TFLOPs\n",
    "        torch.float16: 29.77e12,  # NVIDIA RTX 3080在FP16模式下的29.77 TFLOPs\n",
    "        torch.bfloat16: 29.77e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n",
    "    \"RTX_3090\": {\n",
    "        torch.float32: 35.58e12,  # NVIDIA RTX 3090在FP32模式下的35.58 TFLOPs\n",
    "        torch.float16: 35.58e12,  # NVIDIA RTX 3090在FP16模式下的35.58 TFLOPs\n",
    "        torch.bfloat16: 35.58e12\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW5qWfE7gzPF",
    "outputId": "bb1663bc-ee66-44f1-f54d-0bb66ee0d0c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Model: A100\n",
      "\n",
      "Processing gpt-small (124M)\n",
      "  Batch size 16: Tokens/sec: 34248.82, MFU: 0.3256\n",
      "  Batch size 24: Tokens/sec: 62568.34, MFU: 0.5948\n",
      "\n",
      "Processing gpt-medium (355M)\n",
      "  Batch size 4: Tokens/sec: 20159.93, MFU: 0.5483\n",
      "  Batch size 6: Tokens/sec: 21717.66, MFU: 0.5907\n",
      "  Batch size 7: Tokens/sec: 22536.25, MFU: 0.6130\n",
      "\n",
      "Processing gpt-large (774M)\n",
      "  Batch size 8: Tokens/sec: 12465.21, MFU: 0.7406\n",
      "\n",
      "Processing gpt-xl (1558M)\n",
      "  Batch size 4: Tokens/sec: 6779.92, MFU: 0.8113\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 获取当前使用的GPU型号\n",
    "def get_gpu_model(flops_per_second_dict):\n",
    "    device_name = torch.cuda.get_device_name(0)  # 获取GPU设备的名称\n",
    "    for model in flops_per_second_dict.keys():  # 遍历flops_per_second字典中的GPU型号\n",
    "        if model in device_name:  # 如果当前设备名称包含字典中的某个GPU型号\n",
    "            return model  # 返回匹配的GPU型号\n",
    "    return \"Unknown\"  # 如果没有匹配的型号，则返回\"Unknown\"\n",
    "\n",
    "# 获取当前GPU型号\n",
    "gpu_model = get_gpu_model(flops_per_second)\n",
    "print(\"GPU Model:\", gpu_model)  # 输出GPU型号\n",
    "\n",
    "# 如果成功获取到GPU型号，则继续执行基准测试\n",
    "if gpu_model != \"Unknown\":\n",
    "\n",
    "    # 遍历不同的GPT模型配置\n",
    "    for size in model_configs:\n",
    "        print(f\"\\nProcessing {size}\")  # 打印当前正在处理的模型大小\n",
    "        config = BASE_CONFIG.copy()  # 复制基础配置\n",
    "        config.update(model_configs[size])  # 更新配置为当前模型配置\n",
    "\n",
    "        # 初始化最小批次大小，最大批次大小，和最大可能批次大小\n",
    "        min_batch_size = 1\n",
    "        max_batch_size = None\n",
    "        max_possible_batch_size = 4096  # 最大可能批次大小设为4096\n",
    "\n",
    "        # 进行批次大小的二分查找\n",
    "        while min_batch_size <= max_possible_batch_size:\n",
    "            batch_size = (min_batch_size + max_possible_batch_size) // 2  # 计算当前批次大小\n",
    "\n",
    "            try:\n",
    "                # 生成随机的输入数据，大小为(batch_size, context_length)\n",
    "                input_tensor = torch.randint(\n",
    "                    0, config[\"vocab_size\"],\n",
    "                    (batch_size, config[\"context_length\"]),\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                # 初始化模型，使用bfloat16精度，并将模型加载到GPU上\n",
    "                model = GPTModel(config).bfloat16().to(device)\n",
    "                model.train()  # 设置模型为训练模式\n",
    "\n",
    "                # 记录开始时间\n",
    "                torch.cuda.synchronize()  # 确保所有CUDA操作已完成\n",
    "                start_time = time.time()\n",
    "\n",
    "                # 前向传播和反向传播\n",
    "                output = model(input_tensor)  # 执行前向传播\n",
    "                loss = output.sum()  # 计算损失（使用dummy loss）\n",
    "                loss.backward()  # 执行反向传播\n",
    "\n",
    "                # 记录结束时间\n",
    "                torch.cuda.synchronize()  # 确保所有CUDA操作已完成\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time_seconds = end_time - start_time  # 计算总用时\n",
    "\n",
    "                # 计算前向传播的FLOPs\n",
    "                macs, params = profile(model, inputs=(input_tensor,), verbose=False)  # 计算乘加操作次数\n",
    "                flops_forward = 2 * macs  # 假设一个MAC操作等于两个FLOP\n",
    "\n",
    "                # 估算反向传播的FLOPs，通常是前向传播的两倍\n",
    "                flops_backward = 2 * flops_forward\n",
    "\n",
    "                # 计算前向+反向传播的总FLOPs\n",
    "                total_flops = flops_forward + flops_backward  # 或者使用total_flops = flops_forward * 3\n",
    "\n",
    "                # 获取模型参数的数据类型\n",
    "                data_type = next(model.parameters()).dtype\n",
    "                max_flops_per_second = flops_per_second[gpu_model].get(data_type, 0)  # 获取GPU的最大FLOP性能\n",
    "\n",
    "                # 计算每秒处理的tokens数\n",
    "                tokens_processed = batch_size * config[\"context_length\"]  # 处理的tokens总数\n",
    "                tokens_per_second = tokens_processed / total_time_seconds  # 每秒处理的tokens数\n",
    "\n",
    "                # 计算每个token的FLOPs\n",
    "                flops_per_token = total_flops / tokens_processed\n",
    "\n",
    "                # 计算理论最大每秒处理的tokens数\n",
    "                if flops_per_token > 0:\n",
    "                    theoretical_max_tokens_per_second = max_flops_per_second / flops_per_token\n",
    "                else:\n",
    "                    theoretical_max_tokens_per_second = 0  # 避免除以零的错误\n",
    "\n",
    "                # 计算MFU（模型FLOPs利用率）\n",
    "                if theoretical_max_tokens_per_second > 0:\n",
    "                    mfu = tokens_per_second / theoretical_max_tokens_per_second\n",
    "                else:\n",
    "                    mfu = 0  # 避免除以零的错误\n",
    "\n",
    "                # 打印当前批次大小的性能数据\n",
    "                print(f\"  Batch size {batch_size}: Tokens/sec: {tokens_per_second:.2f}, MFU: {mfu:.4f}\")\n",
    "\n",
    "                # 如果当前批次处理成功，尝试更大的批次\n",
    "                min_batch_size = batch_size + 1\n",
    "                max_batch_size = batch_size\n",
    "\n",
    "                # 清理内存\n",
    "                del model, input_tensor, output, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():  # 如果出现内存不足错误\n",
    "                    # 尝试减少批次大小\n",
    "                    max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                    # 清理内存\n",
    "                    try:\n",
    "                        del model, input_tensor\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                else:\n",
    "                    raise e  # 如果是其他错误，抛出异常\n",
    "\n",
    "# 如果无法识别GPU型号，则提示更新flops_per_second字典\n",
    "else:\n",
    "    print(\"Unknown GPU model. Please update the flops_per_second dictionary with your GPU information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LovmswRigzPG"
   },
   "source": [
    "-\t1.0的值为最佳（等于100%）。\n",
    "-\t请注意，由于我们还进行反向传播操作，消耗更多的内存，因此这次选择的批次大小比之前小。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6bc54f-2b16-4b0f-be69-957eed5d112f",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72953590-5363-4398-85ce-54bde07f3d8a",
   "metadata": {},
   "source": [
    "# 第五章的代码补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ab5ee-e7b9-45d3-a82b-a12bcfc0945a",
   "metadata": {},
   "source": [
    "## **使用 Transformers 从 Hugging Face Model Hub 加载模型权重**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2feea87-49f0-48b9-b925-b8f0dda4096f",
   "metadata": {},
   "source": [
    "- 在主章节中，我们直接从 OpenAI 加载了 GPT 模型权重。  \n",
    "- 本笔记本提供了一种 **替代方案**，使用 [`transformers` 库](https://huggingface.co/docs/transformers/index)从 [Hugging Face Model Hub](https://huggingface.co/docs/hub/en/models-the-hub) 加载模型权重。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b77109-5215-4d07-a618-4d10eff1a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0467eff-b43c-4a38-93e8-5ed87a5fc2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.24.3\n",
      "torch version: 2.3.0\n",
      "transformers version: 4.41.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \"torch\", \"transformers\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc17d7d-bcd8-42ee-82a9-04fd55acf15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea9b1bc-7881-46ad-9555-27a9cf23faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2a4cf4-a54e-4307-9141-fb9f288e4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75be3077-f141-44bb-af88-62580ffd224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda44d37-92c0-4c19-a70a-15711513afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights(gpt, gpt_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddd0d51-3ade-4890-9bab-d63f141d095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves forward, but it's not enough.\n",
      "\n",
      "\"I'm not going to sit here and say, 'I'm not going to do this,'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/03_bonus_pretraining_on_gutenberg/README.md">
# **在 Project Gutenberg 数据集上预训练 GPT**

本目录包含用于在 **Project Gutenberg** 提供的免费电子书上训练小型 GPT 模型的代码。

根据 **Project Gutenberg** 网站的说明，“绝大多数 Project Gutenberg 电子书在美国属于公有领域。”  

在使用 Project Gutenberg 提供的资源之前，请阅读 [Project Gutenberg 许可、权限和常见问题](https://www.gutenberg.org/policy/permission.html) 了解详细信息。
## 使用指南

### 1) 下载数据集 
在本节中，我们使用来自 [`pgcorpus/gutenberg`](https://github.com/pgcorpus/gutenberg) GitHub 存储库的代码从Project Gutenberg下载书籍。

截至撰写本文时，这将需要大约 50 GB 的磁盘空间，大约需要 10-15 个小时，但具体时间可能更长，具体取决于Project Gutenberg当前的大小。

#### Linux 和 macOS 用户的下载说明
Linux 和 macOS 用户可以按照以下步骤下载数据集（如果您是 Windows 用户，请参阅下面的注释）：
1. 将 `03_bonus_pretraining_on_gutenberg` 文件夹设置为工作目录，以在此文件夹中本地克隆 `gutenberg` 存储库（这是运行提供的脚本 `prepare_dataset.py` 和 `pretraining_simple.py` 所必需的）。例如，当位于 `LLMs-from-scratch` 存储库的文件夹中时，通过以下方式导航到 *03_bonus_pretraining_on_gutenberg* 文件夹：
```bash
cd ch05/03_bonus_pretraining_on_gutenberg
```

2. 在该目录中克隆 `gutenberg` 仓库：
```bash
git clone https://github.com/pgcorpus/gutenberg.git
```

3. 进入本地克隆的 `gutenberg` 仓库目录：
```bash
cd gutenberg
```

4. 在 `gutenberg` 仓库目录中，安装 *requirements.txt* 文件中定义的包：
```bash
pip install -r requirements.txt
```

5. 下载数据:
```bash
python get_data.py
```

6. 回到`03_bonus_pretraining_on_gutenberg` 文件姐
```bash
cd ..
```

#### Windows 用户的特别说明  

[`pgcorpus/gutenberg`](https://github.com/pgcorpus/gutenberg) 代码兼容 Linux 和 macOS，但 Windows 用户需要进行一些小调整，例如在 `subprocess` 调用中添加 `shell=True`，以及替换 `rsync` 命令。  

另一种更简单的方法是在 Windows 上使用 **Windows Subsystem for Linux（WSL）**，该功能允许用户在 Windows 环境中运行基于 Ubuntu 的 Linux 系统。详细信息请参考 [Microsoft 官方安装指南](https://learn.microsoft.com/en-us/windows/wsl/install) 和 [官方教程](https://learn.microsoft.com/en-us/training/modules/wsl-introduction/)。  

使用 WSL 时，请确保已安装 Python 3（可通过 `python3 --version` 检查版本，若未安装，可使用 `sudo apt-get install -y python3.10` 安装 Python 3.10）。此外，还需安装以下依赖包：

```bash
sudo apt-get update && \
sudo apt-get upgrade -y && \
sudo apt-get install -y python3-pip && \
sudo apt-get install -y python-is-python3 && \
sudo apt-get install -y rsync
```

> **注意**  
> 有关 Python 环境配置和依赖安装的详细说明，请参考：[可选 Python 配置指南](../../setup/01_optional-python-setup-preferences/README.md) 和 [Python 库安装指南](../../setup/02_installing-python-libraries/README.md)。  
>   
> 此外，本仓库提供了一个基于 Ubuntu 的 Docker 镜像。如果希望使用容器化环境运行代码，请参考 [可选 Docker 环境](../../setup/03_optional-docker-environment/README.md) 获取相关使用说明。  

&nbsp;  
### 2) 准备数据集  

接下来，运行 `prepare_dataset.py` 脚本，该脚本会将（截至撰写本文时，共 60,173 个）文本文件合并为更少数量的大文件，以提高数据传输和访问效率：

```bash
python prepare_dataset.py \
  --data_dir gutenberg/data/raw \
  --max_size_mb 500 \
  --output_dir gutenberg_preprocessed
```

```
...
Skipping gutenberg/data/raw/PG29836_raw.txt as it does not contain primarily English text.                                     Skipping gutenberg/data/raw/PG16527_raw.txt as it does not contain primarily English text.                                     100%|██████████████████████████████████████████████████████████| 57250/57250 [25:04<00:00, 38.05it/s]
42 file(s) saved in /Users/sebastian/Developer/LLMs-from-scratch/ch05/03_bonus_pretraining_on_gutenberg/gutenberg_preprocessed
```


> **💡 提示**  
> 生成的文件均为纯文本格式，未进行预分词处理，以保持简洁。然而，如果计划频繁使用该数据集或进行多轮训练，建议修改代码，将数据存储为 **预分词格式**，以减少计算成本。更多信息请参考本页底部的 *设计决策与优化建议*。  

> **💡 提示**  
> 你可以选择更小的文件大小，例如 **50MB**。这样会生成更多文件，但在测试时，可用于快速预训练少量数据，提高调试效率。  

&nbsp;  
### 3) 运行预训练脚本  

可以使用以下命令运行预训练脚本。请注意，示例中列出的命令行参数均为默认值，仅作说明：

```bash
python pretraining_simple.py \
  --data_dir "gutenberg_preprocessed" \
  --n_epochs 1 \
  --batch_size 4 \
  --output_dir model_checkpoints
```

输出格式如下所示：

> 总文件数：3  
> 正在对文件 1/3 进行分词处理：data_small/combined_1.txt  
> 训练中 ...  
> 轮次 1（步骤 0）：训练损失 9.694，验证损失 9.724  
> 轮次 1（步骤 100）：训练损失 6.672，验证损失 6.683  
> 轮次 1（步骤 200）：训练损失 6.543，验证损失 6.434  
> 轮次 1（步骤 300）：训练损失 5.772，验证损失 6.313  
> 轮次 1（步骤 400）：训练损失 5.547，验证损失 6.249  
> 轮次 1（步骤 500）：训练损失 6.182，验证损失 6.155  
> 轮次 1（步骤 600）：训练损失 5.742，验证损失 6.122  
> 轮次 1（步骤 700）：训练损失 6.309，验证损失 5.984  
> 轮次 1（步骤 800）：训练损失 5.435，验证损失 5.975  
> 轮次 1（步骤 900）：训练损失 5.582，验证损失 5.935  
> ...  
> 轮次 1（步骤 31900）：训练损失 3.664，验证损失 3.946  
> 轮次 1（步骤 32000）：训练损失 3.493，验证损失 3.939  
> 轮次 1（步骤 32100）：训练损失 3.940，验证损失 3.961  
> 模型已保存至 model_checkpoints/model_pg_32188.pth  
> 处理一本书耗时：3 小时 46 分 55 秒  
> 总计耗时：3 小时 46 分 55 秒  
> 预计剩余时间：7 小时 33 分 50 秒  
> 正在对文件 2/3 进行分词处理：data_small/combined_2.txt  
> 训练中 ...  
> 轮次 1（步骤 32200）：训练损失 2.982，验证损失 4.094  
> 轮次 1（步骤 32300）：训练损失 3.920，验证损失 4.097  
> ...


> **💡 提示**  
> 在 macOS 或 Linux 系统上，建议使用 `tee` 命令将日志输出同时打印到终端并保存至 `log.txt` 文件，以便后续分析：

```bash
python -u pretraining_simple.py | tee log.txt
```

> **⚠️ 警告**  
> 在 **V100 GPU** 上，对 `gutenberg_preprocessed` 目录下的 **1 个 ~500MB** 文本文件进行训练大约需要 **4 小时**。  
> 该文件夹包含 **47 个文件**，完整训练预计耗时 **200 小时（超过 1 周）**。  
> 建议选择较少的文件进行训练，以减少训练时间。  

&nbsp;  
## 设计决策与优化建议  

本代码以 **简洁性和可读性** 为主，旨在用于 **教育目的**，但仍有多个方面可优化，以提高 **模型性能** 和 **训练效率**：  

1. **优化数据清理**：修改 `prepare_dataset.py`，去除每本书中的 **Gutenberg 标准页眉页脚**，以提升数据质量。  
2. **预处理分词**：调整数据准备和加载流程，将数据集**预分词并存储为分词格式**，避免每次调用预训练脚本时都重新分词，减少计算开销。  
3. **改进训练过程**：在 `train_model_simple` 中，添加 [附录 D: 训练优化技巧](../../appendix-D/01_main-chapter-code/appendix-D.ipynb) 中介绍的优化功能，如：
   - **余弦衰减调度（Cosine Decay）**  
   - **线性预热（Linear Warmup）**  
   - **梯度裁剪（Gradient Clipping）**  
4. **支持断点恢复**：修改预训练脚本，使其在保存 **模型权重** 的同时保存 **优化器状态**（参考第 5 章 *5.4 PyTorch 的权重加载与保存*，[ch05.ipynb](../../ch05/01_main-chapter-code/ch05.ipynb)），并支持**断点续训**。  
5. **添加可视化日志**：集成 **Weights & Biases** 或其他日志工具，以实时查看训练损失和验证曲线。  
6. **多 GPU 并行训练**：实现 **分布式数据并行（DDP）**，在多个 GPU 设备上加速训练（参考附录 A *A.9.3 多 GPU 训练*，[DDP-script.py](../../appendix-A/01_main-chapter-code/DDP-script.py)）。  
7. **优化注意力机制**：替换 `previous_chapter.py` 中的 `MultiheadAttention` 类，使用 [高效多头注意力实现](../../ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb) 章节中的 `MHAPyTorchScaledDotProduct`，该实现基于 PyTorch **Flash Attention**（`nn.functional.scaled_dot_product_attention`），可大幅提升计算效率。  
8. **加速训练**：采用 **模型编译优化**，可选择：
   - **PyTorch 2.0 的 `torch.compile`**（[`torch.compile` 教程](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)）  
   - **Lightning Thunder 的 `thunder.jit(model)`**（[Thunder GitHub](https://github.com/Lightning-AI/lightning-thunder)）  
9. **低秩梯度投影（GaLore）优化**：通过 **Gradient Low-Rank Projection（GaLore）** 加速预训练，仅需将优化器 **`AdamW` 替换为 `GaLoreAdamW`**，该优化器已集成在 [GaLore Python 库](https://github.com/jiaweizzhao/GaLore) 中。
</file>

<file path="ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0_xya1nyDHfY",
   "metadata": {
    "id": "0_xya1nyDHfY"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l62zIRRSBy_R",
   "metadata": {
    "id": "l62zIRRSBy_R"
   },
   "source": [
    "# 从零开始将 LLaMA 2 转换为 LLaMA 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aFmxTQbwCUMl",
   "metadata": {
    "id": "aFmxTQbwCUMl"
   },
   "source": [
    "- 本笔记本是 [《从零开始将 GPT 架构转换为 LLaMA 2》](./converting-gpt-to-llama2.ipynb) 的后续内容，逐步将 **Meta AI 的 LLaMA 2** 模型架构转换为 **LLaMA 3**、**LLaMA 3.1** 和 **LLaMA 3.2**。  \n",
    "- 本笔记本的解释部分被 **有意简化**，以避免过度冗长，并专注于 **主要代码**。  \n",
    "- 如需了解更多架构细节，请参考 **LLaMA 2** 和 **LLaMA 3** 相关论文：  \n",
    "  - [LLaMA 2: 开放基础模型与微调聊天模型（2023）](https://arxiv.org/abs/2307.09288)  \n",
    "  - [LLaMA 3 模型](https://arxiv.org/abs/2407.21783)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ohhMKUWvGm9z",
   "metadata": {
    "id": "ohhMKUWvGm9z"
   },
   "source": [
    "<img src=\"../image/gpt2-to-llama2-llama3.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ws0wsUzwLH2k",
   "metadata": {
    "id": "ws0wsUzwLH2k"
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBpQwU89ETA1",
   "metadata": {
    "id": "JBpQwU89ETA1"
   },
   "source": [
    "- 下面是所需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
    "outputId": "e3d3d4b6-ee63-4e28-d794-e8b0bdd931fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.24.7\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         # to download pretrained weights\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJJneXpTEg4W",
   "metadata": {
    "id": "UJJneXpTEg4W"
   },
   "source": [
    "&nbsp;\n",
    "# 1. 逐步转换 LLaMA 模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1zpfX2GHBKa",
   "metadata": {
    "id": "v1zpfX2GHBKa"
   },
   "source": [
    "- 如果您 **刚接触** LLM 架构的实现，建议从 **[第 4 章](../../ch04/01_main-chapter-code/ch04.ipynb)** 开始，该章节 **逐步讲解了原始 GPT 架构的实现**。  \n",
    "- 随后，[《从零开始将 GPT 架构转换为 LLaMA 2》](./converting-gpt-to-llama2.ipynb) 介绍了 **LLaMA 特定组件**，包括：\n",
    "  - **RMSNorm** 层  \n",
    "  - **SiLU** 和 **SwiGLU** 激活函数  \n",
    "  - **RoPE（旋转位置编码）**  \n",
    "  - **SentencePiece 分词器**  \n",
    "- 本笔记本将 **LLaMA 2** 结构转换为 **LLaMA 3** 结构，具体步骤如下：\n",
    "  1. **修改旋转位置编码（RoPE）**\n",
    "  2. **实现分组查询注意力（Grouped-Query Attention, GQA）**\n",
    "  3. **采用定制版的 GPT-4 分词器**  \n",
    "- 最后，我们将在此架构中 **加载 Meta AI 提供的原始 LLaMA 3 预训练权重**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b9121-abe1-4a46-99b8-acdef71e5b41",
   "metadata": {
    "id": "c14b9121-abe1-4a46-99b8-acdef71e5b41"
   },
   "source": [
    "&nbsp;\n",
    "## 1.1 复用 LLaMA 2 组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dgDhJGJ6xR4e",
   "metadata": {
    "id": "dgDhJGJ6xR4e"
   },
   "source": [
    "- **LLaMA 2** 与 **LLaMA 3** 结构上 **非常相似**，如上所述，并在本笔记本顶部的示意图中进行了说明。  \n",
    "- 这意味着，我们可以通过以下代码，从 [LLaMA 2 笔记本](./converting-gpt-to-llama2.ipynb) 中 **导入多个基础模块**： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5bc3948-231b-4f1f-8d41-24ad0b7643d0",
   "metadata": {
    "id": "a5bc3948-231b-4f1f-8d41-24ad0b7643d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import nbformat\n",
    "import types\n",
    "\n",
    "def import_from_notebook():\n",
    "    def import_definitions_from_notebook(fullname, names):\n",
    "        current_dir = os.getcwd()\n",
    "        path = os.path.join(current_dir, fullname + \".ipynb\")\n",
    "        path = os.path.normpath(path)\n",
    "\n",
    "        # Load the notebook\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Notebook file not found at: {path}\")\n",
    "\n",
    "        with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "        # Create a module to store the imported functions and classes\n",
    "        mod = types.ModuleType(fullname)\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # Go through the notebook cells and only execute function or class definitions\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == \"code\":\n",
    "                cell_code = cell.source\n",
    "                for name in names:\n",
    "                    # Check for function or class definitions\n",
    "                    if f\"def {name}\" in cell_code or f\"class {name}\" in cell_code:\n",
    "                        exec(cell_code, mod.__dict__)\n",
    "        return mod\n",
    "\n",
    "    fullname = \"converting-gpt-to-llama2\"\n",
    "    names = [\"precompute_rope_params\", \"compute_rope\", \"SiLU\", \"FeedForward\", \"RMSNorm\", \"MultiHeadAttention\"]\n",
    "\n",
    "    return import_definitions_from_notebook(fullname, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d546032d-fce4-47cf-8d0e-682b78b21c61",
   "metadata": {
    "id": "d546032d-fce4-47cf-8d0e-682b78b21c61"
   },
   "outputs": [],
   "source": [
    "imported_module = import_from_notebook()\n",
    "\n",
    "# We need to redefine precompute_rope_params\n",
    "# precompute_rope_params = getattr(imported_module, \"precompute_rope_params\", None)\n",
    "compute_rope = getattr(imported_module, \"compute_rope\", None)\n",
    "SiLU = getattr(imported_module, \"SiLU\", None)\n",
    "FeedForward = getattr(imported_module, \"FeedForward\", None)\n",
    "RMSNorm = getattr(imported_module, \"RMSNorm\", None)\n",
    "\n",
    "# MultiHeadAttention only for comparison purposes\n",
    "MultiHeadAttention = getattr(imported_module, \"MultiHeadAttention\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f",
   "metadata": {
    "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.2 优化RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9_oDcHCx8VI",
   "metadata": {
    "id": "m9_oDcHCx8VI"
   },
   "source": [
    "- **LLaMA 3** 采用与 **LLaMA 2** 类似的 **旋转位置编码（RoPE）**（详细说明请参考 [RoPE 论文](https://arxiv.org/abs/2104.09864)）。  \n",
    "- 但在 RoPE 的具体设置上存在一些 **细微差异**：\n",
    "  - **LLaMA 3** 现在支持 **最多 8,192 个 Token**，是 **LLaMA 2（4,096 个 Token）** 的 **一倍**。  \n",
    "  - **RoPE 的基础值** $\\theta$（见下方公式）从 **10,000（LLaMA 2）** 提高至 **500,000（LLaMA 3）**。  \n",
    "\n",
    "  公式（改编自 [RoPE 论文](https://arxiv.org/abs/2104.09864)）：  \n",
    "  $$\n",
    "  \\Theta = \\left\\{\\theta_i = \\text{base}^{\\frac{-2(i-1)}{d}}, i \\in \\left[1, 2, ..., d/2\\right]\\right\\}\n",
    "  $$\n",
    "\n",
    "- 其中，$\\theta$ 值是一组 **预定义参数**，用于计算 **旋转矩阵** 中的旋转角度，其中 $d$ 为 **嵌入空间维度**。  \n",
    "- **将基础值从 10,000 增加到 500,000** 使得频率（旋转角度）在维度上的衰减 **更缓慢**，意味着高维度的角度比以前更大（本质上是对 **频率的解压缩**）。  \n",
    "- 此外，我们在下面的代码中 **引入 `freq_config` 配置** 以调整频率；但 **LLaMA 3 本身并不需要此调整**（仅适用于 **LLaMA 3.1 和 LLaMA 3.2**）。因此，我们稍后会 **重新讨论 `freq_config`**，目前它默认为 `None` 并被 **忽略**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6Upl109OOAcu",
   "metadata": {
    "id": "6Upl109OOAcu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    ################################ NEW ###############################################\n",
    "    # Frequency adjustments\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jJBvO0YMJBXR",
   "metadata": {
    "id": "jJBvO0YMJBXR"
   },
   "source": [
    "- 总结一下，**LLaMA 3** 相较于 **LLaMA 2** 的主要变化在于：\n",
    "  - **上下文长度（Context Length）** 增加到 **8,192**（LLaMA 2 为 **4,096**）。  \n",
    "  - **RoPE 的基础值 $\\theta$** 从 **10,000** 提高至 **500,000**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c37216-e022-4603-be16-f9d3eaeaf4a1",
   "metadata": {
    "id": "56c37216-e022-4603-be16-f9d3eaeaf4a1"
   },
   "outputs": [],
   "source": [
    "# Instantiate RoPE parameters\n",
    "\n",
    "llama_2_context_len = 4096\n",
    "llama_3_context_len = 8192\n",
    "\n",
    "llama_2_theta_base = 10_000\n",
    "llama_3_theta_base = 500_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_V8v6i7MJItU",
   "metadata": {
    "id": "_V8v6i7MJItU"
   },
   "source": [
    "- **RoPE 的使用方式仍与 LLaMA 2 相同**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae70c8a-eb18-40f9-a2e5-a6af2a57628b",
   "metadata": {
    "id": "dae70c8a-eb18-40f9-a2e5-a6af2a57628b"
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(\n",
    "    head_dim=head_dim,\n",
    "    theta_base=llama_3_theta_base,\n",
    "    context_length=llama_3_context_len\n",
    ")\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19b75c-cf25-47b8-a010-6733fc0e9a8a",
   "metadata": {
    "id": "cd19b75c-cf25-47b8-a010-6733fc0e9a8a"
   },
   "source": [
    "&nbsp;\n",
    "## 1.3 分组查询注意力（Grouped-Query Attention, GQA）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c7d3f-fded-49e8-a617-9fe67b81dddc",
   "metadata": {
    "id": "111c7d3f-fded-49e8-a617-9fe67b81dddc"
   },
   "source": [
    "- 在本节中，我们用 **分组查询注意力（GQA）** 取代 **多头注意力（MHA）**。  \n",
    "- 简而言之，**GQA** 可以看作是 **计算和参数更加高效** 的 **MHA 替代方案**。  \n",
    "- 在 **GQA** 机制中，我们通过 **共享多个注意力头的键（Key）和值（Value）** 来减少 **Key-Value 投影** 的数量。  \n",
    "- 每个注意力头仍然有 **独立的查询（Query）**，但它们会关注 **相同的 Key-Value 组**。  \n",
    "- 下图展示了 **GQA 机制**，其中 **每组 Key-Value 共享 2 个注意力头（kv-groups = 2）**：\n",
    "\n",
    "<img src=\"../image/grouped-query-attention.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perAYa2R_KW2",
   "metadata": {
    "id": "perAYa2R_KW2"
   },
   "source": [
    "- **GQA** 的核心思想是 **减少** 访问 **Key-Value 对** 的 **独立查询组** 数量，从而：\n",
    "  - **减少矩阵乘法的计算量**  \n",
    "  - **降低 MHA（多头注意力）的参数规模**  \n",
    "  - **在不显著影响建模性能的前提下提高计算效率**  \n",
    "- **GQA 代码** 与 **MHA（多头注意力）** 十分相似（代码中的 **\"NEW\"** 部分标注了关键改动）。  \n",
    "- **简而言之，GQA 的主要变化** 是：\n",
    "  - **每个查询组（Query Group）需要重复，以匹配其关联的多头数**（具体实现如下）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842aa71a-4659-424e-8830-392bd6ae86af",
   "metadata": {},
   "source": [
    "- **此外，我们引入 `SharedBuffers` 类**，用于在 **Transformer 块** 中 **复用 `mask`、`cos` 和 `sin` 张量**，以提高计算效率。  \n",
    "  - 这一优化在 **LLaMA 3.1 和 LLaMA 3.2** 等支持 **多达 131k 输入 Token** 的模型中至关重要。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b12e674-ef08-4dd7-8843-615b65b39c91",
   "metadata": {
    "id": "9b12e674-ef08-4dd7-8843-615b65b39c91"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "############################# NEW  #############################\n",
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]\n",
    "############################# NEW  #############################\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            rope_base=10_000,    # NEW\n",
    "            rope_config=None,    # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"  # NEW\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        ############################# NEW  #############################\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "        ################################################################\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        ############################# NEW  #############################\n",
    "        # Fetch buffers using SharedBuffers\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "        ############################# NEW  #############################\n",
    "        \n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape queries, keys, and values\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        ################################################\n",
    "\n",
    "        # Transpose keys, values, and queries\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_query_groups, num_tokens, head_dim)\n",
    "\n",
    "        # Apply RoPE\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # Expand keys and values to match the number of heads\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # For example, before repeat_interleave along dim=1 (query groups):\n",
    "        #   [K1, K2]\n",
    "        # After repeat_interleave (each query group is repeated group_size times):\n",
    "        #   [K1, K1, K2, K2]\n",
    "        # If we used regular repeat instead of repeat_interleave, we'd get:\n",
    "        #   [K1, K2, K1, K2]\n",
    "        ################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roAXSwJs9hR8",
   "metadata": {
    "id": "roAXSwJs9hR8"
   },
   "source": [
    "- 为了直观展示 **参数节省** 的效果，下面以 **GPT** 和 **LLaMA 2** 代码中的 **多头注意力（MHA）** 作为示例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b8f085-349e-4674-a3f0-78fde0664fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4b8f085-349e-4674-a3f0-78fde0664fac",
    "outputId": "9da09d72-43b1-45af-d46f-6928ea4af33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key: torch.Size([4096, 4096])\n",
      "W_value: torch.Size([4096, 4096])\n",
      "W_query: torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 3000\n",
    "max_context_len = 8192\n",
    "embed_dim = 4096\n",
    "num_heads = 32\n",
    "\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "mha(example_batch)\n",
    "\n",
    "print(\"W_key:\", mha.W_key.weight.shape)\n",
    "print(\"W_value:\", mha.W_value.weight.shape)\n",
    "print(\"W_query:\", mha.W_query.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IMQtFkcQ9sXC",
   "metadata": {
    "id": "IMQtFkcQ9sXC"
   },
   "source": [
    "- 现在，如果我们改用 **分组查询注意力（GQA）**，并设置 **8 个 Key-Value 组（kv-groups）**（这是 **LLaMA 3 8B** 使用的设置），我们可以观察到：\n",
    "  - **Key 和 Value 矩阵的行数减少了 4 倍**，  \n",
    "  - 因为 **32 个注意力头 ÷ 8 个 kv-groups = 4**。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e65d3c-7b42-4ed3-bfee-bb09578657bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15e65d3c-7b42-4ed3-bfee-bb09578657bb",
    "outputId": "69709a78-2aaa-4597-8142-2f44eb59753f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key: torch.Size([1024, 4096])\n",
      "W_value: torch.Size([1024, 4096])\n",
      "W_query: torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "gqa = GroupedQueryAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_groups=8,\n",
    "    rope_base=llama_3_theta_base\n",
    ")\n",
    "\n",
    "gqa(example_batch)\n",
    "\n",
    "print(\"W_key:\", gqa.W_key.weight.shape)\n",
    "print(\"W_value:\", gqa.W_value.weight.shape)\n",
    "print(\"W_query:\", gqa.W_query.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d4c88-c66a-483b-b4e2-419ff9fd60d5",
   "metadata": {
    "id": "1a5d4c88-c66a-483b-b4e2-419ff9fd60d5"
   },
   "source": [
    "- **补充说明**：如果希望 **分组查询注意力（GQA）** 等效于 **标准多头注意力（MHA）**，可以将 **查询组数（`num_kv_groups`）** 设为 **注意力头数（`num_heads`）**。  \n",
    "- 最后，我们来对比 **参数数量**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f713aa-ac00-4e2f-8247-94609aa01350",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58f713aa-ac00-4e2f-8247-94609aa01350",
    "outputId": "486dfd9c-9f3a-4b9e-f9a2-35fb43b9a5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:\n",
      "MHA: 67,108,864\n",
      "GQA: 41,943,040\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of parameters:\")\n",
    "\n",
    "mha_total_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"MHA: {mha_total_params:,}\")\n",
    "\n",
    "gqa_total_params = sum(p.numel() for p in gqa.parameters())\n",
    "print(f\"GQA: {gqa_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b60dfd-6c0f-41f7-8f0c-8e57116f07f5",
   "metadata": {
    "id": "78b60dfd-6c0f-41f7-8f0c-8e57116f07f5"
   },
   "outputs": [],
   "source": [
    "# Free up memory:\n",
    "del mha\n",
    "del gqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd8802-2859-45a2-905a-f4fe96629dd9",
   "metadata": {
    "id": "8fcd8802-2859-45a2-905a-f4fe96629dd9"
   },
   "source": [
    "&nbsp;\n",
    "## 1.4 更新 `TransformerBlock` 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KABNccft_YnR",
   "metadata": {
    "id": "KABNccft_YnR"
   },
   "source": [
    "- 接下来，我们更新 **`TransformerBlock`**。  \n",
    "- 主要修改如下：\n",
    "  - **将 `MultiHeadAttention` 替换为 `GroupedQueryAttention`**  \n",
    "  - **添加新的 RoPE 设置**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9fa8eb4-7196-4dee-aec6-0dcbc70921c4",
   "metadata": {
    "id": "f9fa8eb4-7196-4dee-aec6-0dcbc70921c4"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention(  # MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],  # NEW\n",
    "            rope_base=cfg[\"rope_base\"],        # NEW\n",
    "            rope_config=cfg[\"rope_freq\"],      # NEW\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd921ab5-c48c-4c52-bf41-b847b3b822b9",
   "metadata": {
    "id": "fd921ab5-c48c-4c52-bf41-b847b3b822b9"
   },
   "source": [
    "&nbsp;\n",
    "## 1.5 定义model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M_tLAq_r_llN",
   "metadata": {
    "id": "M_tLAq_r_llN"
   },
   "source": [
    "- 在 **设置模型类** 时，我们几乎 **无需修改** 其他内容；只需将模型名称更新为 **`Llama3Model`**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475755d6-01f7-4e6e-ad9a-cec6f031ebf6",
   "metadata": {
    "id": "475755d6-01f7-4e6e-ad9a-cec6f031ebf6"
   },
   "outputs": [],
   "source": [
    "# class Llama2Model(nn.Module):\n",
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60",
   "metadata": {
    "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60"
   },
   "source": [
    "&nbsp;\n",
    "## 2. 初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HoGGRAGykQTE",
   "metadata": {
    "id": "HoGGRAGykQTE"
   },
   "source": [
    "- 现在，我们可以 **定义 LLaMA 3 配置文件**（同时提供 **LLaMA 2 配置文件** 作为对比）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18",
   "metadata": {
    "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18"
   },
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32_000,    # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11_008,    # Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad90f82-15c7-4806-b509-e45b56f57db5",
   "metadata": {
    "id": "2ad90f82-15c7-4806-b509-e45b56f57db5"
   },
   "outputs": [],
   "source": [
    "LLAMA3_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,   # NEW: Larger vocabulary size\n",
    "    \"context_length\": 8192,  # NEW: Larger context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 14_336,    # NEW: Larger size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,        # NEW: Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,  # NEW: The base in RoPE's \"theta\" was increased to 500_000\n",
    "    \"rope_freq\": None,       # NEW: Additional configuration for adjusting the RoPE frequencies\n",
    "    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAP7fiBzkaBz",
   "metadata": {
    "id": "FAP7fiBzkaBz"
   },
   "source": [
    "- 使用这些设置，我们现在可以 **初始化 LLaMA 3 8B 模型**。  \n",
    "- **注意**：该模型 **需要约 34GB 内存**（作为对比，**LLaMA 2 7B 需要约 26GB 内存**）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7004d785-ac9a-4df5-8760-6807fc604686",
   "metadata": {
    "id": "7004d785-ac9a-4df5-8760-6807fc604686"
   },
   "outputs": [],
   "source": [
    "model = Llama3Model(LLAMA3_CONFIG_8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea6334-d1fc-427d-9cf2-4af963ff4bfc",
   "metadata": {},
   "source": [
    "- 下面的代码应输出 **`True`**，以确认 **缓冲区被复用** 而不是 **被不必要地重新创建**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9625cc-9afa-4b11-8aab-d536fd170761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check buffers\n",
    "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
    "print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
    "print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056a521-91a6-440f-8473-591409c3177b",
   "metadata": {},
   "source": [
    "- 现在，我们还将 **计算可训练参数的总数**： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
    "outputId": "0a8cd23b-d9fa-4c2d-ca63-3fc79bc4de0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,030,261,248\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bx14NtzWk2wj",
   "metadata": {
    "id": "Bx14NtzWk2wj"
   },
   "source": [
    "- 如上所示，该模型包含 **80 亿（8B）参数**。  \n",
    "- 此外，我们可以使用以下代码 **计算该模型的内存需求**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
    "outputId": "3425e9ce-d8c0-4b37-bded-a2c60b66a41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 68.08 GB\n",
      "bfloat16: 34.04 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zudd-5PulKFL",
   "metadata": {
    "id": "zudd-5PulKFL"
   },
   "source": [
    "- 最后，如果适用，我们还可以将模型部署到 **NVIDIA GPU** 或 **Apple Silicon GPU**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d",
   "metadata": {
    "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34",
   "metadata": {
    "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34"
   },
   "source": [
    "&nbsp;\n",
    "## 3. 加载tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005",
   "metadata": {
    "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005"
   },
   "source": [
    "- 在本节中，我们将 **加载模型的分词器（Tokenizer）**。  \n",
    "- **LLaMA 2** 使用的是 **Google 的 [SentencePiece](https://github.com/google/sentencepiece) 分词器**，而 **不是** 基于 [Tiktoken](https://github.com/openai/tiktoken) 库的 **OpenAI BPE 分词器**。  \n",
    "- **LLaMA 3** 重新 **回归** 了 **Tiktoken 的 BPE 分词器**，并且 **使用了 GPT-4 分词器**，其 **词汇表** 进行了扩展。  \n",
    "- Meta AI 提供了 **LLaMA 3 适配 Tiktoken 的代码**，可在其官方 **[LLaMA 3 仓库](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py)** 中找到。  \n",
    "- 下面的代码是对 **LLaMA 3 分词器** 的 **简化版实现**，旨在提升可读性，同时保持相同的行为。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f390cbf-8f92-46dc-afe3-d90b5affae10",
   "metadata": {
    "id": "5f390cbf-8f92-46dc-afe3-d90b5affae10"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.model.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1509f8-8778-4fec-ba32-14d95c646167",
   "metadata": {
    "id": "0a1509f8-8778-4fec-ba32-14d95c646167"
   },
   "source": [
    "- **Meta AI** 在 **Hugging Face Hub** 上分享了 **LLaMA 3 原始模型权重**及 **分词器词汇表**。  \n",
    "- 我们将首先从 **Hub** 下载 **分词器词汇表**，然后将其加载到上述代码中。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KbnlzsbYmJU6",
   "metadata": {
    "id": "KbnlzsbYmJU6"
   },
   "source": [
    "- 请注意，**Meta AI** 要求您在下载文件之前 **接受 LLaMA 3 许可协议**。  \n",
    "  - 为此，您需要创建 **Hugging Face Hub 账号**，然后访问 [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) 仓库并 **接受许可条款**。  \n",
    "- 接下来，您需要创建一个 **访问令牌（Access Token）**。  \n",
    "  - 要生成 **具有 READ 权限的访问令牌**，请点击 **右上角的个人头像**，然后选择 **\"Settings\"（设置）**。  \n",
    "\n",
    "<img src=\"../image/settings.webp\" width=\"300px\">\n",
    "\n",
    "- 然后，创建并复制 **访问令牌**，以便在接下来的代码单元中使用。  \n",
    "\n",
    "<img src=\"../image/access-token.webp\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3357a230-b678-4691-a238-257ee4e80185",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3357a230-b678-4691-a238-257ee4e80185",
    "outputId": "a3652def-ea7f-46fb-f293-2a59affb71a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IxGh6ZYQo0VN",
   "metadata": {
    "id": "IxGh6ZYQo0VN"
   },
   "source": [
    "- **使用访问token（Access Token）登录** 后，系统将验证我们已接受 **LLaMA 3 许可协议**，然后即可 **下载分词器词汇表**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
    "outputId": "c9836ba8-5176-4dd5-b618-6cc36fdbe1f0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8BH1Nk0AYCS",
   "metadata": {
    "id": "F8BH1Nk0AYCS"
   },
   "source": [
    "- 请注意，在使用 **LLaMA 3** 相关文件时，我们可能需要 **`blobfile`** 库。  \n",
    "- 该库用于处理存储在 **云存储**（如 **Google Cloud Storage (GCS)**、**Azure Blob Storage** 或 **Amazon S3**）中的 **数据集或模型文件**。  \n",
    "- 您可以通过 **取消注释并运行** 下面的 **`pip` 命令** 来安装此依赖项。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dm6Oz7uAytV",
   "metadata": {
    "id": "5dm6Oz7uAytV"
   },
   "outputs": [],
   "source": [
    "# pip install blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8c0ce6-a6fb-4b8a-8de2-ee7bb7646fd0",
   "metadata": {
    "id": "8b8c0ce6-a6fb-4b8a-8de2-ee7bb7646fd0"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NVhmFeX3pT_M",
   "metadata": {
    "id": "NVhmFeX3pT_M"
   },
   "source": [
    "- 现在，我们可以使用 **`generate` 函数** 让 **LLaMA 3** 模型 **生成新文本**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
    "outputId": "990d7b74-cb35-476b-d8bd-d544006e00f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort_dead aeros Ingredients başında.extensionégor clangmissions güc như submodule.and report官方%，.Reader(\",\");\n",
      "ामल ندار Parliamentary !!! HigginsDynamicZhgmt writeln Globalsletion 사진------\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93WTtAA5paYV",
   "metadata": {
    "id": "93WTtAA5paYV"
   },
   "source": [
    "- 当然，如上所示，当前生成的文本**并无实际意义**，因为我们尚未对 **LLaMA 3** 模型进行训练。  \n",
    "- 在下一节，我们不会自行训练模型（这将花费 **数万到数十万美元**），而是直接 **加载 Meta AI 提供的预训练权重**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cc248-1d27-4eb6-aa50-173b436652f8",
   "metadata": {
    "id": "f63cc248-1d27-4eb6-aa50-173b436652f8"
   },
   "source": [
    "&nbsp;\n",
    "## 4. 加载预训练的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKeN7rUfqZMI",
   "metadata": {
    "id": "aKeN7rUfqZMI"
   },
   "source": [
    "- 下面，我们加载 **[\"meta-llama/Meta-Llama-3-8B\"](https://huggingface.co/meta-llama/Meta-Llama-3-8B)** 预训练基础模型，该模型在微调之前 **仅用于文本补全**。  \n",
    "- 或者，您可以通过 **修改下一代码单元中的字符串**，加载 **指令微调并对齐的 [\"meta-llama/Meta-Llama-3-8B-Instruct\"](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) 模型**。  \n",
    "- 这些 **权重文件的总大小约为 16GB**。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "f3788acce34f4956b0727b58d0cf38c6",
      "6022a9426683420690d9b41a0ca4f870",
      "e9aba3d53b4d45c485a7aad649c7b465",
      "f1a12d7929db4309b9881853135359fc",
      "58c9dec75a3346b1b787f88dd510d254",
      "9492edc02dee456f840325d913fa4e4f",
      "66dc94b23556499f985f8accbb1f89cb",
      "7c6658cfff1a4d27af3de148184f77d9",
      "7266a729edfb4a44b5b1c67dc79be146",
      "76dbab4873f342019c5d7624ae2c9775",
      "3cea4b431147441a8d9bd872811d5974",
      "8ae98969541849efa356cf912ac39b1e",
      "f9373112649945e3b446c3e1ec274dc1",
      "d49791082a304ade95c185c79fae1f41",
      "616e383bb3d442bcb6edb2721a8180b6",
      "87f474861e54432e9d533e0a89bb77da",
      "e805bb6dfee34dab8870f4618d8bffdb",
      "be3e9bf271f04eb0b119659e1af3a0ea",
      "00148825ce0248b7a23eb28e3eca6749",
      "f1a9b0c2431640298a6c1b258298b12d",
      "8ba9f009e92a46fcbcbb401dc444f12e",
      "d74186bb74d142dfb683fa347b6990f7",
      "9bb60a5a3710463ebe3a17f8d2a446be",
      "0a08fb81165748748ccb080e6df0600f",
      "603690f543114a7fb6aebd433c80bdc3",
      "773b802daed942f5a11f3eab3b83be08",
      "7989003a613e45f780d3f800e121543a",
      "9d49589118f5432cac49650251046429",
      "f114549fe8ce49638a791ca2fecb2d89",
      "0aa155b794a8426aa265f4a7670f43ad",
      "a06fbde549cc47fdaddfbdb82d35d823",
      "172c0c6955e1428b999dcb2d133704cd",
      "1bf7108774c34016a2193e2cd7639b7d",
      "ed28e180d94a4b7aa548581612e31232",
      "ff4338faded5494da1ccb660e1c441ed",
      "b46a08cf4929422eb0f76d8d9af11249",
      "f049eb4a50f54c34912ca959d2eaf353",
      "80dfd3e80ceb444a83ec1fd65f9af80e",
      "519147a10b984befbd0f255f78c1f66a",
      "562e82438dbe41b793ff488b8447c5bf",
      "1da83719e47c4196b06f3aa32056b560",
      "c4a2c88326d14fbca87cfde073755a2e",
      "f0ab5a46cbb0444c88ed137d8a95002b",
      "f8f28ac0e149428f9fef42373c6a87d0"
     ]
    },
    "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
    "outputId": "c05118ce-9f81-41c8-a1f2-72caa932ae86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245443330e4d40c887a5649cc1663e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "combined_weights = {}\n",
    "\n",
    "for i in range(1, 5):\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        filename=f\"model-0000{i}-of-00004.safetensors\",\n",
    "        local_dir=\"Llama-3-8B\"\n",
    "    )\n",
    "    current_weights = load_file(weights_file)\n",
    "    combined_weights.update(current_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-15SJ7btq2zE",
   "metadata": {
    "id": "-15SJ7btq2zE"
   },
   "source": [
    "- **`weights`** 变量包含以下 **张量**（为简洁起见，仅展示 **前 15 个**）：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
    "outputId": "2fbc2786-677f-4fea-9472-5fb8542ff14b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.1.input_layernorm.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight',\n",
       " 'model.layers.1.post_attention_layernorm.weight']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combined_weights.keys())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UeeSpnunrDFB",
   "metadata": {
    "id": "UeeSpnunrDFB"
   },
   "source": [
    "- The following function, modeled after the `load_weights_into_gpt` function in [chapter 5](../01_main-chapter-code/ch05.ipynb), loads the pretrained weights into our Llama 3 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65",
   "metadata": {
    "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65"
   },
   "outputs": [],
   "source": [
    "def assign(left, right, tensor_name=\"unknown\"):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params.keys():\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "        print(\"Model uses weight tying.\")\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, LLAMA3_CONFIG_8B, combined_weights)\n",
    "model.to(device);\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TDuv_Us2rNvk",
   "metadata": {
    "id": "TDuv_Us2rNvk"
   },
   "source": [
    "- 接下来，我们已准备好使用 **模型进行文本生成**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "240987e8-a023-462e-9376-9edfb27559ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "240987e8-a023-462e-9376-9edfb27559ec",
    "outputId": "6dab0e56-40a8-45db-a096-ab2b9ee97a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203041e-4794-4157-a978-3ce80909da44",
   "metadata": {
    "id": "1203041e-4794-4157-a978-3ce80909da44"
   },
   "source": [
    "&nbsp;\n",
    "## 5. 使用指令微调的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "akyo7WNyF_YL",
   "metadata": {
    "id": "akyo7WNyF_YL"
   },
   "source": [
    "- 先前我们使用的是 **预训练基础模型**；如果希望使用 **能够遵循指令的模型**，请改用 `\"meta-llama/Llama-3-8B-Instruct\"`，如下所示：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hdA-xjjdS26J",
   "metadata": {
    "id": "hdA-xjjdS26J"
   },
   "outputs": [],
   "source": [
    "# to free up memory\n",
    "\n",
    "import gc\n",
    "\n",
    "del model\n",
    "\n",
    "gc.collect()  # Run Python garbage collector\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nbvAV7vaz6yc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "409470784b6346a981920350de4f6f28",
      "9ba6a11ffd194bf9a0900f52a7ed4d4f",
      "acae8bbbb4a84ed49be72fecd11fb052",
      "e8a4b441281b4038bb0204d093411f68",
      "bdf8b693821344fc97918e6cbc31c8bf",
      "97e8877869cd4be68ff38ce745be5045",
      "cc3da88e93c4499993b7bbb7d3064326",
      "0d51fdc2c416474da04079db6579890f",
      "c4598300a77b4667b1117f9499f5ccb7",
      "77606cd2fe1b4d33a91ede944bb1dec0",
      "f1ba439c26d64c90af2f162c74348405",
      "d598f094c3ce4daeab19fac8094cba7e",
      "0afc2d23514b45c9890b5d2ee4e6fa0b",
      "3da5d38bf3314d3eaa7cedebae41c076",
      "55e6b727a4594078beb3853cc1891308",
      "f17fa78263414ef8b414c7bf3ac03192",
      "e8b187b40ec14db3af17a380830a35bf",
      "e94ca32eaa9f4714a3b05a5fdf24d02b",
      "3edd464991204b8690eae02f10b4cc00",
      "ac1e34f4bd6c420bb6cc2fdde5f3ed4d",
      "1cd5e07cad35450182004952de32c8e7",
      "a63351a6715643378491ba831b3fb05d",
      "98b4680141ee423bb5e43c47613d8440",
      "b02ffefca3f34252914e76f4a8a467dc",
      "31d27bf34a74432f8e0dbfe9ecb76130",
      "a3137f3669b54e84be91010c9654d985",
      "5a2886564d3f40ceaa30b743dbe81f45",
      "15ea8fcfe097471e8fc9502a162f5904",
      "c779e80c50ba4434bfa1d326c5cc9b0f",
      "eb94612785e64552aea8674dc8647a93",
      "279cffe683fe4e7383062162e07ed9ed",
      "6176990205cc499f8995c71fc6b9d4df",
      "66c23ae98bcc45f18fc5c91e0e73c3e4",
      "05b502e1e3a9436297dafbb1ce7af722",
      "25977b0d89084703ad787fe9208b5aad",
      "71a84ee5fc964ec89ff2832c84735cc2",
      "6aed783eccb942318e6384e253ad4924",
      "84c34bfecda64391a609e19f131d51d4",
      "20ecac7c646b45938ed393cb20977c37",
      "ebe04aeaaac042aaaa0885992e45793d",
      "ca81071ab07446df96795a482ce0c630",
      "e0550cab24c7492787af40dc4b8576bf",
      "7015bf6f85954036aaf8cc4f1c44ea0f",
      "2a2ba3d065634484a932b8d3c212af56"
     ]
    },
    "id": "nbvAV7vaz6yc",
    "outputId": "9e1badc9-a6c4-48b7-9125-e0810655528b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7df6bbf8e63448c8a6cb5d2f6208403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  36%|###6      | 1.81G/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4772f31a1c5b4c168c9aabe7a1d2bacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad49eeb9e1204ea2bd2e371df8ccdea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951b9e81613a40a2a503f61e69677f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_weights = {}\n",
    "\n",
    "for i in range(1, 5):\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        filename=f\"model-0000{i}-of-00004.safetensors\",\n",
    "        local_dir=\"Llama-3-8B-Instruct\"\n",
    "    )\n",
    "    current_weights = load_file(weights_file)\n",
    "    combined_weights.update(current_weights)\n",
    "\n",
    "\n",
    "model = Llama3Model(LLAMA3_CONFIG_8B)\n",
    "load_weights_into_llama(model, LLAMA3_CONFIG_8B, combined_weights)\n",
    "model.to(device)\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VlH7qYVdDKQr",
   "metadata": {
    "id": "VlH7qYVdDKQr"
   },
   "source": [
    "- **请注意**，LLaMA 3 模型在使用时，**应当匹配微调时使用的正确 Prompt 模板**（详见 **第 7 章**）。  \n",
    "- 下面是一个 **基于 Meta AI LLaMA 3 专用 [ChatFormat 代码](https://github.com/meta-llama/llama3/blob/11817d47e1ba7a4959b025eb1ca308572e0e3963/llama/tokenizer.py#L202)** 的 **分词器包装类**，用于构造 **Prompt 模板**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4be5b481-1110-46e8-a931-3988d890cf8c",
   "metadata": {
    "id": "4be5b481-1110-46e8-a931-3988d890cf8c"
   },
   "outputs": [],
   "source": [
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "\n",
    "chat_tokenizer = ChatFormat(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M-dkSNvwDttN",
   "metadata": {
    "id": "M-dkSNvwDttN"
   },
   "source": [
    "- 用法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nwBrTGTsUNhn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwBrTGTsUNhn",
    "outputId": "72a495b4-b872-429a-88ef-49a9b4577f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128006, 882, 128007, 271, 9906, 4435, 0, 128009]\n"
     ]
    }
   ],
   "source": [
    "token_ids = chat_tokenizer.encode(\"Hello World!\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fpmpVgYVTRZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0fpmpVgYVTRZ",
    "outputId": "bb3e819a-112a-466c-ac51-5d14a9c3475b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>user<|end_header_id|>\\n\\nHello World!<|eot_id|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wo-aUGeKDvqq",
   "metadata": {
    "id": "Wo-aUGeKDvqq"
   },
   "source": [
    "- 现在，让我们看看 **LLaMA 3 指令模型** 的实际效果：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ozGOBu6XOkEW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozGOBu6XOkEW",
    "outputId": "4f689c70-bed9-46f3-a52a-aea47b641283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Llamas are herbivores, which means they primarily eat plants and plant-based foods. Here are some of the things llamas like to eat:\n",
      "\n",
      "1. Grass: Llamas love to graze on grass, especially in the spring and summer months.\n",
      "2. Hay: Hay is a staple in a llama's diet. They like to eat timothy hay, alfalfa hay, and other types of hay.\n",
      "3. Grains: Llamas may also be fed grains like oats, barley, and corn. However, grains should not make up more than 10-15% of a llama's diet.\n",
      "4. Fruits and vegetables: Llamas may enjoy fruits and vegetables as treats, such as\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"What do llamas eat?\", chat_tokenizer).to(device),\n",
    "    max_new_tokens=150,\n",
    "    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "\n",
    "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
    "    # Find the index of the first occurrence of \"<|end_header_id|>\"\n",
    "    index = text.find(header_end)\n",
    "\n",
    "    if index != -1:\n",
    "        # Return the substring starting after \"<|end_header_id|>\"\n",
    "        return text[index + len(header_end):].strip()  # Strip removes leading/trailing whitespace\n",
    "    else:\n",
    "        # If the token is not found, return the original text\n",
    "        return text\n",
    "\n",
    "print(\"Output text:\\n\", clean_text(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2r5JKrO-ZOHK",
   "metadata": {
    "id": "2r5JKrO-ZOHK"
   },
   "source": [
    "&nbsp;\n",
    "# Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QiQxX0XnP_iC",
   "metadata": {
    "id": "QiQxX0XnP_iC"
   },
   "source": [
    "- **在 LLaMA 3 初次发布几个月后**，Meta AI 推出了 **LLaMA 3.1** 系列模型（详细信息请参考官方博客 [《Introducing Llama 3.1: Our most capable models to date》](https://ai.meta.com/blog/meta-llama-3-1/)）。  \n",
    "- **值得庆幸的是，我们可以直接复用** 之前 **LLaMA 3 的代码** 来实现 **LLaMA 3.1 8B**。  \n",
    "\n",
    "<img src=\"../image/llama3-to-llama31.webp\" width=\"700px\">\n",
    "\n",
    "- **LLaMA 3.1** 采用 **与 LLaMA 3 完全相同的架构**，**唯一的改动** 是在 **配置文件** 中 **调整 RoPE 频率缩放参数**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "X5Fg8XUHMv4M",
   "metadata": {
    "id": "X5Fg8XUHMv4M"
   },
   "outputs": [],
   "source": [
    "LLAMA3_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,   # Vocabulary size\n",
    "    \"context_length\": 8192,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 14_336,    # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,        # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,  # The base in RoPE's \"theta\"\n",
    "    \"rope_freq\": None,       # Additional configuration for adjusting the RoPE frequencies\n",
    "    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n",
    "}\n",
    "\n",
    "LLAMA31_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # NEW: Larger supported context length\n",
    "    \"emb_dim\": 4096,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 32,             # Number of layers\n",
    "    \"hidden_dim\": 14_336,       # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # NEW: RoPE frequency scaling\n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ee464-c112-43b0-9ee8-70df6ac942d0",
   "metadata": {},
   "source": [
    "- **减少上下文长度** 以确保模型能在 **MacBook Air** 上正常运行（如果您的设备有 **更多 RAM**，可以 **取消注释** 下面的代码行）：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55a8769-1a03-4265-8fd0-15f1c423da53",
   "metadata": {
    "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "old_context_length = LLAMA31_CONFIG_8B[\"context_length\"]\n",
    "LLAMA31_CONFIG_8B[\"context_length\"] = 8192\n",
    "\n",
    "\n",
    "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
    "    scaling_factor = context_length_new / context_length_old\n",
    "    theta_new = theta_old * scaling_factor\n",
    "    return theta_new\n",
    "\n",
    "LLAMA31_CONFIG_8B[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA31_CONFIG_8B[\"rope_base\"],\n",
    "    old_context_length,\n",
    "    LLAMA31_CONFIG_8B[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"New RoPE theta:\", LLAMA31_CONFIG_8B[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xa3bpMDtTdBs",
   "metadata": {
    "id": "xa3bpMDtTdBs"
   },
   "source": [
    "- 如前面的代码所示，**RoPE 方法** 通过 **正弦（sine）和余弦（cosine）函数**，将 **位置信息** 直接嵌入 **注意力机制** 中。  \n",
    "- 在 **LLaMA 3.1** 中，我们通过 **额外的配置** 对 **逆频率计算（inverse frequency calculations）** 进行了 **进一步调整**。  \n",
    "- 这些调整会影响 **不同频率分量** 在 **位置编码中的贡献**（详细解释留待以后讨论）。  \n",
    "- 现在，让我们 **实际测试 LLaMA 3.1**，首先 **清理旧模型** 以 **释放 GPU 内存**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dUtYnNUOqhL",
   "metadata": {
    "id": "7dUtYnNUOqhL"
   },
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del model\n",
    "\n",
    "gc.collect()  # Run Python garbage collector\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DbbVsll6TYWR",
   "metadata": {
    "id": "DbbVsll6TYWR"
   },
   "source": [
    "- 接下来，我们 **下载分词器**。  \n",
    "- **请注意**，由于 **LLaMA 3.1** 与 **LLaMA 3** **属于不同的模型系列**，您需要访问 [meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) **仓库**，并 **接受许可协议**，否则您的 **Hugging Face 访问令牌** 无法用于下载。  \n",
    "- **提示**：为简化演示，我们下面只加载 **基础模型（base model）**，如果您希望使用 **指令微调版本**，请将 `\"meta-llama/Llama-3.1-8B\"` **替换为** `\"meta-llama/Llama-3.1-8B-Instruct\"`。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8xDk4chtPNU4",
   "metadata": {
    "id": "8xDk4chtPNU4"
   },
   "outputs": [],
   "source": [
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3.1-8B\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7l21VE4Otcs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7l21VE4Otcs",
    "outputId": "3dd5cfba-bf3f-44d2-9be1-7cd42bfe4ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,030,261,248\n"
     ]
    }
   ],
   "source": [
    "model = Llama3Model(LLAMA31_CONFIG_8B)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "u4J7IxOvOyPM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5bbaa046d8934c8fae0a12c3d7bd991b",
      "e1e4125eac004bae92dc1f22f673bf0e",
      "d5b4bb4891ec4e44be46e9815c7e10dc",
      "4f6595a392b244bd8e887935defc06f0",
      "100c1b15cc4046cea1147f657eb2d8d0",
      "81458e7953a349cfafccaa213b370406",
      "a3dc9dfadae642b4a873705596739468",
      "f55b59efcefa4ad5955d082f4bf7c637",
      "1b02e0c7d1604b1c87a327c4c4f8b0e7",
      "02ad170019454fd096b37347de5c481d",
      "c52e0f34892b4daa84c1bf61500ac399",
      "af985cf6fa26475eb2c4dd81e0c79ff4",
      "8659c3eddb014c3bb5931fd9e6fadad8",
      "f5fa00d96c4c49e48e1806d23a5b8570",
      "080c484114f64f5591fa1287a35b46c9",
      "14dc6a3717484c55a116612e28447dbb",
      "00d3286c9c1d4161bb777b7b65ae744d",
      "66f27fb11edf453b8144c2dfcdc66baa",
      "5798e5118430439fb1f6bf29e1bafe58",
      "357f367cf74146b8825be371acd51d06",
      "94073be250cd42d5b82e196e30cbf22e",
      "0cd0724f825e480389a82f0c49f91e6d",
      "dffa208978f34e6a9aae94ecda92fe67",
      "b8a98f163ebd4ac89af08a49c0881c23",
      "f0d9febe1a634a0ba7e8e50fa104dcc2",
      "e23870f0c7ff40cc8fa6a1e862a4af99",
      "87da9905a0534c26ad0712ad426ca930",
      "b953419300604b8e86fc0ad003fdfd2f",
      "f1865ed0fbcc40eeabdca90a43d00069",
      "ea0128909a9d4801ba312a876b0cf183",
      "d160986df978416c9ad91d1e10fc90fc",
      "5e97f7c2e8f5453dafcdad0552060e60",
      "4b3e7b8774df4b458bb6c6146fe3226d",
      "2ffd8dbed00e46d2887b9a2590cad297",
      "a06dcb3bdfc84905a7222066c32fe500",
      "e7602abc26714ee890a0cf5c0c7b67e1",
      "dc5d555099f64a998514ebde90eeb6df",
      "ef93a2f58cc54373941f43658bb808cf",
      "fea1e2327d2944859af3d91c216b9008",
      "320c00a5d18c45ccae634d166f1bd810",
      "6c857e69d5204cd3b7c3bf426993ad1f",
      "2145e47428f1446fba3e62b3cde0a7f5",
      "3d519ce3562c4e249bf392c7f43d04c0",
      "cc20ffcf0c1a4656945959bf457dfd84"
     ]
    },
    "id": "u4J7IxOvOyPM",
    "outputId": "925348d7-fc69-4d1b-90f1-7029426bcfcf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabfde3ef38b436ea750e6fb50a02b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e117ad45771747ae95c16f9876e6dc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170185f2f046437dab57c2ad23163c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e65f5d6c5af4ab78bc7b3778b98ef86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_weights = {}\n",
    "\n",
    "for i in range(1, 5):\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=\"meta-llama/Llama-3.1-8B\",\n",
    "        filename=f\"model-0000{i}-of-00004.safetensors\",\n",
    "        local_dir=\"Llama-3.1-8B\"\n",
    "    )\n",
    "    current_weights = load_file(weights_file)\n",
    "    combined_weights.update(current_weights)\n",
    "\n",
    "load_weights_into_llama(model, LLAMA31_CONFIG_8B, combined_weights)\n",
    "model.to(device);\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "wJFnF8ATPbtD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJFnF8ATPbtD",
    "outputId": "67d5cb66-3588-4fd4-ac75-39bfe3aa82d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA31_CONFIG_8B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DR9NBDUjPrDp",
   "metadata": {
    "id": "DR9NBDUjPrDp"
   },
   "source": [
    "&nbsp;\n",
    "# Llama 3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imoxFiDzJcxk",
   "metadata": {
    "id": "imoxFiDzJcxk"
   },
   "source": [
    "- 截至目前，**Meta AI 最新发布的模型** 是 **LLaMA 3.2** 系列，官方公告见 [这里](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)。  \n",
    "- **LLaMA 3.2 文本模型的代码** 与 **LLaMA 3.1** **基本相似**，但有以下 **两大变化**：\n",
    "  1. **模型尺寸缩小**，提供 **1B 和 3B 版本**（相比 LLaMA 3.1 8B 更轻量）。  \n",
    "  2. **重新引入权重共享（Weight Tying）**，这一方法最早应用于 **GPT-2 架构**，它 **复用输入（token）嵌入层和输出层的权重参数**，从而提高效率。  \n",
    "- **LLaMA 3.2 1B 版本体积较小，可在许多移动设备上运行**，极大提升了部署灵活性。  \n",
    "- **下图展示了 LLaMA 3.1 8B 与 LLaMA 3.2 1B 之间的架构差异**：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OL1EoXQ6TPb7",
   "metadata": {
    "id": "OL1EoXQ6TPb7"
   },
   "source": [
    "<img src=\"../image/llama31-to-llama32.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K0KgjwCCJ9Fb",
   "metadata": {
    "id": "K0KgjwCCJ9Fb"
   },
   "source": [
    "- 从上图可以看出，**LLaMA 3.1 8B** 与 **LLaMA 3.2 1B** **架构的主要区别** 在于 **模型尺寸**。  \n",
    "- **另一个小改动** 是 **RoPE 重新缩放因子（RoPE Rescaling Factor）增加**，这一变化已在 **配置文件** 中体现。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "Yv_yF3NCQTBx",
   "metadata": {
    "id": "Yv_yF3NCQTBx"
   },
   "outputs": [],
   "source": [
    "LLAMA31_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # NEW: Larger supported context length\n",
    "    \"emb_dim\": 4096,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 32,             # Number of layers\n",
    "    \"hidden_dim\": 14_336,       # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usagey\n",
    "    \"rope_freq\": {              # NEW: RoPE frequency scaling\n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "LLAMA32_CONFIG_1B = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 2048,            # NEW: Half the embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 16,             # NEW: Half the number of layers\n",
    "    \"hidden_dim\": 8192,         # NEW: Almost half the size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,         # NEW: Adjustment of the rescaling factor\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd351b-d883-460d-9cdc-47e15ddb884a",
   "metadata": {},
   "source": [
    "- **减少上下文长度** 以确保模型能在 **MacBook Air** 上正常运行（如果您的设备有 **更多 RAM**，可以 **取消注释** 下面的代码行）：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f001a6-7ae0-4204-aa83-a27a8878dfd2",
   "metadata": {
    "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "old_context_length = LLAMA32_CONFIG_1B[\"context_length\"]\n",
    "LLAMA32_CONFIG_1B[\"context_length\"] = 8192\n",
    "\n",
    "LLAMA32_CONFIG_1B[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA32_CONFIG_1B[\"rope_base\"],\n",
    "    old_context_length,\n",
    "    LLAMA32_CONFIG_1B[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"New RoPE theta:\", LLAMA32_CONFIG_1B[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dl4_0EoJKKYv",
   "metadata": {
    "id": "Dl4_0EoJKKYv"
   },
   "source": [
    "- 下面，我们可以 **复用 LLaMA 3.1 8B 章节的代码** 来加载 **LLaMA 3.2 1B** 模型。  \n",
    "- **请注意**，由于 **LLaMA 3.2** **属于不同的模型系列**，您需要访问 [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) **仓库**，并 **接受许可协议**，否则您的 **Hugging Face 访问令牌** 无法用于下载。  \n",
    "- **提示**：为简化演示，我们下面只加载 **基础模型（base model）**，如果您希望使用 **指令微调版本**，请将 `\"meta-llama/Llama-3.2-1B\"` **替换为** `\"meta-llama/Llama-3.2-1B-Instruct\"`。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tCstHgyRRD2x",
   "metadata": {
    "id": "tCstHgyRRD2x"
   },
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del model\n",
    "\n",
    "\n",
    "gc.collect()  # Run Python garbage collector\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "jt8BKAHXRCPI",
   "metadata": {
    "id": "jt8BKAHXRCPI"
   },
   "outputs": [],
   "source": [
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.2-1B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3.2-1B\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "uf8KjasmRFSt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf8KjasmRFSt",
    "outputId": "4e718852-2aa1-4b5a-bec3-3d5f866a4038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,498,482,688\n",
      "\n",
      "Total number of unique parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "model = Llama3Model(LLAMA32_CONFIG_1B)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9FbCIYW7RIOe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FbCIYW7RIOe",
    "outputId": "35588405-e2e1-4871-a1db-1d4bcb852e49"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c309c56a6cdf426e8ba7967b6a21864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uses weight tying.\n"
     ]
    }
   ],
   "source": [
    "weights_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.2-1B\",\n",
    "    filename=f\"model.safetensors\",\n",
    "    local_dir=\"Llama-3.2-1B\"\n",
    ")\n",
    "current_weights = load_file(weights_file)\n",
    "\n",
    "load_weights_into_llama(model, LLAMA32_CONFIG_1B, current_weights)\n",
    "model.to(device);\n",
    "del current_weights  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pPp5yjir6FYJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPp5yjir6FYJ",
    "outputId": "6c8e79d2-0769-43a7-93b3-f04c030e1aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight tying: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight tying:\", torch.equal(model.tok_emb.weight, model.out_head.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3kh7yrw2W4qr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kh7yrw2W4qr",
    "outputId": "b7e66a17-57ec-4b0e-c4ff-8d9a6b8e6ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort is made to ensure that the information on this website is accurate. However, we cannot guarantee that the information is accurate, complete\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA32_CONFIG_1B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VO4Qf0zyW1ZC",
   "metadata": {
    "id": "VO4Qf0zyW1ZC"
   },
   "source": [
    "&nbsp;\n",
    "# 展望"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CjCewpo2XPAd",
   "metadata": {
    "id": "CjCewpo2XPAd"
   },
   "source": [
    "- 本笔记本至此完成了 **从 GPT 到 LLaMA 3.2 的转换**。  \n",
    "- 如果您希望获取一个 **更紧凑、独立的 LLaMA 3.2 代码笔记本**，请参考 [standalone-llama32.ipynb](standalone-llama32.ipynb)。  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00148825ce0248b7a23eb28e3eca6749": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00d3286c9c1d4161bb777b7b65ae744d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02ad170019454fd096b37347de5c481d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05b502e1e3a9436297dafbb1ce7af722": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25977b0d89084703ad787fe9208b5aad",
       "IPY_MODEL_71a84ee5fc964ec89ff2832c84735cc2",
       "IPY_MODEL_6aed783eccb942318e6384e253ad4924"
      ],
      "layout": "IPY_MODEL_84c34bfecda64391a609e19f131d51d4"
     }
    },
    "080c484114f64f5591fa1287a35b46c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94073be250cd42d5b82e196e30cbf22e",
      "placeholder": "​",
      "style": "IPY_MODEL_0cd0724f825e480389a82f0c49f91e6d",
      "value": " 5.00G/5.00G [00:15&lt;00:00, 326MB/s]"
     }
    },
    "0a08fb81165748748ccb080e6df0600f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d49589118f5432cac49650251046429",
      "placeholder": "​",
      "style": "IPY_MODEL_f114549fe8ce49638a791ca2fecb2d89",
      "value": "model-00003-of-00004.safetensors: 100%"
     }
    },
    "0aa155b794a8426aa265f4a7670f43ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0afc2d23514b45c9890b5d2ee4e6fa0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8b187b40ec14db3af17a380830a35bf",
      "placeholder": "​",
      "style": "IPY_MODEL_e94ca32eaa9f4714a3b05a5fdf24d02b",
      "value": "model-00002-of-00004.safetensors: 100%"
     }
    },
    "0cd0724f825e480389a82f0c49f91e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d51fdc2c416474da04079db6579890f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "100c1b15cc4046cea1147f657eb2d8d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14dc6a3717484c55a116612e28447dbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15ea8fcfe097471e8fc9502a162f5904": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "172c0c6955e1428b999dcb2d133704cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b02e0c7d1604b1c87a327c4c4f8b0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1bf7108774c34016a2193e2cd7639b7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cd5e07cad35450182004952de32c8e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1da83719e47c4196b06f3aa32056b560": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20ecac7c646b45938ed393cb20977c37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2145e47428f1446fba3e62b3cde0a7f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25977b0d89084703ad787fe9208b5aad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20ecac7c646b45938ed393cb20977c37",
      "placeholder": "​",
      "style": "IPY_MODEL_ebe04aeaaac042aaaa0885992e45793d",
      "value": "model-00004-of-00004.safetensors: 100%"
     }
    },
    "279cffe683fe4e7383062162e07ed9ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a2ba3d065634484a932b8d3c212af56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ffd8dbed00e46d2887b9a2590cad297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a06dcb3bdfc84905a7222066c32fe500",
       "IPY_MODEL_e7602abc26714ee890a0cf5c0c7b67e1",
       "IPY_MODEL_dc5d555099f64a998514ebde90eeb6df"
      ],
      "layout": "IPY_MODEL_ef93a2f58cc54373941f43658bb808cf"
     }
    },
    "31d27bf34a74432f8e0dbfe9ecb76130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb94612785e64552aea8674dc8647a93",
      "max": 4915916176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_279cffe683fe4e7383062162e07ed9ed",
      "value": 4915916176
     }
    },
    "320c00a5d18c45ccae634d166f1bd810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "357f367cf74146b8825be371acd51d06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3cea4b431147441a8d9bd872811d5974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d519ce3562c4e249bf392c7f43d04c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3da5d38bf3314d3eaa7cedebae41c076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3edd464991204b8690eae02f10b4cc00",
      "max": 4999802720,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac1e34f4bd6c420bb6cc2fdde5f3ed4d",
      "value": 4999802720
     }
    },
    "3edd464991204b8690eae02f10b4cc00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409470784b6346a981920350de4f6f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ba6a11ffd194bf9a0900f52a7ed4d4f",
       "IPY_MODEL_acae8bbbb4a84ed49be72fecd11fb052",
       "IPY_MODEL_e8a4b441281b4038bb0204d093411f68"
      ],
      "layout": "IPY_MODEL_bdf8b693821344fc97918e6cbc31c8bf"
     }
    },
    "4b3e7b8774df4b458bb6c6146fe3226d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f6595a392b244bd8e887935defc06f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ad170019454fd096b37347de5c481d",
      "placeholder": "​",
      "style": "IPY_MODEL_c52e0f34892b4daa84c1bf61500ac399",
      "value": " 4.98G/4.98G [00:16&lt;00:00, 316MB/s]"
     }
    },
    "519147a10b984befbd0f255f78c1f66a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55e6b727a4594078beb3853cc1891308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cd5e07cad35450182004952de32c8e7",
      "placeholder": "​",
      "style": "IPY_MODEL_a63351a6715643378491ba831b3fb05d",
      "value": " 5.00G/5.00G [00:16&lt;00:00, 291MB/s]"
     }
    },
    "562e82438dbe41b793ff488b8447c5bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5798e5118430439fb1f6bf29e1bafe58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c9dec75a3346b1b787f88dd510d254": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a2886564d3f40ceaa30b743dbe81f45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bbaa046d8934c8fae0a12c3d7bd991b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1e4125eac004bae92dc1f22f673bf0e",
       "IPY_MODEL_d5b4bb4891ec4e44be46e9815c7e10dc",
       "IPY_MODEL_4f6595a392b244bd8e887935defc06f0"
      ],
      "layout": "IPY_MODEL_100c1b15cc4046cea1147f657eb2d8d0"
     }
    },
    "5e97f7c2e8f5453dafcdad0552060e60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6022a9426683420690d9b41a0ca4f870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9492edc02dee456f840325d913fa4e4f",
      "placeholder": "​",
      "style": "IPY_MODEL_66dc94b23556499f985f8accbb1f89cb",
      "value": "model-00001-of-00004.safetensors: 100%"
     }
    },
    "603690f543114a7fb6aebd433c80bdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0aa155b794a8426aa265f4a7670f43ad",
      "max": 4915916176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a06fbde549cc47fdaddfbdb82d35d823",
      "value": 4915916176
     }
    },
    "616e383bb3d442bcb6edb2721a8180b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ba9f009e92a46fcbcbb401dc444f12e",
      "placeholder": "​",
      "style": "IPY_MODEL_d74186bb74d142dfb683fa347b6990f7",
      "value": " 5.00G/5.00G [00:16&lt;00:00, 305MB/s]"
     }
    },
    "6176990205cc499f8995c71fc6b9d4df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66c23ae98bcc45f18fc5c91e0e73c3e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66dc94b23556499f985f8accbb1f89cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66f27fb11edf453b8144c2dfcdc66baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6aed783eccb942318e6384e253ad4924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7015bf6f85954036aaf8cc4f1c44ea0f",
      "placeholder": "​",
      "style": "IPY_MODEL_2a2ba3d065634484a932b8d3c212af56",
      "value": " 1.17G/1.17G [00:04&lt;00:00, 297MB/s]"
     }
    },
    "6c857e69d5204cd3b7c3bf426993ad1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7015bf6f85954036aaf8cc4f1c44ea0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71a84ee5fc964ec89ff2832c84735cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca81071ab07446df96795a482ce0c630",
      "max": 1168138808,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0550cab24c7492787af40dc4b8576bf",
      "value": 1168138808
     }
    },
    "7266a729edfb4a44b5b1c67dc79be146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76dbab4873f342019c5d7624ae2c9775": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "773b802daed942f5a11f3eab3b83be08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_172c0c6955e1428b999dcb2d133704cd",
      "placeholder": "​",
      "style": "IPY_MODEL_1bf7108774c34016a2193e2cd7639b7d",
      "value": " 4.92G/4.92G [00:16&lt;00:00, 297MB/s]"
     }
    },
    "77606cd2fe1b4d33a91ede944bb1dec0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7989003a613e45f780d3f800e121543a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c6658cfff1a4d27af3de148184f77d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80dfd3e80ceb444a83ec1fd65f9af80e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81458e7953a349cfafccaa213b370406": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84c34bfecda64391a609e19f131d51d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8659c3eddb014c3bb5931fd9e6fadad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00d3286c9c1d4161bb777b7b65ae744d",
      "placeholder": "​",
      "style": "IPY_MODEL_66f27fb11edf453b8144c2dfcdc66baa",
      "value": "model-00002-of-00004.safetensors: 100%"
     }
    },
    "87da9905a0534c26ad0712ad426ca930": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87f474861e54432e9d533e0a89bb77da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ae98969541849efa356cf912ac39b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9373112649945e3b446c3e1ec274dc1",
       "IPY_MODEL_d49791082a304ade95c185c79fae1f41",
       "IPY_MODEL_616e383bb3d442bcb6edb2721a8180b6"
      ],
      "layout": "IPY_MODEL_87f474861e54432e9d533e0a89bb77da"
     }
    },
    "8ba9f009e92a46fcbcbb401dc444f12e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94073be250cd42d5b82e196e30cbf22e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9492edc02dee456f840325d913fa4e4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97e8877869cd4be68ff38ce745be5045": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98b4680141ee423bb5e43c47613d8440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b02ffefca3f34252914e76f4a8a467dc",
       "IPY_MODEL_31d27bf34a74432f8e0dbfe9ecb76130",
       "IPY_MODEL_a3137f3669b54e84be91010c9654d985"
      ],
      "layout": "IPY_MODEL_5a2886564d3f40ceaa30b743dbe81f45"
     }
    },
    "9ba6a11ffd194bf9a0900f52a7ed4d4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97e8877869cd4be68ff38ce745be5045",
      "placeholder": "​",
      "style": "IPY_MODEL_cc3da88e93c4499993b7bbb7d3064326",
      "value": "model-00001-of-00004.safetensors: 100%"
     }
    },
    "9bb60a5a3710463ebe3a17f8d2a446be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a08fb81165748748ccb080e6df0600f",
       "IPY_MODEL_603690f543114a7fb6aebd433c80bdc3",
       "IPY_MODEL_773b802daed942f5a11f3eab3b83be08"
      ],
      "layout": "IPY_MODEL_7989003a613e45f780d3f800e121543a"
     }
    },
    "9d49589118f5432cac49650251046429": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a06dcb3bdfc84905a7222066c32fe500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fea1e2327d2944859af3d91c216b9008",
      "placeholder": "​",
      "style": "IPY_MODEL_320c00a5d18c45ccae634d166f1bd810",
      "value": "model-00004-of-00004.safetensors: 100%"
     }
    },
    "a06fbde549cc47fdaddfbdb82d35d823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3137f3669b54e84be91010c9654d985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6176990205cc499f8995c71fc6b9d4df",
      "placeholder": "​",
      "style": "IPY_MODEL_66c23ae98bcc45f18fc5c91e0e73c3e4",
      "value": " 4.92G/4.92G [00:16&lt;00:00, 297MB/s]"
     }
    },
    "a3dc9dfadae642b4a873705596739468": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a63351a6715643378491ba831b3fb05d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac1e34f4bd6c420bb6cc2fdde5f3ed4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "acae8bbbb4a84ed49be72fecd11fb052": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d51fdc2c416474da04079db6579890f",
      "max": 4976698672,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4598300a77b4667b1117f9499f5ccb7",
      "value": 4976698672
     }
    },
    "af985cf6fa26475eb2c4dd81e0c79ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8659c3eddb014c3bb5931fd9e6fadad8",
       "IPY_MODEL_f5fa00d96c4c49e48e1806d23a5b8570",
       "IPY_MODEL_080c484114f64f5591fa1287a35b46c9"
      ],
      "layout": "IPY_MODEL_14dc6a3717484c55a116612e28447dbb"
     }
    },
    "b02ffefca3f34252914e76f4a8a467dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15ea8fcfe097471e8fc9502a162f5904",
      "placeholder": "​",
      "style": "IPY_MODEL_c779e80c50ba4434bfa1d326c5cc9b0f",
      "value": "model-00003-of-00004.safetensors: 100%"
     }
    },
    "b46a08cf4929422eb0f76d8d9af11249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1da83719e47c4196b06f3aa32056b560",
      "max": 1168138808,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4a2c88326d14fbca87cfde073755a2e",
      "value": 1168138808
     }
    },
    "b8a98f163ebd4ac89af08a49c0881c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b953419300604b8e86fc0ad003fdfd2f",
      "placeholder": "​",
      "style": "IPY_MODEL_f1865ed0fbcc40eeabdca90a43d00069",
      "value": "model-00003-of-00004.safetensors: 100%"
     }
    },
    "b953419300604b8e86fc0ad003fdfd2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdf8b693821344fc97918e6cbc31c8bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be3e9bf271f04eb0b119659e1af3a0ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4598300a77b4667b1117f9499f5ccb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4a2c88326d14fbca87cfde073755a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c52e0f34892b4daa84c1bf61500ac399": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c779e80c50ba4434bfa1d326c5cc9b0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca81071ab07446df96795a482ce0c630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc20ffcf0c1a4656945959bf457dfd84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc3da88e93c4499993b7bbb7d3064326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d160986df978416c9ad91d1e10fc90fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d49791082a304ade95c185c79fae1f41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00148825ce0248b7a23eb28e3eca6749",
      "max": 4999802720,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1a9b0c2431640298a6c1b258298b12d",
      "value": 4999802720
     }
    },
    "d598f094c3ce4daeab19fac8094cba7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0afc2d23514b45c9890b5d2ee4e6fa0b",
       "IPY_MODEL_3da5d38bf3314d3eaa7cedebae41c076",
       "IPY_MODEL_55e6b727a4594078beb3853cc1891308"
      ],
      "layout": "IPY_MODEL_f17fa78263414ef8b414c7bf3ac03192"
     }
    },
    "d5b4bb4891ec4e44be46e9815c7e10dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f55b59efcefa4ad5955d082f4bf7c637",
      "max": 4976698672,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b02e0c7d1604b1c87a327c4c4f8b0e7",
      "value": 4976698672
     }
    },
    "d74186bb74d142dfb683fa347b6990f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc5d555099f64a998514ebde90eeb6df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d519ce3562c4e249bf392c7f43d04c0",
      "placeholder": "​",
      "style": "IPY_MODEL_cc20ffcf0c1a4656945959bf457dfd84",
      "value": " 1.17G/1.17G [00:03&lt;00:00, 328MB/s]"
     }
    },
    "dffa208978f34e6a9aae94ecda92fe67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b8a98f163ebd4ac89af08a49c0881c23",
       "IPY_MODEL_f0d9febe1a634a0ba7e8e50fa104dcc2",
       "IPY_MODEL_e23870f0c7ff40cc8fa6a1e862a4af99"
      ],
      "layout": "IPY_MODEL_87da9905a0534c26ad0712ad426ca930"
     }
    },
    "e0550cab24c7492787af40dc4b8576bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1e4125eac004bae92dc1f22f673bf0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81458e7953a349cfafccaa213b370406",
      "placeholder": "​",
      "style": "IPY_MODEL_a3dc9dfadae642b4a873705596739468",
      "value": "model-00001-of-00004.safetensors: 100%"
     }
    },
    "e23870f0c7ff40cc8fa6a1e862a4af99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e97f7c2e8f5453dafcdad0552060e60",
      "placeholder": "​",
      "style": "IPY_MODEL_4b3e7b8774df4b458bb6c6146fe3226d",
      "value": " 4.92G/4.92G [00:20&lt;00:00, 317MB/s]"
     }
    },
    "e7602abc26714ee890a0cf5c0c7b67e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c857e69d5204cd3b7c3bf426993ad1f",
      "max": 1168138808,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2145e47428f1446fba3e62b3cde0a7f5",
      "value": 1168138808
     }
    },
    "e805bb6dfee34dab8870f4618d8bffdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8a4b441281b4038bb0204d093411f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77606cd2fe1b4d33a91ede944bb1dec0",
      "placeholder": "​",
      "style": "IPY_MODEL_f1ba439c26d64c90af2f162c74348405",
      "value": " 4.98G/4.98G [00:16&lt;00:00, 296MB/s]"
     }
    },
    "e8b187b40ec14db3af17a380830a35bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e94ca32eaa9f4714a3b05a5fdf24d02b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9aba3d53b4d45c485a7aad649c7b465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c6658cfff1a4d27af3de148184f77d9",
      "max": 4976698672,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7266a729edfb4a44b5b1c67dc79be146",
      "value": 4976698672
     }
    },
    "ea0128909a9d4801ba312a876b0cf183": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb94612785e64552aea8674dc8647a93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebe04aeaaac042aaaa0885992e45793d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed28e180d94a4b7aa548581612e31232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff4338faded5494da1ccb660e1c441ed",
       "IPY_MODEL_b46a08cf4929422eb0f76d8d9af11249",
       "IPY_MODEL_f049eb4a50f54c34912ca959d2eaf353"
      ],
      "layout": "IPY_MODEL_80dfd3e80ceb444a83ec1fd65f9af80e"
     }
    },
    "ef93a2f58cc54373941f43658bb808cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f049eb4a50f54c34912ca959d2eaf353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0ab5a46cbb0444c88ed137d8a95002b",
      "placeholder": "​",
      "style": "IPY_MODEL_f8f28ac0e149428f9fef42373c6a87d0",
      "value": " 1.17G/1.17G [00:03&lt;00:00, 307MB/s]"
     }
    },
    "f0ab5a46cbb0444c88ed137d8a95002b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0d9febe1a634a0ba7e8e50fa104dcc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea0128909a9d4801ba312a876b0cf183",
      "max": 4915916176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d160986df978416c9ad91d1e10fc90fc",
      "value": 4915916176
     }
    },
    "f114549fe8ce49638a791ca2fecb2d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f17fa78263414ef8b414c7bf3ac03192": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1865ed0fbcc40eeabdca90a43d00069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1a12d7929db4309b9881853135359fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76dbab4873f342019c5d7624ae2c9775",
      "placeholder": "​",
      "style": "IPY_MODEL_3cea4b431147441a8d9bd872811d5974",
      "value": " 4.98G/4.98G [00:16&lt;00:00, 309MB/s]"
     }
    },
    "f1a9b0c2431640298a6c1b258298b12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1ba439c26d64c90af2f162c74348405": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3788acce34f4956b0727b58d0cf38c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6022a9426683420690d9b41a0ca4f870",
       "IPY_MODEL_e9aba3d53b4d45c485a7aad649c7b465",
       "IPY_MODEL_f1a12d7929db4309b9881853135359fc"
      ],
      "layout": "IPY_MODEL_58c9dec75a3346b1b787f88dd510d254"
     }
    },
    "f55b59efcefa4ad5955d082f4bf7c637": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5fa00d96c4c49e48e1806d23a5b8570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5798e5118430439fb1f6bf29e1bafe58",
      "max": 4999802720,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_357f367cf74146b8825be371acd51d06",
      "value": 4999802720
     }
    },
    "f8f28ac0e149428f9fef42373c6a87d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9373112649945e3b446c3e1ec274dc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e805bb6dfee34dab8870f4618d8bffdb",
      "placeholder": "​",
      "style": "IPY_MODEL_be3e9bf271f04eb0b119659e1af3a0ea",
      "value": "model-00002-of-00004.safetensors: 100%"
     }
    },
    "fea1e2327d2944859af3d91c216b9008": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff4338faded5494da1ccb660e1c441ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_519147a10b984befbd0f255f78c1f66a",
      "placeholder": "​",
      "style": "IPY_MODEL_562e82438dbe41b793ff488b8447c5bf",
      "value": "model-00004-of-00004.safetensors: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/07_gpt_to_llama/standalone-llama32.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c",
   "metadata": {
    "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
   },
   "source": [
    "# Llama 3.2 从零构建（独立笔记本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
   "metadata": {
    "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
   },
   "source": [
    "## 本笔记本简介\n",
    "\n",
    "本笔记本经过精简设计，重点关注实现 **Llama 3.2 1B 和 3B** 大型语言模型（LLM）的代码实现。\n",
    "\n",
    "### 相关笔记本\n",
    "如果你希望详细了解各个组件以及 **GPT、Llama 2 和 Llama 3** 之间的关系，请参考以下配套笔记本：\n",
    "\n",
    "- [从零构建 GPT 并转换为 Llama 2](converting-gpt-to-llama2.ipynb)\n",
    "- [从零实现 Llama 2 并升级至 Llama 3.2](converting-llama2-to-llama3.ipynb)\n",
    "\n",
    "<img src=\"../image/llama32.webp\" width=\"700px\">\n",
    "\n",
    "---\n",
    "\n",
    "### 代码说明\n",
    "- **所有代码均为本人编写**，旨在将 **Llama 3 架构** 映射到我在 [《从零构建大型语言模型》](http://mng.bz/orYv) 一书中实现的模型代码。代码遵循 **Apache 2.0 开源许可协议**（参见 [LICENSE.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt)）。\n",
    "- **分词器（Tokenizer）** 代码借鉴了 Meta AI 官方 [Llama 3 分词器](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py) 的设计，该分词器对 **Tiktoken GPT-4 分词器** 进行了扩展。\n",
    "- **RoPE 重新缩放（Rescaling）** 部分参考了 `transformers` 库中的 [`_compute_llama3_parameters` 函数](https://github.com/huggingface/transformers/blob/5c1027bf09717f664b579e01cbb8ec3ef5aeb140/src/transformers/modeling_rope_utils.py#L329-L347)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c201adb-747e-437b-9a62-442802941e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
    "outputId": "4f762354-e0a3-4cc2-e5d4-e61a227a202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.25.2\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         # to download pretrained weights\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
   },
   "source": [
    "# 架构代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Frequency adjustments\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,\n",
    "            rope_base=10_000,\n",
    "            rope_config=None,\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # Fetch buffers using SharedBuffers\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape queries, keys, and values\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        # Transpose keys, values, and queries\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_query_groups, num_tokens, head_dim)\n",
    "\n",
    "        # Apply RoPE\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        # Expand keys and values to match the number of heads\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # For example, before repeat_interleave along dim=1 (query groups):\n",
    "        #   [K1, K2]\n",
    "        # After repeat_interleave (each query group is repeated group_size times):\n",
    "        #   [K1, K1, K2, K2]\n",
    "        # If we used regular repeat instead of repeat_interleave, we'd get:\n",
    "        #   [K1, K2, K1, K2]\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            rope_base=cfg[\"rope_base\"],\n",
    "            rope_config=cfg[\"rope_freq\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "&nbsp;\n",
    "# 2. 模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dea40c-fe20-4a75-be25-d6fce5863c01",
   "metadata": {
    "id": "23dea40c-fe20-4a75-be25-d6fce5863c01"
   },
   "source": [
    "## 本笔记本的剩余部分使用 Llama 3.2 1B 模型  \n",
    "如果需要使用 **3B 版本**，请取消注释以下代码单元中的第二个配置文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "# Llama 3.2 1B\n",
    "\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 2048,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 16,             # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Llama 3.2 3B\n",
    "\n",
    "# LLAMA32_CONFIG = {\n",
    "#     \"vocab_size\": 128_256,      # Vocabulary size\n",
    "#     \"context_length\": 131_072,  # Context length\n",
    "#     \"emb_dim\": 3072,            # Embedding dimension\n",
    "#     \"n_heads\": 24,              # Number of attention heads\n",
    "#     \"n_layers\": 28,             # Number of layers\n",
    "#     \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "#     \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "#     \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "#     \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "#     \"rope_freq\": {              # RoPE frequency scaling\n",
    "#         \"factor\": 32.0,\n",
    "#         \"low_freq_factor\": 1.0,\n",
    "#         \"high_freq_factor\": 4.0,\n",
    "#         \"original_context_length\": 8192,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34535172-797e-4dd0-84fb-65bc75ad5b06",
   "metadata": {
    "id": "34535172-797e-4dd0-84fb-65bc75ad5b06"
   },
   "source": [
    "## 调整上下文长度以适配 MacBook Air\n",
    "\n",
    "为了确保模型能够在 **MacBook Air** 等低内存设备上流畅运行，可以 **减少上下文长度（context length）**。如果你的设备拥有 **更大的 RAM**，可以选择 **注释掉** 相关代码，允许更长的上下文。\n",
    "\n",
    "### ✂️ **调整代码**\n",
    "```python\n",
    "# 调整上下文长度以减少内存占用\n",
    "context_length = min(context_length, 512)  # MacBook Air 适配\n",
    "\n",
    "# 如果你的设备内存充足，可以注释掉上述代码：\n",
    "# context_length = 原始设置值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c",
   "metadata": {
    "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "old_context_length = LLAMA32_CONFIG[\"context_length\"]\n",
    "LLAMA32_CONFIG[\"context_length\"] = 8192\n",
    "\n",
    "\n",
    "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
    "    scaling_factor = context_length_new / context_length_old\n",
    "    theta_new = theta_old * scaling_factor\n",
    "    return theta_new\n",
    "\n",
    "LLAMA32_CONFIG[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA32_CONFIG[\"rope_base\"],\n",
    "    old_context_length,\n",
    "    LLAMA32_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"New RoPE theta:\", LLAMA32_CONFIG[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [],
   "source": [
    "model = Llama3Model(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1",
   "metadata": {
    "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1"
   },
   "source": [
    "## 确认缓冲区复用（Buffer Reuse）\n",
    "\n",
    "以下代码应当打印 `True`，以验证缓冲区被正确复用，而不是重复创建（避免不必要的资源浪费）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e95db6d-2712-41a5-a5e0-86c49897f4cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e95db6d-2712-41a5-a5e0-86c49897f4cf",
    "outputId": "8efc4937-e616-40d0-cd59-670d7eb3e841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check buffers\n",
    "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
    "print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
    "print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,498,482,688\n",
      "\n",
      "Total number of unique parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 11.42 GB\n",
      "bfloat16: 5.71 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e091e1-afa8-4d23-9aea-cced86181bfd",
   "metadata": {
    "id": "78e091e1-afa8-4d23-9aea-cced86181bfd"
   },
   "source": [
    "# 3. 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77",
   "metadata": {
    "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.model.decode(tokens)\n",
    "\n",
    "\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self.tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771b60c-c198-4b30-bf10-42031197ae86",
   "metadata": {
    "id": "b771b60c-c198-4b30-bf10-42031197ae86"
   },
   "source": [
    "## 下载 Llama 3.2：授权与访问令牌配置\n",
    "\n",
    "在下载 Llama 3.2 相关文件之前，你需要完成 **Meta AI 许可协议** 的接受流程，并创建 Hugging Face Hub 账户。具体步骤如下：\n",
    "\n",
    "### 1️. 接受 Llama 3.2 许可协议  \n",
    "Meta AI 要求用户在下载文件前接受 Llama 3.2 的许可条款。你需要：\n",
    "- 访问 [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) 仓库  \n",
    "- 登录 Hugging Face Hub 账户（如无账户，需先注册）  \n",
    "- 在页面上 **接受许可条款**  \n",
    "\n",
    "### 2️. 生成 Hugging Face 访问令牌  \n",
    "为了下载模型，你需要创建一个 **访问令牌（Access Token）** 并授予 **READ** 权限：\n",
    "\n",
    "**步骤**：\n",
    "1. 点击 Hugging Face Hub 右上角的 **个人头像**，然后选择 **“Settings”**  \n",
    "   \n",
    "   ![Hugging Face 设置](../image/settings.webp)\n",
    "\n",
    "2. 进入 **Access Tokens** 页面，创建新的令牌，并设置权限为 **READ**  \n",
    "3. **复制生成的访问令牌**，以便后续粘贴到代码单元格中  \n",
    "\n",
    "   ![访问令牌生成](../image/access-token.webp)\n",
    "\n",
    "**接下来，你可以将访问令牌粘贴到相应的代码单元格，完成身份验证！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed",
    "outputId": "e6e6dc05-7330-45bc-a9a7-331919155bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "a1608feac06d4687967a3e398f01c489",
      "518fb202e4b44aaba47f07d1a61b6762",
      "672cdc5aea954de3af851c001a667ad3",
      "eebf8874618746b39cf4a21a2728dc7f",
      "5176834aa8784bba9ec21234b87a8948",
      "e2dc407afcd945c798e30597fddfcb3c",
      "0dccd57dcc5c43a588157cef957c07e8",
      "33ca0cdf2c7f41598a381c4ebe6a4ee1",
      "ee44487f58454dacb522b1e084ffb733",
      "d2c41e71a3f441deaed091b620ac5603",
      "3326b6141a1a4eba9f316df528a9b99a"
     ]
    },
    "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
    "outputId": "5dd7334b-4c71-465a-94d2-c3e95b9ddc58"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "_gBhxDtU_nxo",
   "metadata": {
    "id": "_gBhxDtU_nxo"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_file_path)\n",
    "chat_tokenizer = ChatFormat(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172f89f-d301-439f-b809-46169e5f5945",
   "metadata": {
    "id": "c172f89f-d301-439f-b809-46169e5f5945"
   },
   "source": [
    "# 4. 加载预训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75166128-5899-4995-9b88-9672e135650e",
   "metadata": {
    "id": "75166128-5899-4995-9b88-9672e135650e"
   },
   "outputs": [],
   "source": [
    "def assign(left, right, tensor_name=\"unknown\"):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params.keys():\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "        print(\"Model uses weight tying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9881b6995c3f49dc89e6992fd9ab660b",
      "17a3174e65c54476b2e0d1faf8f011ca",
      "1bbf2e62c0754d1593beb4105a7f1ac1",
      "b82112e1dec645d98aa1c1ba64abcb61",
      "271e2bd6a35e4a8b92de8697f7c0be5f",
      "90a79523187446dfa692723b2e5833a7",
      "431ffb83b8c14bf182f0430e07ea6154",
      "a8f1b72a33dd4b548de23fbd95e0da18",
      "25cc36132d384189acfbecc59483134b",
      "bfd06423ad544218968648016e731a46",
      "d029630b63ff44cf807ade428d2eb421"
     ]
    },
    "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
    "outputId": "55b2f28c-142f-4698-9d23-d27456d3ed6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uses weight tying.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "if LLAMA_SIZE_STR == \"1B\":\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "        filename=f\"model.safetensors\",\n",
    "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "    )\n",
    "    combined_weights = load_file(weights_file)\n",
    "\n",
    "\n",
    "else:\n",
    "    combined_weights = {}\n",
    "    for i in range(1, 3):\n",
    "        weights_file = hf_hub_download(\n",
    "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "            filename=f\"model-0000{i}-of-00002.safetensors\",\n",
    "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "        )\n",
    "        current_weights = load_file(weights_file)\n",
    "        combined_weights.update(current_weights)\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
    "model.to(device)\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37",
   "metadata": {
    "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight tying: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight tying:\", torch.equal(model.tok_emb.weight, model.out_head.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07df1-4401-4792-b549-7c4cc5632323",
   "metadata": {
    "id": "57d07df1-4401-4792-b549-7c4cc5632323"
   },
   "source": [
    "# 5. 生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
   "metadata": {
    "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
   },
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
   "metadata": {
    "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Llamas are herbivores, which means they primarily eat plants. Their diet consists mainly of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses and grassy meadows.\n",
      "2. Hay: Llamas also eat hay, which is a dry, compressed form of grass or other plants.\n",
      "3. Alfalfa: Alfalfa is a legume that is commonly fed to llamas. It is high in protein and fiber.\n",
      "4. Other plants: Llamas will also eat other plants, such as wild grasses, shrubs, and trees.\n",
      "\n",
      "It's worth noting that the diet of llamas can vary depending on the region, climate,\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"What do llamas eat?\"\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(PROMPT, chat_tokenizer).to(device),\n",
    "    max_new_tokens=150,\n",
    "    context_size=LLAMA32_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "\n",
    "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
    "    # Find the index of the first occurrence of \"<|end_header_id|>\"\n",
    "    index = text.find(header_end)\n",
    "\n",
    "    if index != -1:\n",
    "        # Return the substring starting after \"<|end_header_id|>\"\n",
    "        return text[index + len(header_end):].strip()  # Strip removes leading/trailing whitespace\n",
    "    else:\n",
    "        # If the token is not found, return the original text\n",
    "        return text\n",
    "\n",
    "print(\"Output text:\\n\", clean_text(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549324d6-5c71-4147-ae21-2e67675faa3d",
   "metadata": {
    "id": "549324d6-5c71-4147-ae21-2e67675faa3d"
   },
   "source": [
    "&nbsp;\n",
    "# 展望"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c",
   "metadata": {
    "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c"
   },
   "source": [
    "## LLM 教程：从零构建与架构转换\n",
    "\n",
    "本笔记本内容经过精心简化，以便聚焦核心概念。如果你想深入了解各组件的具体实现，可以参考以下配套笔记本：\n",
    "\n",
    "![GPT 与 Llama 变换架构](../image/gpt-and-all-llamas.webp)\n",
    "\n",
    "1. [从零构建 GPT 并转换为 Llama 2](converting-gpt-to-llama2.ipynb)  \n",
    "2. [从零实现 Llama 2 并升级至 Llama 3.2](converting-llama2-to-llama3.ipynb)  \n",
    "\n",
    "如果你希望系统学习如何 **从零构建大型语言模型（LLM）**，并深入理解其内部机制，可以参考我的完整指南：  \n",
    "[📘 《从零构建大型语言模型》](http://mng.bz/orYv)\n",
    "\n",
    "[![书籍封面](../image/cover-small.webp)](http://mng.bz/orYv)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dccd57dcc5c43a588157cef957c07e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "17a3174e65c54476b2e0d1faf8f011ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_90a79523187446dfa692723b2e5833a7",
      "placeholder": "​",
      "style": "IPY_MODEL_431ffb83b8c14bf182f0430e07ea6154",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors:  35%"
     }
    },
    "1bbf2e62c0754d1593beb4105a7f1ac1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a8f1b72a33dd4b548de23fbd95e0da18",
      "max": 2471645608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25cc36132d384189acfbecc59483134b",
      "tabbable": null,
      "tooltip": null,
      "value": 880803840
     }
    },
    "25cc36132d384189acfbecc59483134b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "271e2bd6a35e4a8b92de8697f7c0be5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3326b6141a1a4eba9f316df528a9b99a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "33ca0cdf2c7f41598a381c4ebe6a4ee1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "431ffb83b8c14bf182f0430e07ea6154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "5176834aa8784bba9ec21234b87a8948": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "518fb202e4b44aaba47f07d1a61b6762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e2dc407afcd945c798e30597fddfcb3c",
      "placeholder": "​",
      "style": "IPY_MODEL_0dccd57dcc5c43a588157cef957c07e8",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.model: 100%"
     }
    },
    "672cdc5aea954de3af851c001a667ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_33ca0cdf2c7f41598a381c4ebe6a4ee1",
      "max": 2183982,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee44487f58454dacb522b1e084ffb733",
      "tabbable": null,
      "tooltip": null,
      "value": 2183982
     }
    },
    "90a79523187446dfa692723b2e5833a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9881b6995c3f49dc89e6992fd9ab660b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17a3174e65c54476b2e0d1faf8f011ca",
       "IPY_MODEL_1bbf2e62c0754d1593beb4105a7f1ac1",
       "IPY_MODEL_b82112e1dec645d98aa1c1ba64abcb61"
      ],
      "layout": "IPY_MODEL_271e2bd6a35e4a8b92de8697f7c0be5f",
      "tabbable": null,
      "tooltip": null
     }
    },
    "a1608feac06d4687967a3e398f01c489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_518fb202e4b44aaba47f07d1a61b6762",
       "IPY_MODEL_672cdc5aea954de3af851c001a667ad3",
       "IPY_MODEL_eebf8874618746b39cf4a21a2728dc7f"
      ],
      "layout": "IPY_MODEL_5176834aa8784bba9ec21234b87a8948",
      "tabbable": null,
      "tooltip": null
     }
    },
    "a8f1b72a33dd4b548de23fbd95e0da18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b82112e1dec645d98aa1c1ba64abcb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_bfd06423ad544218968648016e731a46",
      "placeholder": "​",
      "style": "IPY_MODEL_d029630b63ff44cf807ade428d2eb421",
      "tabbable": null,
      "tooltip": null,
      "value": " 870M/2.47G [00:20&lt;00:37, 42.8MB/s]"
     }
    },
    "bfd06423ad544218968648016e731a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d029630b63ff44cf807ade428d2eb421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "d2c41e71a3f441deaed091b620ac5603": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2dc407afcd945c798e30597fddfcb3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee44487f58454dacb522b1e084ffb733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eebf8874618746b39cf4a21a2728dc7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_d2c41e71a3f441deaed091b620ac5603",
      "placeholder": "​",
      "style": "IPY_MODEL_3326b6141a1a4eba9f316df528a9b99a",
      "tabbable": null,
      "tooltip": null,
      "value": " 2.18M/2.18M [00:00&lt;00:00, 9.47MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E_HhLEeYqFG"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuWudYFWYiH7"
   },
   "source": [
    "# **高效加载模型权重（Memory-efficient Model Weight Loading）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt0Qyg6ewUt6"
   },
   "source": [
    "- 本笔记本提供了一些 **在 GPU（或 CPU）内存受限时加载大型预训练或微调模型的技巧**。  \n",
    "- 具体来说，它重点介绍了 **如何加载使用 `torch.save(model.state_dict(), \"model.pth\")` 保存的模型**（例如在 **第 5-7 章** 中），以便在新会话中继续 **预训练（Pretraining）或额外微调（Finetuning）**。  \n",
    "- **尽管示例使用的是 LLM**，但本笔记本介绍的方法 **适用于任何 PyTorch 模型的加载**，不仅限于 LLM。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/memory-efficient-loading.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxQzFoS-IXdY",
    "outputId": "b28ebfbd-9036-4696-d95a-7f96fdf29919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_profiler version: 0.61.0\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y47iQaQKyHap"
   },
   "source": [
    "&nbsp;\n",
    "## 1. **基准测试工具（Benchmark Utilities）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQeOEoo6yT0X"
   },
   "source": [
    "- 首先，我们定义一些 **用于追踪 VRAM（GPU 内存）** 的工具函数。  \n",
    "- 之后，我们还将 **引入一个工具来监测主系统 RAM（CPU 内存）**。  \n",
    "- 这些函数的作用将在后续应用时变得更加清晰。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pEiqjYrVivgt"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def start_memory_tracking():\n",
    "    \"\"\"Initialize GPU memory tracking.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        print(\"This notebook is intended for CUDA GPUs but CUDA is not available.\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert bytes to GB\n",
    "    print(f\"Maximum GPU memory allocated: {max_gpu_memory:.1f} GB\")\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(3)  # some buffer time to allow memory to clear\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 3)\n",
    "    print(f\"Maximum GPU memory allocated: {max_memory_allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5oJwoc-kkXs"
   },
   "source": [
    "&nbsp;\n",
    "## 2. 模型设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfJE0vnMyr88"
   },
   "source": [
    "- 该代码部分 **用于初始化模型**。  \n",
    "- 这里，我们使用 **\"GPT-2 Large\" 模型** 以增加实验的挑战性（如果希望减少 **内存占用** 和 **执行时间**，可以选择 **\"GPT-2 Small (124M)\"**）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tMuhCYaVI0w7"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-xl (1558M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWYoo1z5y8aX"
   },
   "source": [
    "- 现在，我们 **测试 GPU 内存监测函数的运行情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK3NEA3eJv3f",
    "outputId": "60573d6e-c603-45e7-8283-b1e92e2a0013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n"
     ]
    }
   ],
   "source": [
    "start_memory_tracking()\n",
    "\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIhwBEBxzBsF"
   },
   "source": [
    "- 此外，我们通过 **输入示例张量（tensor）** 来确保 **模型能够正常运行**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i_j6nZruUd7g"
   },
   "outputs": [],
   "source": [
    "# Test if the model works (no need to track memory here)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgNb8c32zh4g"
   },
   "source": [
    "- 接下来，假设我们 **完成了模型的预训练，并希望将其保存以便后续使用**。  \n",
    "- **（为简洁起见，这里跳过实际的预训练过程，仅保存初始化的模型，但概念相同。）** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wUIXjcsimXU7"
   },
   "outputs": [],
   "source": [
    "# Training code would go here...\n",
    "\n",
    "model.train()\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9tBS4HUzz1g"
   },
   "source": [
    "- 最后，我们 **在 Python 会话中删除模型和示例张量，以释放 GPU 内存**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqmTzztqKnTs",
    "outputId": "1198afb9-2d97-4b6a-9bdb-41551f25749d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "del model, test_input\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EnO8beUJ6Sb"
   },
   "source": [
    "&nbsp;\n",
    "## 3. 加载权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtAXKjsG0AVL"
   },
   "source": [
    "- **接下来进入关键部分**，我们将 **加载预训练模型的权重**。  \n",
    "- 让我们 **检查加载之前保存的模型所需的 GPU 内存**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCrQNbSJJO9w",
    "outputId": "9b203868-a8ef-4011-fc2b-611cc0d10994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Then load pretrained weights\n",
    "\n",
    "start_memory_tracking()\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    ")\n",
    "model.to(device)\n",
    "model.eval();\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AGvOrcN0KdJ"
   },
   "source": [
    "- **注意**，当前 **内存占用是前一阶段的 2 倍**。  \n",
    "- 这是因为，在加载模型权重时，**短暂地在内存中存储了两份模型**：\n",
    "  - **第一次**：通过 `model.to(device)` 将模型移动到设备（GPU/CPU）。  \n",
    "  - **第二次**：调用  \n",
    "    ```python\n",
    "    model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "    ```  \n",
    "    **在这一步，模型权重会被加载到 `state_dict` 中，然后复制到模型本体**。但在 **复制完成前**，**内存中同时存在完整的模型和加载的 `state_dict`**，导致占用翻倍。  \n",
    "- **接下来的章节将介绍如何优化这一过程，以减少内存占用**。  \n",
    "- 在此之前，我们先 **测试模型是否正确加载，并重置 GPU 内存**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvlUn-nmmbuj",
    "outputId": "11d3ab68-f570-4c1e-c631-fe5547026799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Test if the model works (no need to track memory here)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "\n",
    "del model, test_input\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdPnW3iLLrjX"
   },
   "source": [
    "&nbsp;\n",
    "## **4. 按顺序加载权重（Loading Weights Sequentially）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYqtUON602TD"
   },
   "source": [
    "- **为了解决上一节提到的“模型权重在 GPU 内存中出现两次”的问题**，可以采用 **按顺序加载（sequential loading）** 的方法。  \n",
    "- 具体来说，我们 **分步加载模型**：\n",
    "  1. **首先**，将 **模型架构** 加载到 **GPU 内存**。  \n",
    "  2. **然后**，将 **模型权重** 先加载到 **CPU 内存**。  \n",
    "  3. **最后**，逐个 **参数** 复制到 **GPU 内存**，避免一次性加载导致的内存峰值。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOIGTNWTmx9G",
    "outputId": "145162e6-aaa6-4c2a-ed8f-f1cf068adb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.7 GB\n"
     ]
    }
   ],
   "source": [
    "start_memory_tracking()\n",
    "\n",
    "model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# Sequentially copy weights to the model's parameters\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in state_dict:\n",
    "            param.copy_(state_dict[name].to(device))\n",
    "        else:\n",
    "            print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn9xD_xL1ZzM"
   },
   "source": [
    "- 如上所示，**采用按序加载方法后，内存占用明显降低**。  \n",
    "- 需要注意的是，**内存从 6.4GB 增加到 6.7GB**，原因如下：  \n",
    "  - **最初**，仅模型结构被加载到 **GPU 内存**。  \n",
    "  - **随后**，每次加载一个参数张量（parameter tensor）并移动到 GPU，以便使用 `\".to()\"` 方法将其赋值到模型中。  \n",
    "  - **整个过程中，我们只在 GPU 中存储一个额外的参数张量**，避免了大规模的额外占用。  \n",
    "- **总体来看，这种方法显著降低了 GPU 内存峰值**。  \n",
    "- 接下来，我们 **简单测试模型的正确性**，然后 **重置 GPU 内存，为下一节做准备**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRHnjA48nJgw",
    "outputId": "dcd6b1b2-538f-4862-96a6-a5fcbf3326a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Test if the model works (no need to track memory here)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "\n",
    "del model, test_input, state_dict, param\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M92LK7usb-Z"
   },
   "source": [
    "&nbsp;\n",
    "## **5. 在低 CPU 内存环境中加载模型（Loading the Model with Low CPU Memory）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R45qgeB613e2"
   },
   "source": [
    "- 在上一节中，我们通过 **先将权重 (`state_dict`) 加载到 CPU 内存，再逐个复制到 GPU**，成功降低了 **GPU 内存占用**。  \n",
    "- 但是，如果 **CPU 内存也有限**，我们该如何加载模型？  \n",
    "- 本节将介绍 **PyTorch 的 `\"meta\"` 设备（Meta Device）方法**，该方法适用于 **GPU 内存充足但 CPU 内存较小的设备**。  \n",
    "- 在此之前，我们先 **定义一个工具函数，用于监测 CPU 内存使用情况**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BrcWy0q-3Bbe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def memory_usage_in_gb(func, *args, **kwargs):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # Measure the baseline memory usage before running the function\n",
    "    baseline_mem = process.memory_info().rss / 1024 ** 3  # in GB\n",
    "\n",
    "    # Start monitoring memory in a separate thread\n",
    "    mem_usage = []\n",
    "    done = False\n",
    "\n",
    "    def monitor_memory():\n",
    "        while not done:\n",
    "            mem_usage.append(process.memory_info().rss / 1024 ** 3)  # Convert to GB\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    t = Thread(target=monitor_memory)\n",
    "    t.start()\n",
    "\n",
    "    # Run the function\n",
    "    func(*args, **kwargs)\n",
    "\n",
    "    # Stop monitoring\n",
    "    done = True\n",
    "    t.join()\n",
    "\n",
    "    peak_mem_usage_gb = max(mem_usage) - baseline_mem\n",
    "    return peak_mem_usage_gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayy30Ytd5hjF"
   },
   "source": [
    "- **首先，我们追踪上一节** 使用 **顺序加载权重（Sequential Weight Loading）** 方法时的 **CPU 内存占用情况**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCkV6IbQtpVn",
    "outputId": "26c0435a-1e3d-4e8f-fbe2-f9655bad61b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.7 GB\n",
      "-> Maximum CPU memory allocated: 6.3 GB\n"
     ]
    }
   ],
   "source": [
    "def load_sequentially():\n",
    "    start_memory_tracking()\n",
    "\n",
    "    model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "    state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Sequentially copy weights to the model's parameters\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in state_dict:\n",
    "                param.copy_(state_dict[name].to(device))\n",
    "            else:\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWrmnCML5oKy"
   },
   "source": [
    "- **假设我们使用的设备 CPU 内存较小，但 GPU 内存充足**。  \n",
    "- 我们可以 **利用 PyTorch 的 `\"meta\"` 设备**，在 **CPU 和 GPU 内存占用之间进行权衡**。  \n",
    "- **PyTorch 的 `\"meta\"` 设备** 是一种特殊的设备类型，它允许创建 **不实际分配内存** 的张量，即 **\"meta\" 张量**。  \n",
    "- 这对于 **模型分析（Model Analysis）或架构定义（Architecture Definition）** 等任务非常有用，在这些任务中，我们只需要 **张量的形状和数据类型**，而不需要 **真正的内存分配**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBErC_5Yt8ly",
    "outputId": "8799db06-191c-47c4-92fa-fbb95d685aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n",
      "Maximum GPU memory allocated: 12.8 GB\n",
      "-> Maximum CPU memory allocated: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "def load_sequentially_with_meta():\n",
    "    start_memory_tracking()\n",
    "\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    state_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Sequentially copy weights to the model's parameters\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in state_dict:\n",
    "                param.copy_(state_dict[name])\n",
    "            else:\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially_with_meta)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpnCABp75-VQ"
   },
   "source": [
    "- 如上所示，通过 **在 `\"meta\"` 设备上创建模型，并直接将权重加载到 GPU 内存**，我们 **显著降低了 CPU 内存占用**。  \n",
    "- 这可能引发一个问题：**“那么，顺序加载权重（Sequential Weight Loading）是否仍然有必要？它与原始方法相比效果如何？”**  \n",
    "- 接下来，我们对比 **PyTorch 的标准权重加载方法**（即本笔记本开头的 **初始权重加载方式**），看看它的 CPU 和 GPU 内存占用情况。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f-bqBNRuR39",
    "outputId": "f7c0a901-b404-433a-9b93-2bbfa8183c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n",
      "-> Maximum CPU memory allocated: 4.4 GB\n"
     ]
    }
   ],
   "source": [
    "def baseline():\n",
    "    start_memory_tracking()\n",
    "\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    model.to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "    model.to(device)\n",
    "    model.eval();\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(baseline)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKAjxbX86xnb"
   },
   "source": [
    "- 如上所示，**不使用 `\"meta\"` 设备的“简单”权重加载方式，占用了更多的内存**。  \n",
    "- 换句话说，**如果设备的 CPU 内存有限**，可以采用 **`\"meta\"` 设备方法**，直接将模型权重加载到 **GPU 内存**，从而 **降低 CPU 内存峰值占用**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## **6. 使用 `mmap=True`（推荐方法）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **如果你是 PyTorch `torch.load` 的中高级用户**，可能会想知道这些方法与 **`mmap=True` 选项** 有何区别。  \n",
    "- **`mmap=True`** 选项 **启用了内存映射文件 I/O（Memory-Mapped File I/O）**，使得张量可以 **直接从磁盘访问数据**，而不需要将整个文件加载到 RAM 中，从而在 **RAM 受限时显著降低内存占用**。  \n",
    "- 另请参考 **[mikaylagawarecki 的有用评论](https://github.com/rasbt/LLMs-from-scratch/issues/402)**。  \n",
    "- 乍一看，`mmap=True` **可能比前面介绍的顺序加载方法效率更低**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKwV0AMNemuR",
    "outputId": "e207f2bf-5c87-498e-80fe-e8c4016ac711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "-> Maximum CPU memory allocated: 5.9 GB\n"
     ]
    }
   ],
   "source": [
    "def best_practices():\n",
    "  with torch.device(\"meta\"):\n",
    "      model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "  model.load_state_dict(\n",
    "      torch.load(\"model.pth\", map_location=device, weights_only=True, mmap=True),\n",
    "      assign=True\n",
    "  )\n",
    "\n",
    "  print_memory_usage()\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(best_practices)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **当前 CPU RAM 占用较高的原因**，是因为 **该设备的 CPU 内存充足**，因此 PyTorch 默认将模型加载到 RAM 中。  \n",
    "- 但是，**如果设备的 CPU RAM 受限**，`mmap` 方法 **会显著降低内存使用量**，因为它允许张量 **直接从磁盘访问数据，而无需将整个文件加载到 RAM**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 7. 其他方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本笔记本主要介绍 **PyTorch 内置的简单方法**，用于高效加载模型权重。  \n",
    "- **在 CPU 内存受限的情况下**，推荐使用 **`mmap=True` 方法**（前文已详细介绍）。  \n",
    "- 另外，还有一种 **“暴力”方法**：即 **将每个权重张量（Tensor）分别存储和加载**，以减少内存占用。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2CgPEZUIb00w"
   },
   "outputs": [],
   "source": [
    "model = GPTModel(BASE_CONFIG)\n",
    "# Assume `model` is your trained model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Create a directory to store individual parameter files\n",
    "os.makedirs(\"model_parameters\", exist_ok=True)\n",
    "\n",
    "# Save each parameter tensor separately\n",
    "for name, param in state_dict.items():\n",
    "    torch.save(param.cpu(), f\"model_parameters/{name}.pt\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTsmtJK-b4yy",
    "outputId": "d361e2d3-e34c-48d7-9047-846c9bfd291e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.4 GB\n",
      "-> Maximum CPU memory allocated: 0.3 GB\n"
     ]
    }
   ],
   "source": [
    "def load_individual_weights():\n",
    "\n",
    "    start_memory_tracking()\n",
    "\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    print_memory_usage()\n",
    "    param_dir = \"model_parameters\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            weight_path = os.path.join(param_dir, f\"{name}.pt\")\n",
    "            if os.path.exists(weight_path):\n",
    "                param_data = torch.load(weight_path, map_location=\"cpu\", weights_only=True)\n",
    "                param.copy_(param_data)\n",
    "                del param_data  # Free memory\n",
    "            else:\n",
    "                print(f\"Warning: {name} not found in {param_dir}.\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(load_individual_weights)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="ch06/01_main-chapter-code/ch06.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabadb8-5935-45ff-b39c-db7a29012129",
   "metadata": {
    "id": "bfabadb8-5935-45ff-b39c-db7a29012129"
   },
   "source": [
    "# 第 6 章：进行文本分类的微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.7.2\n",
      "numpy version: 1.24.3\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for tiktoken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Work/anaconda3/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m pkgs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m      \u001b[38;5;66;03m# 用于加载数据集的库\u001b[39;00m\n\u001b[1;32m      9\u001b[0m        ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pkgs:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/anaconda3/lib/python3.11/importlib/metadata/__init__.py:1008\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distribution(distribution_name)\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/Work/anaconda3/lib/python3.11/importlib/metadata/__init__.py:981\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    976\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[1;32m    978\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Distribution\u001b[38;5;241m.\u001b[39mfrom_name(distribution_name)\n",
      "File \u001b[0;32m~/Work/anaconda3/lib/python3.11/importlib/metadata/__init__.py:565\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for tiktoken"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # 用于加载OpenAI的预训练权重的TensorFlow库\n",
    "        \"pandas\"      # 用于加载数据集的库\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "#查看版本号Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445828a-ff10-4efa-9f60-a2e2aed4c87d",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/1.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c3e56-b04b-4b0f-b35f-b485ce5b28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 防止某些单元格执行两次\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "executed_cells = set()\n",
    "#使用一个 set 数据结构来存储已经执行过的单元格标识符\n",
    "@register_line_cell_magic\n",
    "#注册了一个名为 run_once 的魔法命令\n",
    "def run_once(line, cell):\n",
    "    if line not in executed_cells:\n",
    "        get_ipython().run_cell(cell)\n",
    "        executed_cells.add(line)\n",
    "    else:\n",
    "        print(f\"Cell '{line}' has already been executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c",
   "metadata": {
    "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c"
   },
   "source": [
    "## 6.1 不同类型的微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3d731-5123-4f02-accd-c670ce50a5a3",
   "metadata": {
    "id": "ede3d731-5123-4f02-accd-c670ce50a5a3"
   },
   "source": [
    "- 本章节没有代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45579d-d485-47dc-829e-43be7f4db57b",
   "metadata": {},
   "source": [
    "- 常见微调大语言模型的方法包括：指令微调( instructionfinetuning)和分类微调(classification finetuning)\n",
    "- 如下是:指令微调，也是下一章节所要讲的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29ef42-46d9-43d4-8bb4-94974e1665e4",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/2.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60321-95b8-46a9-97bf-1d07fda2c3dd",
   "metadata": {},
   "source": [
    "- 如果您具有机器学习的背景，对于分类微调您可能已经熟悉. 举个例子，分类微调类似于训练卷积网络来对手写数字进行分类的过程\n",
    "- 在分类微调中，模型可以输出特定的分类标签（例如，“spam”和“not spam”）\n",
    "- 分类微调模型只能预测它在训练期间所熟知的类别标签（例如，“垃圾邮件”或“非垃圾邮件”），而指令微调模型通常可以执行更广泛的任务\n",
    "- 我们可以将分类微调模型视为高度专业化的模型;在实践中，开发专业化的模型通常比开发在许多不同任务上表现良好的通用模型要容易得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37a0c4-0bb1-4061-b1fe-eaa4416d52c3",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/3.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## 6.2 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628975-d2e8-4f7f-ab38-92bb868b7067",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/4.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d",
   "metadata": {
    "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d"
   },
   "source": [
    "- 本节准备我们用于分类微调的数据集\n",
    "- 我们使用由垃圾邮件和非垃圾邮件组成的数据集来对 LLM 进行分类微调\n",
    "- 首先，我们下载并解压缩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "#完成导入数据\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # 下载文件\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # 解压文件\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    # 添加.tsv文件扩展名\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1",
   "metadata": {
    "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1"
   },
   "source": [
    "- 数据集保存为一个以制表符分隔的文本文件SMSSpamCollection.tsv，位于 SMSSpamCollection 文件夹中。我们可以使用以下代码将其加载到 pandas DataFrame 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
    "outputId": "a16c5cde-d341-4887-a93f-baa9bec542ab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109",
   "metadata": {
    "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109"
   },
   "source": [
    "- 执行如下代码来查看数据集中的数据类别分布，会发现数据中“非垃圾消息（ham）”的出现频率远高于“垃圾消息（spam）”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
    "outputId": "761e0482-43ba-4f46-f4b7-6774dae51b38"
   },
   "outputs": [],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773f054-0bdc-4aad-bbf6-397621bf63db",
   "metadata": {
    "id": "f773f054-0bdc-4aad-bbf6-397621bf63db"
   },
   "source": [
    "- 为了简化处理，并且出于教学目的考虑，我们选择使用小规模数据集，这有助于更快地对大语言模型进行微调。因此，我们对数据集进行了下采样，使每个类别包含 747 个实例。\n",
    "- （除了下采样之外，还有其他几种方法可以处理类别不平衡，但这些超出了本书的范围; 你可以在 imbalanced-learn 中找到示例和更多信息）[`imbalanced-learn` 用户指南](https://imbalanced-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4a0a2-9704-4a96-b38f-240339818688",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7be4a0a2-9704-4a96-b38f-240339818688",
    "outputId": "396dc415-cb71-4a88-e85d-d88201c6d73f"
   },
   "outputs": [],
   "source": [
    "%%run_once balance_df\n",
    "#单元模型启动\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    #算一下类别为spam的样本出现的次数\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    #随机采样 “ham” 使其数量与“spam”样本数量一致\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    #合并 “spam” 和采样后的 “ham”数据\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6",
   "metadata": {
    "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6"
   },
   "source": [
    "- 随后我们把标签 \"ham\" 和 \"spam\" 转换为整数类标签“0”和“1”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd",
   "metadata": {
    "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd"
   },
   "outputs": [],
   "source": [
    "%%run_once label_mapping\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7f062-ef4e-4020-8275-71990cab4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f",
   "metadata": {
    "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f"
   },
   "source": [
    "- 现在自定义一个函数,用于把数据集随机划分为训练集、验证集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uQl0Psdmx15D",
   "metadata": {
    "id": "uQl0Psdmx15D"
   },
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # 把数据集Dataframe打乱\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # 计算分割系数\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # 分割数据集Dataframe\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# 剩余部分为测试集，占总数据集的比例为 0.2\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)\n",
    "#DataFrame 被随机划分为训练集、验证集和测试集并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7a0c5-1d5f-458a-b685-3f49520b0094",
   "metadata": {},
   "source": [
    "## 6.3 创建数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c",
   "metadata": {
    "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c"
   },
   "source": [
    "- 由于文本消息长度随机,因此在批量化组合训练数据之前要做数据归一化,我们有两种操作可供选择\n",
    "  1. 将所有消息截断到数据集中最短消息的长度或批次长度\n",
    "  2. 将所有消息填充到数据集中最长消息的长度或批次长度\n",
    "\n",
    "- 这里我们选择操作2,填充数据\n",
    "- 并且,我们使用`<|endoftext|>` 作为填充标识符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f33f-1428-4f22-9886-7fee633b3666",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/5.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
    "outputId": "b5b48439-32c8-4b37-cca2-c9dc8fa86563"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# 打印<|endoftext|>对应的词元id\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f582ff-68bf-450e-bd87-5fb61afe431c",
   "metadata": {
    "id": "04f582ff-68bf-450e-bd87-5fb61afe431c"
   },
   "source": [
    "- 下面我们首先将 `SpamDataset` 训练数据集中最长的序列，然后将填充token（<|endoftext|>）添加到其他序列末端以匹配该序列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7791b52-af18-4ac4-afa9-b921068e383e",
   "metadata": {
    "id": "d7791b52-af18-4ac4-afa9-b921068e383e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        #先读入文本\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        #编码成对应的词元id\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            #如果序列长度超过 max_length，则进行截断\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # 填充到最长序列的长度\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    def __getitem__(self, index):\n",
    "        #获取指定索引的数据样本\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        #输出序列长度\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uzj85f8ou82h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzj85f8ou82h",
    "outputId": "d08f1cf0-c24d-445f-a3f8-793532c3716f"
   },
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd932-97eb-4b88-9cf9-d766ea4c3a60",
   "metadata": {},
   "source": [
    "- 我们还将验证和测试集填充到最长的训练序列\n",
    "- 在本节代码中,任何超过最长训练示例长度的验证集和测试集样本都将使用之前定义的 `SpamDataset` 中的代码 `encoded_text[:self.max_length]`进行截断\n",
    "- 此行为完全是可选的，如果我们在验证和测试集情况下都设置 `max_length=Non`，代码也能正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e",
   "metadata": {
    "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e"
   },
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "#将验证集和测试集数据填充到最长序列的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20170d89-85a0-4844-9887-832f5d23432a",
   "metadata": {},
   "source": [
    "- 接下来，我们使用数据集来实例化数据加载器，这与前几章中创建数据加载器类似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcc349-205f-48f8-9655-95ff21f5e72f",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/6.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
    "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "# 设置种子确保可复现\n",
    "torch.manual_seed(123)\n",
    "# 初始化数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "# 对齐超参数,但是dataset需要区别一下训练集、验证集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {},
   "source": [
    "- 作为验证步骤，我们遍历数据加载器，并确保每个批次包含 8 个训练样本，其中每个训练样本由 120 个 token 组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "    #如果这个数据在训练集出现过,则跳过\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {},
   "source": [
    "- 最后，让我们通过打印来查看每个数据集的批次数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
   },
   "outputs": [],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c",
   "metadata": {
    "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c"
   },
   "source": [
    "## 6.4 用预训练权重初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1af8b-8bd1-4b44-8b8b-dc031496e208",
   "metadata": {},
   "source": [
    "- 在本节中，我们将初始化在上一章中使用的预训练模型\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/7.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992d779-f9fb-4812-a117-553eb790a5a9",
   "metadata": {
    "id": "2992d779-f9fb-4812-a117-553eb790a5a9"
   },
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "# 输入超参数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "# 输入模型的参数\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")\n",
    "# 断言语句，用于验证 train_dataset.max_length 是否符合模型的上下文长度（context_length）要求。\n",
    "# 如果不符合，将抛出一个异常的讯号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a649a-44f5-466c-8a8e-326c063384f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022a649a-44f5-466c-8a8e-326c063384f5",
    "outputId": "7091e401-8442-4f47-a1d9-ecb42a1ef930"
   },
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "# 引入超参数跟模型设置\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e056c-abe0-415f-b34d-df686204259e",
   "metadata": {},
   "source": [
    "- 为了确保模型已正确加载，让我们仔细检查它是否生成了连贯的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac25ff-74b1-4149-8dc5-4c429d464330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "# 生成文本\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162550-6a02-4ece-8db1-06c71d61946f",
   "metadata": {},
   "source": [
    "- 在我们将模型微调为分类器之前，让我们看看该模型是否已经可以通过提示对垃圾邮件进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94224aa9-c95a-4f8a-a420-76d01e3a800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce39ed0-2c77-410d-8392-dd15d4b22016",
   "metadata": {},
   "source": [
    "- 正如我们所看到的，该模型不太擅长遵循指示\n",
    "- 这是意料之中的，因为它只是经过了预训练，还没有经过指令微调（指令微调将在下一章中介绍）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522",
   "metadata": {
    "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522"
   },
   "source": [
    "## 6.5 添加分类头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9d66f-76b2-40fc-9ec5-3f972a8db9c0",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/8.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bac05-78df-4412-bd80-612f8061c01d",
   "metadata": {},
   "source": [
    "- 在本节中，我们将修改预训练的 LLM，为分类微调做准备\n",
    "- 首先，让我们先看一下模型的架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23aff91-6bd0-48da-88f6-353657e6c981",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8f7a01-b7c0-48d4-b1e7-8c12cc7ad932",
    "outputId": "b6a5b9b5-a92f-498f-d7cb-b58dd99e4497"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f640a76-dd00-4769-9bc8-1aed0cec330d",
   "metadata": {},
   "source": [
    "- 我们可以看到,在第 4 章中实现的架构\n",
    "- 我们的目标是替换和微调输出层\n",
    "- 为了实现这一点，我们首先冻结模型，这意味着我们使所有层都是不可训练的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fkMWFl-0etea",
   "metadata": {
    "id": "fkMWFl-0etea"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 返回 model 中所有的参数,但是不参与反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72155f83-87d9-476a-a978-a15aa2d44147",
   "metadata": {},
   "source": [
    "- 然后，我们替换输出层`(model.out_head)`，它最初将层输入映射到 50,257 个维度（词汇表的大小）\n",
    "- 由于我们对二元分类模型进行了微调（预测 2 个类别，“spam”和“not spam”），因此我们可以替换如下所示的输出层，默认情况下它是可训练的\n",
    "- 我们使用 `BASE_CONFIG[\"emb_dim\"]`（在 “gpt2-small （124M）” 模型中为 768）来保持下面的代码更通用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e759fa0-0f69-41be-b576-17e5f20e04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5475-ae77-4f97-8f3e-dec462b1339f",
   "metadata": {},
   "source": [
    "- 从技术上讲，只需训练输出层就足够了\n",
    "- 但是，正如在 [Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)中表明，微调其他层可以显著提高性能\n",
    "- 因此，我们还使最后一个 transformer 模块和最后一个 `LayerNorm` 模块连接起来，将最后一个 transformer 模块连接到输出层，使其可训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c1eb-c46c-4065-8525-eea1b8c66d10",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/9.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7",
   "metadata": {
    "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7"
   },
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "# 对于需要更新权重则设将该参数设置为True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b899-8284-4d3a-97c0-8a48eb33ba2e",
   "metadata": {},
   "source": [
    "- 即使我们添加了新的输出层并标记了某些层为可训练或不可训练，我们仍然可以像之前一样使用这个模型\n",
    "- 例如,我们输入点文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
    "outputId": "27e041b1-d731-48a1-cf60-f22d4565304e"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)\n",
    "# 编码并输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf8481-772d-467b-851c-a62b86d0cb1b",
   "metadata": {},
   "source": [
    "- 与前几章相比，它现在有两个输出维度，而不再是 50,257 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
    "outputId": "9cae7448-253d-4776-973e-0af190b06354"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # 形状：（批次大小，词元数量）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75430a01-ef9c-426a-aca0-664689c4f461",
   "metadata": {},
   "source": [
    "- 如前几章所述，对于每个 input token，模型都会返回一个 output vector\n",
    "- 由于我们向模型提供了具有 4 个输入 token 的文本样本，因此输出由上面的 4 个 2 维输出向量组成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9144f-6817-4be4-8d4b-5d4dadfe4a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/10.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb8616-c791-4f5c-bac0-5302f663e46a",
   "metadata": {},
   "source": [
    "- 在第 3 章中，我们讨论了attention机制，它将每个输入 token 连接到另一个输入 token\n",
    "- 在第 3 章中，我们还介绍了类 GPT 模型中使用的因果注意力掩码(causal attention mask);这种掩码让当前token只关注当前和先前出现过的位置\n",
    "- 基于这种因果注意力机制，第 4 个（最后一个）token在所有token中包含最多的信息，因为唯一包含所有其他全部信息只有它\n",
    "- 因此，我们在微调过程中格外关注这个最后的词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
    "outputId": "e79eb155-fa1f-46ed-ff8c-d828c3a3fabd"
   },
   "outputs": [],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08ae0-e664-4670-b7c5-8a2280d9b41b",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/11.png\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa4aef-e1e9-491b-9adf-5aa973e59b8c",
   "metadata": {},
   "source": [
    "## 6.6 计算分类损失和准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1fd1-ace8-44b4-b438-185ed0ba8b33",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/12.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7df4ee-0a34-4a4d-896d-affbbf81e0b3",
   "metadata": {},
   "source": [
    "- 在解释损失计算之前，让我们简单了解一下模型输出是如何转换为类标签的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557996dd-4c6b-49c4-ab83-f60ef7e1d69e",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/13.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77faab1-3461-4118-866a-6171f2b89aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd71fa-628a-4d00-b81d-6d8bcb2c341d",
   "metadata": {},
   "source": [
    "- 与第 5 章类似，我们通过 `softmax` 函数将输出 （logits） 转换为概率分数，然后通过`argmax`函数获得最大概率值的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81efa92-9be1-4b9e-8790-ce1fc7b17f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a6f02-307e-4147-a416-14d115bf8179",
   "metadata": {},
   "source": [
    "- 如第 5 章所述，softmax 函数在这里是可选的，因为最大的输出对应于最大的概率分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9ad66-4969-4501-8239-3ccdb37e71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb20d3a-cbba-4ab1-8584-d94e16589505",
   "metadata": {},
   "source": [
    "- 我们可以应用这个概念来计算所谓的\"分类准确性\"，计算给定数据集中正确预测的百分比\n",
    "- 为了计算分类准确率，我们可以将前面基于 `argmax` 的预测代码应用于数据集中的所有示例，并按如下方式计算正确预测的分数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf9572-aed0-4a21-9c3b-7f9f2aec5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    # 先改成评估模式,在初始化两个储存器\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "        # num_batches设为None则取全部数据\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        # 取num_batches和len(data_loader)中较小的那个\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            # 仅处理前 num_batches 批次的数据\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # 最后一个输出词元的概率\n",
    "            # 预测内容取概率最大值\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            # 当前批次的样本数\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            # 预测正确的样本数量\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165fe46-a284-410b-957f-7524877d1a1a",
   "metadata": {},
   "source": [
    "- 用该函数计算不同数据集的分类精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e5255-8427-488c-adef-e1c10ab4fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Note:\n",
    "\n",
    "#取消注释以下行将允许代码在 Apple Silicon 芯片上运行（如果适用），\n",
    "#这比在 Apple CPU 上运行快大约 2 倍（根据 M3 MacBook Air 的测量结果）。\n",
    "#截至目前，在 PyTorch 2.4 版本中，通过 CPU 和 MPS 得到的结果是相同的。\n",
    "#然而，在 PyTorch 的早期版本中，使用 MPS 时，可能会观察到不同的结果。\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Running on {device} device.\")\n",
    "\n",
    "model.to(device) # 对于 nn.Module 类，无需 model = model.to(device)赋值操作\n",
    "\n",
    "torch.manual_seed(123) # 由于训练数据加载器中的随机打乱，因此设置随机种子以确保可重复性\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "#各种计算\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30345e2a-afed-4d22-9486-f4010f90a871",
   "metadata": {},
   "source": [
    "- 可以看到，预测的准确性不是很好，这是因为因为我们还没有对模型进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a9d15-8fc7-48a2-8734-d92a2f265328",
   "metadata": {},
   "source": [
    "- 在开始微调 （/training） 之前，我们首先必须定义要在训练期间优化的损失函数\n",
    "- 目标是最大限度地提高模型的垃圾邮件分类准确性;\n",
    "- 由于分类准确率不是可微分的，我们使用交叉熵损失作为最大化准确率的替代（您可以在我免费提供的[深度学习入门课程](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression)的第 8 讲中了解有关此主题的更多信息）\n",
    "\n",
    "- `calc_loss_batch` 函数与第 5 章相同，只是我们只对优化最后一个 `tokens model（input_batch）``[：， -1， ：]` 感兴趣，而不是所有 `tokens model（input_batch）`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4",
   "metadata": {
    "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "    #用交叉熵计算损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013aab9-f854-4866-ad55-5b8350adb50a",
   "metadata": {},
   "source": [
    "使用 `calc_closs_loader`，我们在开始训练之前计算初始训练集、验证集和测试集损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b83e10-5720-45e7-ac5e-369417ca846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果 num_batches 超过数据加载器中的批次数，则减少批次数以匹配数据加载器中的总批次数\n",
    "        # num_batches = min(num_batches, len(data_loader))\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "                # 总损失值\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            else:\n",
    "                break\n",
    "    return total_loss / num_batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
    "outputId": "49df8648-9e38-4314-854d-9faacd1b2e89"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # 因为我们不进行训练，所以为了提高效率禁用梯度跟踪\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b980b-e583-4f62-84a0-4edafaf99d5d",
   "metadata": {},
   "source": [
    "- 在下一节中，我们将训练模型以最小化损失值，从而提高分类准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ae0fd-6261-42b4-ab6a-d24289953083",
   "metadata": {
    "id": "456ae0fd-6261-42b4-ab6a-d24289953083"
   },
   "source": [
    "## 6.7 在有监督数据上微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b099b-0829-4f72-8a2b-4363e3497026",
   "metadata": {},
   "source": [
    "- 在本节中，我们将定义并使用训练函数来提高模型的分类准确率\n",
    "- 下面的 `train_classifier_simple` 函数实际上与我们在第 5 章中用于预训练模型的 `train_model_simple `函数相同\n",
    "- 唯二区别\n",
    "  1. 我们现在跟踪的是已经看到的训练样本数量（examples_seen），而不是token数量\n",
    "  2. 我们在每个周期后计算准确率，而不是打印一个样例文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b6222-1dc2-4530-9d01-b6b04fe3de12",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/14.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Csbr60to50FL",
   "metadata": {
    "id": "Csbr60to50FL"
   },
   "outputs": [],
   "source": [
    "# 跟第五章的一摸一样那就当重新复习了\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    # 初始化分类头\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        # 每次都进入训练模块\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            # 清零梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() \n",
    "            # 对损失值执行反向传播记录梯度\n",
    "            optimizer.step() \n",
    "            # 用梯度优化权重\n",
    "            examples_seen += input_batch.shape[0] \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624cb30-3e3a-45be-b006-c00475b58ae8",
   "metadata": {},
   "source": [
    "- `evaluate_model` 在`train_classifier_simple` 是跟第五章相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7bc04-6aa6-4516-a147-460e2f466eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "    # 调用辅助函数 calc_loss_loader 计算模型在 训练集、验证集上的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807bfe9-364d-46b2-9e25-3b000c3ef6f9",
   "metadata": {},
   "source": [
    "- M3 MacBook Air五分钟训练完\n",
    "- V100 or A100 GPU大概用半分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7kU3aAj7vTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7kU3aAj7vTJ",
    "outputId": "504a033e-2bf8-41b5-a037-468309845513"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "# 输出每一次的损失值跟在一定频次下进行准确率输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261bf90-3ce7-4591-895a-044a05538f30",
   "metadata": {},
   "source": [
    "- 跟第五章相似,我们用Matplot作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cURgnDqdCeka",
   "metadata": {
    "id": "cURgnDqdCeka"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "   \n",
    "    ax2 = ax1.twiny()  \n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  \n",
    "\n",
    "    fig.tight_layout()  \n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "# 一个经典的画图操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OIqRt466DiGk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "OIqRt466DiGk",
    "outputId": "b16987cf-0001-4652-ddaf-02f7cffc34db"
   },
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd28174-1836-44ba-b6c0-7e0be774fadc",
   "metadata": {},
   "source": [
    "- 图中, 根据训练的斜率, 我们可以发现模型训练的很好\n",
    "- 此外, 训练和验证损失非常接近可以表明，该模型不会倾向于过拟合训练数据\n",
    "- 同样的, 我们可以对精度进行绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yz8BIsaF0TUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "yz8BIsaF0TUo",
    "outputId": "3a7ed967-1f2a-4c6d-f4a3-0cc8cc9d6c5f"
   },
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba699-21bc-42de-a69c-99f370bb0363",
   "metadata": {},
   "source": [
    "- 根据上面的准确率图，我们可以看到该模型在第 4 个和第 5 循环之后,训练和验证准确率变得相对较高\n",
    "- 但是，别忘了我们之前在 training 函数中指定了 `eval_iter=5`，这意味着我们只估计了训练和验证集的性能\n",
    "- 我们可以计算完整数据集的训练、验证和测试集性能，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UHWaJFrjY0zW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHWaJFrjY0zW",
    "outputId": "e111e6e6-b147-4159-eb9d-19d4e809ed34"
   },
   "outputs": [],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "# 输出总的训练集、验证集和测试集的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882649f-dc7b-401f-84d2-024ff79c74a1",
   "metadata": {},
   "source": [
    "- 我们可以看到，训练集和验证集的表现实际上是相同的。\n",
    "- 然而，由于测试集表现略微较差，我们可以看出，模型在一定程度上对训练数据进行了过拟合，同时也对用于调整超参数（如学习率）的验证数据进行了过拟合。\n",
    "- 不过，这种情况是正常的，并且通过增加模型的 dropout rate（drop_rate）或优化器设置中的 weight_decay，可能进一步减小这种差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d9ad7-3ec1-450e-8c9f-4fc46d3d5bb0",
   "metadata": {},
   "source": [
    "## 6.8 使用LLM作为垃圾消息分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebcfa2-479e-408b-9cf0-7421f6144855",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch6/15.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5408e6-83e4-4e5a-8503-c2fba6073f31",
   "metadata": {},
   "source": [
    "- 最后，让我们将微调后的 GPT 模型投入实际应用。\n",
    "- 以下的 `classify_review` 函数实现了类似于我们之前实现的 `SpamDataset` 的数据预处理步骤。\n",
    "- 然后，函数返回模型预测的整数类别标签，并返回对应的类别名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aHdn6xvL-IW5",
   "metadata": {
    "id": "aHdn6xvL-IW5"
   },
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    # 将模型设置为评估模式，禁用dropout等训练特定操作\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    # 使用tokenizer将输入文本 转换token ID 列表\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # 获取模型支持的最大上下文长度（由位置嵌入的权重数量决定）\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    # 如果 token 序列长度超过模型支持的最大长度，则进行截断\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    # 对序列进行填充\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) \n",
    "    # 将 token ID 转换为张量并移动到指定设备（如 GPU），同时增加 batch 维度\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :] \n",
    "        # 禁用梯度计算，仅进行推理\n",
    "        # 将输入张量传入模型，获取 logits，并提取最后一个 token 的 logits\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    # 在 logits 上取 softmax 概率最大值的索引，作为预测类别\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\"\n",
    "    # 如果预测标签是 `1`，返回 \"spam\"，否则返回 \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29682d8-a899-4d9b-b973-f8d5ec68172c",
   "metadata": {},
   "source": [
    "- 试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apU_pf51AWSV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apU_pf51AWSV",
    "outputId": "d0fde0a5-e7a3-4dbe-d9c5-0567dbab7e62"
   },
   "outputs": [],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1g5VTOo_Ajs5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1g5VTOo_Ajs5",
    "outputId": "659b08eb-b6a9-4a8a-9af7-d94c757e93c2"
   },
   "outputs": [],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e39-0d47-40c1-8d18-1f716cf7a81e",
   "metadata": {},
   "source": [
    "- 最后，让我们保存模型，以便以后如果需要重用模型时，无需重新训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mYnX-gI1CfQY",
   "metadata": {
    "id": "mYnX-gI1CfQY"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")\n",
    "# 储存!大功告成!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78cf7c-6b80-4f71-a50e-3ccc73839af6",
   "metadata": {},
   "source": [
    "- 下次我们可以这么唤醒这个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e68a5-d492-493b-87ef-45c475f353f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4",
   "metadata": {
    "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4"
   },
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdc910-d616-47ab-aa85-f90c6e7ed80e",
   "metadata": {},
   "source": [
    "- 请参阅 [./gpt_class_finetune.py](./gpt_class_finetune.py) 脚本，这是一个自包含的分类微调脚本。\n",
    "- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习解决方案。\n",
    "- 此外，感兴趣的读者可以在 [附录E](../../appendix-E) 中找到关于低秩适应（LoRA）的参数高效微调的介绍。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch06/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 6 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "## Exercise 6.1: 增加上下文长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    "我们可以通过将最大长度设置为 1024 来填充输入至模型支持的最大token数：\n",
    "\n",
    "```python\n",
    "max_length = 1024\n",
    "\n",
    "train_dataset = SpamDataset(base_path / \"train.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(base_path / \"validation.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(base_path / \"test.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "```\n",
    "\n",
    "或者，我们也可以通过以下方式定义 `max_length`：\n",
    "\n",
    "``` python\n",
    "max_length = model.pos_emb.weight.shape[0]\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "max_length = BASE_CONFIG[\"context_length\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f4d5d-17fd-4265-93d8-ea08a22fdaf8",
   "metadata": {},
   "source": [
    "为了方便起见，您可以通过以下命令运行此实验：\n",
    "```bash\n",
    "python additional-experiments.py --context_length \"model_context_length\"\n",
    "```\n",
    "使用 ../02_bonus_additional-experiments 文件夹中的代码，这将导致测试准确率大幅下降，达到 78.33%（而在主章节中为 95.67%）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780455-f52a-48d1-ab82-6afd40bcad8b",
   "metadata": {},
   "source": [
    "## Exercise 6.2: 微调整个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa5208-aa29-4165-a0ec-7480754e2a18",
   "metadata": {},
   "source": [
    "我们可以通过移除以下代码行来微调整个模型，而不仅仅是微调最后一个 Transformer 层：\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "为了方便起见，您可以通过以下命令运行此实验：\n",
    "\n",
    "```bash\n",
    "python additional-experiments.py --trainable_layers all\n",
    "```\n",
    "\n",
    "用这儿的的代码[../02_bonus_additional-experiments](../02_bonus_additional-experiments) 可以提升一个点!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269bce3-f2b5-4a76-a692-5977c75a57b6",
   "metadata": {},
   "source": [
    "## Exercise 6.3: 微调第一个token与最后一个token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418a629-51b6-4aa2-83b7-bc0261bc370f",
   "metadata": {},
   "source": [
    "我们可以通过将以下代码中的最后一个输出token改为第一个输出token来微调第一个token：\n",
    "\n",
    "```python\n",
    "model(input_batch)[:, -1, :]\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "model(input_batch)[:, 0, :]\n",
    "```\n",
    "\n",
    "在代码的所有相关位置进行修改。\n",
    "\n",
    "为了方便起见，您可以通过以下命令运行此实验：\n",
    "\n",
    "```\n",
    "python additional-experiments.py --trainable_token first\n",
    "```\n",
    "\n",
    "[../02_bonus_additional-experiments](../02_bonus_additional-experiments) 这里的代码将导致测试准确率大幅下降，降至 75.00%（在主章节中为 95.67%）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch06/01_main-chapter-code/load-finetuned-model.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1545a16b-bc8d-4e49-b9a6-db6631e7483d",
   "metadata": {
    "id": "1545a16b-bc8d-4e49-b9a6-db6631e7483d"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f83194-82b9-4478-9550-5ad793467bd0",
   "metadata": {
    "id": "f3f83194-82b9-4478-9550-5ad793467bd0"
   },
   "source": [
    "# 加载并使用预训练好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e",
   "metadata": {
    "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e"
   },
   "source": [
    "- 本笔记本包含 **尽可能少的代码**，用于加载在 **第 6 章** 中 **训练并保存的微调模型**，详见 [ch06.ipynb](ch06.ipynb)。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
    "outputId": "9eeefb8e-a7eb-4d62-cf78-c797b3ed4e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201",
   "metadata": {
    "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "finetuned_model_path = Path(\"review_classifier.pth\")\n",
    "if not finetuned_model_path.exists():\n",
    "    print(\n",
    "        f\"Could not find '{finetuned_model_path}'.\\n\"\n",
    "        \"Run the `ch06.ipynb` notebook to finetune and save the finetuned model.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb02584a-5e31-45d5-8377-794876907bc6",
   "metadata": {
    "id": "fb02584a-5e31-45d5-8377-794876907bc6"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# 初始化基础模型\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94",
   "metadata": {
    "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 将模型转换为分类器，如ch06.ipynb第6.5节所示\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
    "\n",
    "# 然后加载预训练权重\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"review_classifier.pth\", map_location=device, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5",
   "metadata": {
    "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2",
   "metadata": {
    "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2"
   },
   "outputs": [],
   "source": [
    "# 此函数已在ch06.ipynb中实现\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # 准备模型的输入\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "\n",
    "    # 如果序列过长，则进行截断\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # 填充序列以匹配最大序列长度\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # 添加批次维度\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor.to(device))[:, -1, :]  # 最后一个输出token的logits\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # 返回分类结果\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
    "outputId": "28eb2c02-0e38-4356-b2a3-2bf6accb5316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=120\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78472e05-cb4e-4ec4-82e8-23777aa90cf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78472e05-cb4e-4ec4-82e8-23777aa90cf8",
    "outputId": "0cd3cd62-f407-45f3-fa4f-51ff665355eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=120\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch06/03_bonus_imdb-classification/sklearn-baseline.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8968a681-2db1-4840-bb73-7d6c95986825",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e1cdd-b14e-4368-bdbb-9bf7ab821791",
   "metadata": {},
   "source": [
    "# Scikit-learn 逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a72242-6197-4bef-aa05-696a152350d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 80.23 MB | 4.37 MB/s | 18.38 sec elapsed"
     ]
    }
   ],
   "source": [
    "!python download-prepare-dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f32433-e19c-4066-b806-8f30b408107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"validation.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0808b212-fe91-48d9-80b8-55519f8835d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only reason I saw \"Shakedown\" was that it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is absolute drivel, designed to shock and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lots of scenes and dialogue are flat-out goofy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>** and 1/2 stars out of **** Lifeforce is one ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I learned a thing: you have to take this film ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The only reason I saw \"Shakedown\" was that it ...      0\n",
       "1  This is absolute drivel, designed to shock and...      0\n",
       "2  Lots of scenes and dialogue are flat-out goofy...      1\n",
       "3  ** and 1/2 stars out of **** Lifeforce is one ...      1\n",
       "4  I learned a thing: you have to take this film ...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae87bc1-14ca-4f89-8e12-49f77b0ec00d",
   "metadata": {},
   "source": [
    "## Scikit-learn 基线模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "180318b7-de18-4b05-b84a-ba97c72b9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25090b7c-f516-4be2-8083-3a7187fe4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_val = vectorizer.transform(val_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train, y_val, y_test = train_df[\"label\"], val_df[\"label\"], test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0247de3a-88f0-4b9c-becd-157baf3acf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # 进行预测\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # 计算准确率和平衡准确率\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "    balanced_accuracy_val = balanced_accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    balanced_accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Training Accuracy: {accuracy_train*100:.2f}%\")\n",
    "    print(f\"Validation Accuracy: {accuracy_val*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {accuracy_test*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c29c6dfc-f72d-40ab-8cb5-783aad1a15ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 50.01%\n",
      "Validation Accuracy: 50.14%\n",
      "Test Accuracy: 49.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# 创建一个虚拟分类器，其策略是预测频率最高的类别\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "eval(dummy_clf, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "088a8a3a-3b74-4d10-a51b-cb662569ae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.80%\n",
      "Validation Accuracy: 88.62%\n",
      "Test Accuracy: 88.85%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "eval(model, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/01_main-chapter-code/ch07.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e91914-5f51-43fa-b65b-625e73b4d17b",
   "metadata": {
    "id": "12e91914-5f51-43fa-b65b-625e73b4d17b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
   "metadata": {
    "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
   },
   "source": [
    "# 第七章：指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "9d937b84-d8f8-4ce9-cc3c-211188f49a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.7.1\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n",
      "tqdm version: 4.66.4\n",
      "tensorflow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"matplotlib\",  # 绘图库\n",
    "    \"tiktoken\",    # 分词器\n",
    "    \"torch\",       # 深度学习库\n",
    "    \"tqdm\",        # 进度条\n",
    "    \"tensorflow\",  # 用于加载OpenAI的预训练权重\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "# 读取并输出版本号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fca98-2f9a-4193-b435-2abfa3b4142f",
   "metadata": {
    "id": "264fca98-2f9a-4193-b435-2abfa3b4142f"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/1.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813",
   "metadata": {
    "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"
   },
   "source": [
    "## 7.1 指令微调的介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab",
   "metadata": {
    "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab"
   },
   "source": [
    "- 在第5章中，我们看到大语言模型的预训练是通过让模型学习逐个生成单词来实现的。\n",
    "- 由此可见,预训练的大语言模型擅长文本补全的任务，但不擅长执行指令。\n",
    "- 在本章中，我们将微调大语言模型使其更好地遵循指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc0535-0904-44ed-beaf-9b678292ef35",
   "metadata": {
    "id": "18dc0535-0904-44ed-beaf-9b678292ef35"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/2.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8",
   "metadata": {
    "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8"
   },
   "source": [
    "- 在下图中,你可以看到本章节所涉及的话题\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/3.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86",
   "metadata": {
    "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86"
   },
   "source": [
    "## 7.2 为有监督指令微调准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b34ff8-619f-4e89-bd03-ce513269760d",
   "metadata": {
    "id": "f8b34ff8-619f-4e89-bd03-ce513269760d"
   },
   "source": [
    "- 使用我为本章准备的一个指令数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0G3axLw6kY1N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G3axLw6kY1N",
    "outputId": "a5f70eb8-6248-4834-e7ae-6105e94e5afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "# 在网上下载并打开数据库\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "# 看一下数据一共有多少条"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "- 每个我们从上述 JSON 文件加载的 `data` 列表中的项都是一个字典，格式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "cc742019-b8d7-40f9-b21a-6a5ddf821377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])\n",
    "# 打印第51个json的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46",
   "metadata": {
    "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46"
   },
   "source": [
    "- 有时输入也可能是空的，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uFInFxDDk2Je",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFInFxDDk2Je",
    "outputId": "70241295-a9ec-4b7d-caf5-ab6f267e3271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034799a-6575-45fd-98c9-9d1012d0fd58",
   "metadata": {
    "id": "f034799a-6575-45fd-98c9-9d1012d0fd58"
   },
   "source": [
    "- 指令微调通常被称为“监督指令微调”，因为它涉及在一个数据集上训练模型，而该数据集中明确提供了输入-输出对。\n",
    "- 有多种不同的方法可以将样本制作为适应于大语言模型的格式；下图分别展示了两种用于训练Alpaca（[https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)）和Phi-3（[https://arxiv.org/abs/2404.14219](https://arxiv.org/abs/2404.14219)）LLM的示例格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10",
   "metadata": {
    "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/4.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6",
   "metadata": {
    "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6"
   },
   "source": [
    "- 在本章中，我们默认使用Alpaca风格的提示格式，这是一种较早公开并被广泛使用的指令微调提示模板。\n",
    "- 下面，我们将格式化输入内容，并将其作为输入传递给大语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    # 使用数据库的提示词\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    # 如果没有输入的格式，将如何处理\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    \n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6",
   "metadata": {
    "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6"
   },
   "source": [
    "- 格式化的回复如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "13ec7abf-ad94-4e26-860d-6a39a344f31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "# 先使用五十条数据进行测试\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c",
   "metadata": {
    "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c"
   },
   "source": [
    "- 以下是一个没有输入内容对应的格式化响应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
    "outputId": "d6be5713-1293-4a70-c8c8-a86ea8e95817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771",
   "metadata": {
    "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771"
   },
   "source": [
    "- 最后，在下一节中准备 PyTorch 数据加载器之前，我们将数据集划分为训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aFZVopbIlNfx",
   "metadata": {
    "id": "aFZVopbIlNfx"
   },
   "outputs": [],
   "source": [
    "# 自定义训练集、测试集和验证集的大小\n",
    "train_portion = int(len(data) * 0.85)  # 85% 作为训练集\n",
    "test_portion = int(len(data) * 0.1)    # 10% 作为测试集\n",
    "val_portion = len(data) - train_portion - test_portion  # 剩下的 5% 作为验证集\n",
    "\n",
    "# 划分数据集\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "bb5fe8e5-1ce5-4fca-a430-76ecf42e99ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "## 7.3 将数据组织成训练批次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f63bd-9755-4d07-8884-5e2e5345cf27",
   "metadata": {
    "id": "233f63bd-9755-4d07-8884-5e2e5345cf27"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/5.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c",
   "metadata": {
    "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c"
   },
   "source": [
    "- 下图总结了我们处理数据的几种方式\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/6.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af423f-aad9-4b3c-bea5-153021c04862",
   "metadata": {
    "id": "b9af423f-aad9-4b3c-bea5-153021c04862"
   },
   "source": [
    "- 首先，我们实现一个 `InstructionDataset` 类，它对数据集中的所有输入进行预分词，类似于第6章中的 `SpamDataset`。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/7.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    # 指示数据类的构建\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        # 实例化数据\n",
    "        # 对文本进行预编码\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    # 链表访问\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f0e69-4b22-41c0-a25d-f077527eddd1",
   "metadata": {
    "id": "384f0e69-4b22-41c0-a25d-f077527eddd1"
   },
   "source": [
    "- 与第6章类似, 为了加速训练, 我们希望将多个批次收集到同个训练轮次中, 这要求将所有输入填充到相同的长度。\n",
    "- 同样与上一章类似，我们使用 `<|endoftext|>` 作为填充 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
    "outputId": "4d63f8b8-b4ad-45d9-9e93-c9dd8c2b7706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# gpt2作为编码模型\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427",
   "metadata": {
    "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"
   },
   "source": [
    "- 在第6章中，我们将数据集中的所有例子填充为相同的长度。\n",
    "  - 而在这里，我们采取了一种更为复杂的方法: 开发了一个自定义的 \"collate\" 函数，并将其传递给数据加载器。\n",
    "  - 这个自定义的collate函数会将每个批次中的训练示例填充到相同的长度（不同批次的长度可以不同）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3",
   "metadata": {
    "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/8.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
   "metadata": {
    "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到批次中最长的序列\n",
    "    # 并将最大长度增加1，这样会在后面添加一个额外的填充 token\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    inputs_lst = []\n",
    "    # 准备输入\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        # 复制后进行填充\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 去掉最后一个表示并保存\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "    # 堆积起来并输送给gpu\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    \n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
    "outputId": "8705ca9a-e999-4f70-9db8-1ad084eba7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b",
   "metadata": {
    "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/9.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769a19-b961-4213-92ef-34f441b2d1d6",
   "metadata": {
    "id": "17769a19-b961-4213-92ef-34f441b2d1d6"
   },
   "source": [
    "- 上述内容中，我们仅返回了给大语言模型的输入。\n",
    "- 然而，对于大语言模型的训练，我们还需要目标值。\n",
    "- 与我们在预训练大语言模型时的做法相似，目标token序号与输入token序号相对应，但相比起来向右移动了一个位置（见下图），这样的设计使得大语言模型能够学习如何预测序列中的下一个词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef",
   "metadata": {
    "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/10.png\" width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
   "metadata": {
    "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到最大的序列长度\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    # 准备一个空列表\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 输入值是第一个到倒数第二个\n",
    "        inputs = torch.tensor(padded[:-1]) \n",
    "        # 目标值是第二个到最后一个，这样子保证了长度一样\n",
    "        targets = torch.tensor(padded[1:]) \n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
    "outputId": "b9ceae14-13c2-49f7-f4a4-b503f3db3009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15",
   "metadata": {
    "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15"
   },
   "source": [
    "- 接下来，我们引入了一个 `ignore_index` 值，用于将所有填充 token 的ID替换为一个新值；引入 `ignore_index` 的目的是使我们能够在损失函数中忽略填充值（稍后会详细讨论）。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/11.png\" width=500px>\n",
    "\n",
    "- 具体来说，这意味着我们将 `50256` 对应的 token ID替换为 `-100`，如图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bed33-956e-4b3f-a09c-586d8203109a",
   "metadata": {
    "id": "bd4bed33-956e-4b3f-a09c-586d8203109a"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/12.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346513e-c3f4-44fe-af22-4ebd36497728",
   "metadata": {
    "id": "5346513e-c3f4-44fe-af22-4ebd36497728"
   },
   "source": [
    "- （此外，我们还引入了 `allowed_max_length`，以支持“限制样本的长度”。如果您打算使用比GPT-2模型支持的1024个 token 上下文长度更长的数据集，这个设置将非常有用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到批次中最长的序列\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 填充并准备输入和目标\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 添加一个 <|endoftext|> token \n",
    "        new_item += [pad_token_id]\n",
    "        # 将序列填充到最大长度\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # 截断最后一个 token 作为输入\n",
    "        targets = torch.tensor(padded[1:])  # 向右移1个位置作为目标\n",
    "\n",
    "        # 新增：将目标中除了第一个填充 token 外的所有填充 token 替换为 ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # 新增：根据需要，限制序列的最大长度\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 将输入和目标的列表转换为张量，并转移到目标设备\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "a5501547-239d-431d-fb04-da7fa2ffad79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7",
   "metadata": {
    "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7"
   },
   "source": [
    "- 看看填充 token 替换为 -100 产生了什么效果。\n",
    "- 为了说明，假设我们有一个小型分类任务，包含两个类标签，0和1，类似于第6章的内容。\n",
    "- 如果有以下的logits值（模型最后一层的输出），我们可以计算出以下的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "W2jvh-OP9MFV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2jvh-OP9MFV",
    "outputId": "b5cd858e-7c58-4a21-c5a7-e72768bd301c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "# 两个训练的实例\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "# 计算交叉熵\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd3244-8886-4505-92e9-367d28529e1e",
   "metadata": {
    "id": "5edd3244-8886-4505-92e9-367d28529e1e"
   },
   "source": [
    "- 显然,多了一个token会影响loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nvVMuil89v9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvVMuil89v9N",
    "outputId": "e4a07b99-a23c-4404-ccdb-5f93c39f3b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # 新增第3个训练实例\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca331-40e0-468b-b690-189fe156ba8f",
   "metadata": {
    "id": "54dca331-40e0-468b-b690-189fe156ba8f"
   },
   "source": [
    "- 但是我们看看如果这个token变成了-100会怎样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "RTyB1vah9p56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTyB1vah9p56",
    "outputId": "28c16387-1d9c-48a7-eda7-aa270864683d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)\n",
    "# 综上所述、交叉熵会忽略-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef09d21-b652-4760-abea-4f76920e6a25",
   "metadata": {
    "id": "cef09d21-b652-4760-abea-4f76920e6a25"
   },
   "source": [
    "- 如上述所见，这3个训练样本计算得到的损失与我们从2个样本计算得到的损失相同，可以看出交叉熵损失函数忽略了带有 -100 标签的训练样本。\n",
    "- 默认情况下，PyTorch 的 `cross_entropy(..., ignore_index=-100)` 设置会忽略对应于标签 -100 的样本。\n",
    "- 使用这个 -100 的 `ignore_index`，我们可以忽略在批次中填充训练样本到相同长度时使用的额外结束 token（填充 token）。\n",
    "- 然而，我们忽略第一个结束 token（50256）也不是个好选择，因为这个 token 有助于向LLM发出**响应已完成**的信号。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524",
   "metadata": {
    "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524"
   },
   "source": [
    "- 除了屏蔽填充词元，实践中我们通常还会屏蔽与指令相关的目标token ID（这是本章节的练习）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39",
   "metadata": {
    "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/13.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "## 7.4 创建指令数据集的数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50",
   "metadata": {
    "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50"
   },
   "source": [
    "- 在本节中，我们使用 `InstructionDataset` 类和 `custom_collate_fn` 函数来实例化训练集、验证集和测试集数据加载器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffe390-b226-4d5c-983f-9f4da773cb82",
   "metadata": {
    "id": "9fffe390-b226-4d5c-983f-9f4da773cb82"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/14.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932677e9-9317-42e8-b461-7b0269518f97",
   "metadata": {
    "id": "932677e9-9317-42e8-b461-7b0269518f97"
   },
   "source": [
    "- 之前的 `custom_collate_fn` 函数的另一个改进之处在于，我们现在直接将数据移动到目标设备（例如GPU），而不是在主训练循环中执行。这提高了效率，因为当我们将 `custom_collate_fn` 作为数据加载器的一部分使用时，数据的移动可以在后台进行。\n",
    "- 我们使用 Python 标准库中的 `functools` 模块的 `partial` 函数，创建了一个新函数，其中原始函数的 `device` 参数已预先填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "925faf3a-6df4-4ad0-f276-f328493619c3"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3650425726.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    if torch.cuda.is_available():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意：\n",
    "# 如果适用，取消注释以下行将使代码能够在Apple Silicon芯片上运行，\n",
    "# 这比在Apple CPU上运行要快得多（在M3 MacBook Air上测得）。\n",
    "# 然而，计算得到的loss可能会略有不同。\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 初始化定义\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- 接下来，我们像之前的章节一样实例化数据加载器，唯一不同的是，我们现在为批处理过程提供了自定义的collate函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 初始化训练\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "# 初始化验证与测试\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "- 看看输入和输出批次的维度是怎样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "53a9695d-87cb-4d7c-8b43-1561dfa68ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657",
   "metadata": {
    "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657"
   },
   "source": [
    "- 如上面的输出所示，所有批次的批次大小为8，但长度各不相同，正如预期的那样。\n",
    "- 我们还可以通过输出 `inputs` 批次中第一个训练样本的内容，再次确认输入中包含了与 token ID 50256 对应的 `<|endoftext|>` 填充 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "ce919ecd-5ded-453c-a312-10cf55c13da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360",
   "metadata": {
    "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360"
   },
   "source": [
    "- 类似地，我们通过输出，直观地检查目标中是否包含 -100 占位符 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "fdf486f3-e99d-4891-9814-afc9e4991020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## 7.5 加载预训练的大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b",
   "metadata": {
    "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b"
   },
   "source": [
    "- GPT跟本书ch05和ch06章节演示是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4",
   "metadata": {
    "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/15.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2",
   "metadata": {
    "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2"
   },
   "source": [
    "- 然而，我们没有加载1.24亿参数的最小模型，而是选择了3.55亿参数的中型版本，因为1.24亿参数的模型对于通过指令微调获得合理的结果来说过于简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
    "outputId": "3f08f5e1-ca7c-406d-e2ae-1b5fcafad3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 02:22:49.969483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 02:22:50.023103: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-25 02:22:50.023136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-25 02:22:50.024611: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-25 02:22:50.033304: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 02:22:51.282247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 169kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.43MiB/s]\n",
      "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 168kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [00:56<00:00, 25.0MiB/s]\n",
      "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 16.5MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 1.96MiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.53MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
   "metadata": {
    "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
   },
   "source": [
    "- 在下一节开始微调模型之前，我们先来看一下它在一个验证集数据上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
    "outputId": "30d4fbd9-7d22-4545-cfc5-c5749cc0bd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
   "metadata": {
    "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2fda5-f796-4954-8f72-1dd1123e3344",
   "metadata": {
    "id": "36e2fda5-f796-4954-8f72-1dd1123e3344"
   },
   "source": [
    "- 请注意，之前章节中使用的 `generate` 函数返回的是输入和输出文本的合并结果，这在上一节中便于生成可读的文本。\n",
    "- 为了提取响应，我们可以从 `generated_text` 的开头减去指令部分获得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
    "outputId": "b46de9b3-98f0-45e4-a9ae-86870c3244a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    # 从生成的文本开始计数\n",
    "    generated_text[len(input_text):]\n",
    "    #如果生成的文本包含 `### Response:`，则删除它\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    #去掉空格\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
   "metadata": {
    "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
   },
   "source": [
    "- 如我们所见，模型还无法正确地执行指令，但它创建了一个“response”部分，虽然只是简单地重复了原始输入句子和指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## 7.6 在指令数据上微调大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a",
   "metadata": {
    "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a"
   },
   "source": [
    "- 在这一部分,我们将要微调模型\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/16.png\" width=500px>\n",
    "\n",
    "- 之前所使用的loss函数和训练函数我们都可以复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65444865-df87-4d98-9faf-875e1c4be860",
   "metadata": {
    "id": "65444865-df87-4d98-9faf-875e1c4be860"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "- 在开始训练之前，让我们计算初始的训练集和验证集损失（与之前章节一样，目标是最小化损失）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "36fdf03b-6fa6-46c3-c77d-ecc99e886265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.82590970993042\n",
      "Validation loss: 3.761933755874634\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "# 先看一次没有微调的结果\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9",
   "metadata": {
    "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9"
   },
   "source": [
    "- 因为模型更大了,我们的计算成本就比之前高了不少\n",
    "- 下表列出了不同设备运行该模型的时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b57fb-e689-4550-931c-6d34a932487c",
   "metadata": {
    "id": "db4b57fb-e689-4550-931c-6d34a932487c"
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Model              | Device                | Runtime for 2 Epochs |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "cea0618c-56ca-418a-c972-bcc060362727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.728\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.503, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.666\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.687\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.683\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.682\n",
      "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.680\n",
      "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675\n",
      "Ep 2 (Step 000155): Train loss 0.413, Val loss 0.675\n",
      "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.683\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.681\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.669\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.657\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.648\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.634\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.634\n",
      "Ep 2 (Step 000205): Train loss 0.352, Val loss 0.630\n",
      "Ep 2 (Step 000210): Train loss 0.367, Val loss 0.630\n",
      "Ep 2 (Step 000215): Train loss 0.394, Val loss 0.635\n",
      "Ep 2 (Step 000220): Train loss 0.299, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.346, Val loss 0.661\n",
      "Ep 2 (Step 000230): Train loss 0.292, Val loss 0.659\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 1.84 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 用Adam训练,并定义了学习率、权重衰减等参数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ise3wGjlB-iq",
   "metadata": {
    "id": "Ise3wGjlB-iq"
   },
   "source": [
    "- 从上面的输出可以看出，模型训练得很好，训练损失和验证损失值不断下降。\n",
    "- 此外，从每个epoch结束后输出的响应文本来看，我们可以看到模型正确地执行了指令，将输入句子 `'The chef cooks the meal every day.'` 转换为被动语态 `'The meal is cooked every day by the chef.'`（我们将在后续章节中对响应进行适当的格式化和评估）。\n",
    "- 最后，让我们看看训练损失和验证损失曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
    "outputId": "680da58a-9bd7-402d-ac95-470a4a29a6c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0",
   "metadata": {
    "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0"
   },
   "source": [
    "- 如我们所见，在第一个训练轮次的开始，损失急剧下降，这意味着模型开始迅速学习。\n",
    "- 大约在训练1个训练轮次时，模型出现了轻微的过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3",
   "metadata": {
    "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3"
   },
   "source": [
    "## 7.7 抽取并保存模型回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49",
   "metadata": {
    "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/17.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427",
   "metadata": {
    "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427"
   },
   "source": [
    "- 在本节中，我们保存测试集的响应，以便在下一节进行评估。\n",
    "- 我们还保存了模型的副本，以备将来使用。\n",
    "- 但首先，让我们粗略的查看一下微调后的模型生成的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "8416b4ac-1993-4628-dea6-7789cdc8926c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab64c1-586f-4939-8def-23feeb1b3599",
   "metadata": {
    "id": "49ab64c1-586f-4939-8def-23feeb1b3599"
   },
   "source": [
    "- 从测试集中的指令、给定的响应以及模型的响应来看，模型的表现相对较好。\n",
    "- 第一个和最后一个指令的回答显然是正确的。\n",
    "- 第二个回答接近正确；模型回答为“积云”（cumulus cloud），而不是“雷雨云”（cumulonimbus）（不过需要注意的是，积云可能发展成雷雨云，而雷雨云具有产生雷暴的能力）。\n",
    "- 最重要的是，我们可以看到，模型评估结果不像第六章那样直接，因为在第六章中我们只需要计算正确的垃圾邮件/非垃圾邮件类别标签的百分比来获得分类准确率。\n",
    "- 实际上，指令微调后的LLM（如聊天机器人）通常通过多种方法进行评估：\n",
    "  - 短答案和多选基准，如MMLU（“大规模多任务语言理解测量”，[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)），测试模型的知识。\n",
    "  - 与其他LLM的人工偏好比较，如LMSYS聊天机器人竞技场（[https://arena.lmsys.org](https://arena.lmsys.org)）。\n",
    "  - 自动化对话基准，其中使用另一个LLM，如GPT-4，来评估响应，例如AlpacaEval（[https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/)）。\n",
    "- 在下一节中，我们将使用类似AlpacaEval的方法，使用另一个LLM来评估我们模型的响应；不过，我们将使用自己的测试集，而不是公开可用的基准数据集。\n",
    "- 为此，我们将模型的响应添加到 `test_data` 字典中，并将其保存为 `\"instruction-data-with-response.json\"` 文件，以便记录，这样我们可以在需要时在单独的Python会话中加载并分析它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "-PNGKzY4snKP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PNGKzY4snKP",
    "outputId": "0453dfb3-51cd-49e2-9e63-f65b606c3478"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:11<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\"设置用于美化输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d6fa7-d162-44c3-bef1-4013c027b155",
   "metadata": {
    "id": "228d6fa7-d162-44c3-bef1-4013c027b155"
   },
   "source": [
    "- 让我们再检查一下其中一个条目，确认回复是否已正确添加到 `test_data` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "u-AvCCMTnPSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-AvCCMTnPSE",
    "outputId": "ce3b2545-8990-4446-e44c-a945e0049c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "- 最后保存这个模型以便日后复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "d6e7f226-9310-43f5-f31f-adc3a893a8e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# Load model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obgoGI89dgPm",
   "metadata": {
    "id": "obgoGI89dgPm"
   },
   "source": [
    "## 7.8 评价微调后的大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9d30-7336-499f-abb5-4a21be3129f5",
   "metadata": {
    "id": "805b9d30-7336-499f-abb5-4a21be3129f5"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/18.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1",
   "metadata": {
    "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1"
   },
   "source": [
    "- 在本节中，我们通过使用另一个更强大的LLM来自动化微调后的LLM的响应评估。\n",
    "- 具体来说，我们使用了Meta AI发布的8B参数指令微调Llama 3模型，该模型可以通过ollama本地运行（[https://ollama.com](https://ollama.com)）。\n",
    "- （另外，如果您更喜欢通过OpenAI API使用像GPT-4这样的更强大的LLM，请参阅 [llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb) ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9",
   "metadata": {
    "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9"
   },
   "source": [
    "- Ollama 是一个高效运行LLM的程序。\n",
    "- 它是 llama.cpp 的一个封装器（[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)），该项目使用纯C/C++实现LLM，以最大化效率。\n",
    "- 请注意，Ollama 是用于生成文本（进行模型推理）的工具，而不是用于训练或微调LLM的工具。\n",
    "- 在运行以下代码之前，请访问 [https://ollama.com](https://ollama.com) 安装Ollama，并按照指示操作（例如，点击“下载”按钮并下载适用于您操作系统的Ollama应用程序）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822",
   "metadata": {
    "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822"
   },
   "source": [
    "- 对于macOS和Windows用户，点击已下载的ollama应用程序；如果提示安装命令行工具，选择“是”。\n",
    "- Linux用户可以使用ollama网站上提供的安装命令。\n",
    "\n",
    "- 通常，在我们通过命令行使用ollama之前，需要先启动ollama应用程序或在单独的终端中运行 `ollama serve`。\n",
    "\n",
    "<img src=\"../image/ollama-run.webp\" width=700px>\n",
    "\n",
    "- 在另一个终端运行ollama应用程序或 `ollama serve` 后，在命令行中执行以下命令，尝试使用8B参数的Llama 3模型（该模型占用4.7GB的存储空间，第一次执行此命令时会自动下载）。\n",
    "```bash\n",
    "# 8B 模型\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "\n",
    "输出可能如下所示\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest\n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "- 请注意，`llama3` 指的是经过指令微调的8B参数Llama 3模型。\n",
    "\n",
    "- 使用ollama与 `\"llama3\"` 模型（8B参数模型）时需要16GB的RAM；如果您的机器不支持，可以尝试使用更小的模型，例如3.8B参数的phi-3模型，通过设置 `model = \"phi-3\"`，该模型只需8GB的RAM。\n",
    "\n",
    "- 如果您的机器支持，您还可以使用更大的70B参数Llama 3模型，只需将 `llama3` 替换为 `llama3:70b`。\n",
    "\n",
    "- 下载完成后，您将看到一个命令行提示符，可以与模型进行对话。\n",
    "\n",
    "- 尝试输入类似 \"What do llamas eat?\" 的提示，模型应返回类似以下的输出：\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered\n",
    "stomach and eat plants that are high in fiber. In the wild, llamas\n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall\n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4",
   "metadata": {
    "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4"
   },
   "source": [
    "- 结束运行仅需要输入 `/bye`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3",
   "metadata": {
    "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3"
   },
   "source": [
    "- 以下代码检查ollama会话是否正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026e8570-071e-48a2-aa38-64d7be35f288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "026e8570-071e-48a2-aa38-64d7be35f288",
    "outputId": "e30d3533-e1f5-4aa9-b24f-33273fc7b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "# 检查运行情况\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0",
   "metadata": {
    "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0"
   },
   "outputs": [],
   "source": [
    "# 该单元格是可选的；它允许您重新启动notebook\n",
    "# 并只运行第7.7节，而无需重新运行之前的代码。\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# 我们的初始数据集\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3464705-d026-4594-977f-fb357e51c3a9",
   "metadata": {
    "id": "b3464705-d026-4594-977f-fb357e51c3a9"
   },
   "source": [
    "- 现在，与我们之前使用的 `ollama run` 命令互动模型的另一种方式是通过其REST API，在Python中通过以下函数进行操作。\n",
    "- 在运行notebook中的下一个单元格之前，请确保ollama仍在运行（前面的代码单元格应显示 `\"Ollama running: True\"`）。\n",
    "- 接下来，运行以下代码单元格来查询模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
   "metadata": {
    "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # 预处理的类型与符号\n",
    "    # 创建数据负载字典\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # 以下设置是为了获得确定性的响应\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    # 将字典转换为JSON格式的字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    # 对JSON进行编码\n",
    "    # 创建请求对象，设置请求方法为POST并添加必要的头部信息\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    # 设置请求头部为JSON格式\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并捕获响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc",
   "metadata": {
    "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc"
   },
   "source": [
    "- 现在，使用我们上面定义的 `query_model` 函数，我们可以评估微调后的模型的响应；让我们在前面一节中查看的前三个测试集响应上试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
   "metadata": {
    "id": "86b839d4-064d-4178-b2d7-01691b452e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a bullet.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).\n",
      "* The comparison is relevant and makes sense, as bullets are known for their high velocity.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that some people might find the comparison slightly less vivid or evocative than others. For example, comparing something to lightning (as in the original response) can be more dramatic and attention-grabbing. However, \"as fast as a bullet\" is still a strong and effective simile that effectively conveys the idea of the car's speed.\n",
      "\n",
      "Overall, I think the model did a great job!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 40 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).\n",
      "* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.\n",
      "* The response lacks precision and accuracy in its description.\n",
      "\n",
      "Overall, while the model attempts to address the instruction, it provides an incorrect answer, which is a significant error.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 95 out of 100. Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and clear, making it easy to understand.\n",
      "* There are no grammatical errors or ambiguities that could lead to confusion.\n",
      "\n",
      "The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not necessary to rephrase the question in the answer. A more concise response would be simply \"Jane Austen.\"\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    # 自定义提示词\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3",
   "metadata": {
    "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3"
   },
   "source": [
    "- 如我们所见，Llama 3 模型给出了合理的评估\n",
    "- 如果模型的回答不完全正确，它会根据部分正确的内容给予相应的分数，例如“积云”这个回答。\n",
    "- 请注意，之前的提示会返回详细的评估结果；我们可以调整提示，使其生成介于0到100之间的整数分数（其中100为最佳），以便计算模型的平均得分。\n",
    "- 对测试集中的110个条目进行评估大约需要1分钟(在M3 MacBook Air上运行)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
   "metadata": {
    "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 110/110 [01:10<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 50.32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成模型得分的函数\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    \n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2",
   "metadata": {
    "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2"
   },
   "source": [
    "- 我们的模型平均得分超过50分，可以将其作为参考，与其他模型进行对比;或尝试其他训练设置来改进模型表现。\n",
    "- 截至汉化的时候,ollama 在不同操作系统上的结果可能会有所不同，因此您得到的数值可能与上面显示的略有差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94",
   "metadata": {
    "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94"
   },
   "source": [
    "- 以下看作参考\n",
    "  - Llama 3 8B 基础模型得分为 58.51\n",
    "  - Llama 3 8B 指令微调模型得分为 82.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d7325-284a-446c-92a1-5aa8acc52dee",
   "metadata": {
    "id": "412d7325-284a-446c-92a1-5aa8acc52dee"
   },
   "source": [
    "## 7.9 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tIbNMluCDjVM",
   "metadata": {
    "id": "tIbNMluCDjVM"
   },
   "source": [
    "### 7.9.1 展望\n",
    "\n",
    "- 本章标志着本书的结束。\n",
    "- 我们覆盖了LLM开发的主要步骤：实现LLM架构、预训练LLM以及微调模型。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/19.png\" width=500px>\n",
    "\n",
    "- 正如本章所述,指令微调后，有时会进行一个可选步骤——偏好微调.\n",
    "- 偏好微调有助于定制模型，更好地符合用户偏好。如果您感兴趣，可以查看 [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) 文件夹。\n",
    "\n",
    "- 本GitHub仓库还包含大量额外的附加资料，您可能会喜欢；有关更多信息，请查看本仓库README页面中的 [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分。\n",
    "\n",
    "### 7.9.2 紧跟时代潮流\n",
    "\n",
    "- 本节没有代码内容。\n",
    "\n",
    "### 7.9.3 寄语\n",
    "\n",
    "- 我希望您喜欢这个从零实现LLM的过程，以及编写预训练和微调代码的过程。\n",
    "- 在我看来，从零构建大模型是理解LLM如何工作的最佳方式；我希望您通过这种方式获得了更深入的理解。\n",
    "- 虽然本书是为了教学目的，但您可能有兴趣将不同的、更强大的LLM应用于实际应用中。\n",
    "  - 为此，您可以考虑使用一些流行工具，如axolotl（[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)）或LitGPT（[https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)），这是我参与开发的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9853e7f-a81a-4806-9728-be1690807185",
   "metadata": {
    "id": "f9853e7f-a81a-4806-9728-be1690807185"
   },
   "source": [
    "## 总结与收获\n",
    "\n",
    "- 请参阅 [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) 脚本，它是一个自包含的分类微调脚本。\n",
    "- [./ollama_evaluate.py](./ollama_evaluate.py) 是基于第7.8节的独立脚本，通过Ollama和Llama 3评估包含“output”和“response”键的JSON文件。\n",
    "- [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) 笔记本演示了如何在新会话中加载微调后的模型。\n",
    "- 您可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 找到习题解答。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc51ec-e06c-4470-b626-48401a037851",
   "metadata": {},
   "source": [
    "## 接下来做什么？\n",
    "\n",
    "- 恭喜您完成了本书！如果您在寻找更多资源，我在这个GitHub仓库中添加了几个附加部分，您可能会感兴趣。\n",
    "- 附加资料的完整列表可以在主README的 [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分查看。\n",
    "- 在这里，我想特别强调几个我喜欢的部分：\n",
    "  1. [从零开始的直接偏好优化（DPO）用于LLM对齐](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) 实现了一种流行的偏好调优机制，使本章的模型与人类偏好更加契合。\n",
    "  2. [从零开始实现Llama 3.2](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb)，这是Meta AI流行的Llama 3.2的从零实现，包括加载官方的预训练权重；如果您想做一些额外的实验，可以将每章中的 `GPTModel` 模型替换为 `Llama3Model` 类（它可以作为1:1替代）。\n",
    "  3. [将GPT转换为Llama](../../ch05/07_gpt_to_llama) 包含逐步指南的代码，解释了GPT-2和各种Llama模型之间的区别。\n",
    "  4. [理解嵌入层和线性层的区别](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) 是一个概念性解释，说明了我们在LLM输入阶段使用的PyTorch中的 `Embedding` 层在数学上等价于对独热编码数据应用的线性层。\n",
    "- 祝您学习愉快！"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/01_main-chapter-code/exercise-solutions.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 7 课后练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
   "metadata": {},
   "source": [
    "## Exercise 7.1: 改变prompt风格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be25a95-2a33-433b-a698-2365b5fc9357",
   "metadata": {},
   "source": [
    "假如我们有如下json内容\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Identify the correct spelling of the following word.\",\n",
    "  \"input\": \"Ocassion\",\n",
    "  \"output\": \"The correct spelling is 'Occasion.'\"\n",
    "}\n",
    "```\n",
    "\n",
    "在主章节中，我们按照Alpaca风格的prompt模板进行了格式化。\n",
    "\n",
    "```\n",
    "下面是一个描述任务的指令。编写一个适当完成请求的响应。\n",
    "\n",
    "### 指令：\n",
    "找出下列单词的正确拼写。\n",
    "\n",
    "### 输入：\n",
    "Occassion\n",
    "\n",
    "### 响应：\n",
    "正确的拼写是 'Occasion.'\n",
    "```\n",
    "\n",
    "在这个练习中，我们改为使用 Phi-3 提示模板，把数据条目格式化：\n",
    "如下\n",
    "\n",
    "```\n",
    "### 指令：\n",
    "找出下列单词的正确拼写：'Occasion'\n",
    "\n",
    "### 响应：\n",
    "正确的拼写是 'Occasion'。\n",
    "```\n",
    "\n",
    "请注意，这个提示模板明显更简短，这减少了微调LLM和生成文本的运行时间和硬件需求，因为输入提示更短。\n",
    "为了进行此更改，我们将 `format_input` 函数更新如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50",
   "metadata": {},
   "source": [
    "我们通过应用于两个输入样本来确保它正常工作，一个有 `'input'` 字段内容，一个没有。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877a57e2-535f-4363-b32a-a093edd951b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Identify the correct spelling of the following word.\n",
      "Ocassion\n",
      "\n",
      "<|user|>\n",
      "What is an antonym of 'complicated'?\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\n",
    "    {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}, \n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n",
    "]\n",
    "\n",
    "print(format_input(sample_data[0]))\n",
    "print()\n",
    "print(format_input(sample_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3",
   "metadata": {},
   "source": [
    "接下来，我们更新 `InstructionDataset` 类，使用 `<|endoftext|>` 提示模板来处理响应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f1a42c-7cc0-4746-8a6d-3a4cb37e2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # 预先分词化文本\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "\n",
    "            ###################################################################\n",
    "            # 新增：使用 `format_input_phi` 并调整响应文本模板\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n<|assistant|>:\\n{entry['output']}\"\n",
    "            ###################################################################\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0650926-c39f-4442-8116-cb7494416f28",
   "metadata": {},
   "source": [
    "最后，我们还需要更新提取生成响应的方式，当我们收集测试集的响应时："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb",
   "metadata": {},
   "source": [
    "```python\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "    tokenizer=tokenizer\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    # 新增: 调整提取关键词 ###Response -> <|assistant|>\n",
    "    response_text = generated_text[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd557c-3838-45e4-a26a-baed4b11175a",
   "metadata": {},
   "source": [
    "为了方便起见，习题解答已在 [exercise_experiments.py](exercise_experiments.py) 脚本中实现，您可以按如下方式运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution phi3_prompt\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 3.71630220413208\n",
    "   Validation loss: 3.6440994262695314\n",
    "Ep 1 (Step 000000): Train loss 2.633, Val loss 2.622\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.424, Val loss 0.928\n",
    "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.' <|assistant|>: The meal is prepared every day by the chef....\n",
    "Training completed in 1.50 minutes.\n",
    "Plot saved as loss-plot-phi3-prompt.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [00:11<00:00,  9.27it/s]\n",
    "Responses saved as instruction-data-with-response-phi3-prompt.json\n",
    "Model saved as gpt2-medium355M-sft-phi3-prompt.pth\n",
    "```\n",
    "\n",
    "作为比较，您可以通过运行原始的第7章微调代码 `python exercise_experiments.py --exercise_solution baseline` 来查看。\n",
    "\n",
    "请注意，在Nvidia L4 GPU上，使用Phi-3提示模板的代码运行时间为1.5分钟。而与此相比，使用Alpaca风格模板的运行时间为1.80分钟。因此，Phi-3模板大约快了17%，因为它导致了更短的模型输入。\n",
    "\n",
    "让我们来看一些响应，确保它们已正确格式化：\n",
    "\n",
    "```json\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the sentence using a simile.\",\n",
    "        \"input\": \"The car is very fast.\",\n",
    "        \"output\": \"The car is as fast as lightning.\",\n",
    "        \"model_response\": \"The car is as fast as a cheetah.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What type of cloud is typically associated with thunderstorms?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The type of cloud typically associated with thunderstorms is cumulonimbus.\",\n",
    "        \"model_response\": \"The type of cloud associated with thunderstorms is a cumulus cloud.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Name the author of 'Pride and Prejudice'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Jane Austen.\",\n",
    "        \"model_response\": \"The author of 'Pride and Prejudice' is Jane Austen.\"\n",
    "    },\n",
    "```\n",
    "\n",
    "我们可以使用Ollama Llama 3方法评估性能，出于方便考虑，该方法也已在 `python exercise_experiments.py` 脚本中实现，您可以按如下方式运行：\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-phi3-prompt.json\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|████████████████████████| 110/110 [01:08<00:00,  1.60it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 48.87\n",
    "```\n",
    "\n",
    "得分接近50，和我们之前使用Alpaca风格提示时取得的得分相近。\n",
    "\n",
    "没有固有的优势或理由说明为什么 Phi 提示风格会更好，但它可以更简洁、更高效，除了下面提示部分提到的警告。\n",
    "\n",
    "**提示：考虑特殊标记**\n",
    "\n",
    "- 请注意，Phi-3 提示模板包含特殊标记，例如 `<|user|>` 和 `<|assistant|>`，这对于 GPT-2 编码器来说可能不是最佳选择\n",
    "- 虽然 GPT-2 编码器将 `<|endoftext|>` 识别为特殊标记（编码为token ID 50256），但它在处理其他特殊标记（例如上述标记）时效率低下\n",
    "- 例如，`<|user|>` 被编码为 5 个单独的token ID（27、91、7220、91、29），效率非常低下\n",
    "- 我们可以通过 allowed_special 参数将 `<|user|>` 作为 tiktoken 中的新特殊标记添加，但请记住，如果不进行额外修改，GPT-2 词汇表将无法处理它\n",
    "- 如果您对如何扩展标记器和 LLM 来处理特殊标记感到好奇，请参阅 [extend-tiktoken.ipynb](https://github.com/MLNLP-World/LLMs-from-scratch-CN/blob/main/ch05/09_extending-tokenizers) 奖励内容（请注意，这不是此处必需的，但只是对好奇的读者来说一个有趣/额外的考虑）\n",
    "- 此外，我们可以假设，通过词汇表支持提示模板的这些特殊标记的模型可能表现得更高效，总体上也更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 练习 7.2：指令和输入掩蔽(mask)\n",
    "\n",
    "为了像下图所示掩蔽指令，我们需要对 `InstructionDataset` 类和 `custom_collate_fn` 进行一些小的修改。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/20.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405196a-db81-470b-be39-167a059587b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个 `format_input` 函数是从原始的第7章代码中复制的\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83658c09-af8a-425a-b940-eb1f06e43c0b",
   "metadata": {},
   "source": [
    "我们可以修改 `InstructionDataset` 类来收集指令的长度，稍后在编写 `collate` 函数时，我们将使用这些长度来定位目标中的指令内容位置，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        ##########################################################################################\n",
    "        # 新增：用于存储指令长度的列表\n",
    "        self.instruction_lengths = []\n",
    "        ##########################################################################################\n",
    "        \n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "            ##########################################################################################\n",
    "            # 新增：收集指令长度\n",
    "            instruction_length = len(tokenizer.encode(instruction_plus_input))\n",
    "            self.instruction_lengths.append(instruction_length)\n",
    "            ##########################################################################################\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 新增：分别返回指令长度和文本\n",
    "        return self.instruction_lengths[index], self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a186394-4960-424d-bb6a-f58459dd5994",
   "metadata": {},
   "source": [
    "接下来，我们更新 `custom_collate_fn`，由于在 `InstructionDataset` 数据集中进行了修改，每个 `batch` 现在是一个包含 `(instruction_length, item)` 的元组，而不仅仅是 `item`。此外，我们现在在目标ID列表中掩蔽相应的指令 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到批次中最长的序列\n",
    "    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # 新增：批次现在是一个元组\n",
    "\n",
    "    # 填充并准备输入和目标\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for instruction_length, item in batch:  # 新增：批次现在是一个元组\n",
    "        new_item = item.copy()\n",
    "        # 添加一个 <|endoftext|> 标记\n",
    "        new_item += [pad_token_id]\n",
    "        # 将序列填充到最大长度\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # 截取最后一个标记作为输入\n",
    "        targets = torch.tensor(padded[1:])  # 向右移动 1个位置作为目标\n",
    "\n",
    "        # 将目标中除第一个填充标记外的所有填充标记替换为 ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        ##########################################################################################\n",
    "        # 新增：屏蔽目标和指令标记\n",
    "        targets[:instruction_length-1] = -100\n",
    "        ##########################################################################################\n",
    "        \n",
    "        # 可选：截断到最大序列长度\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 将输入和目标列表转换为张量并传输到目标设备\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57",
   "metadata": {},
   "source": [
    "试试下面指出的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n",
    "    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n",
    "    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InstructionDataset(sample_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(sample_data),\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([3, 64]) torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n",
      "          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n",
      "        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n",
      "        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n",
      "         1168, 37052, 50256, 50256])\n",
      "\n",
      "\n",
      "Targets:\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n",
      "           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n",
      "        37052, 50256,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\\n\", inputs[1])\n",
    "print(\"\\n\\nTargets:\\n\", targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628",
   "metadata": {},
   "source": [
    "从 `targets` 可以看出，指令和填充 token 现在都使用 -100 占位符 token 进行了掩蔽。\n",
    "让我们解码(decode)输入，以确保它们看起来正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Zebra, Elephant, Crocodile\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(list(inputs[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd",
   "metadata": {},
   "source": [
    "解码非-100的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "non_masked_targets = targets[1][targets[1] != -100]\n",
    "\n",
    "print(tokenizer.decode(list(non_masked_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6",
   "metadata": {},
   "source": [
    "如上所示，未被掩蔽的目标 token 排除了 `\"Instruction\"` 和 `\"Input\"` 字段，达到了预期效果。现在，我们可以运行修改后的代码，看看使用这种掩蔽策略进行微调时，LLM 的表现如何。\n",
    "\n",
    "为了方便起见，您可以使用 `exercise_experiments.py` 代码进行比较，按如下方式运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a76097-9114-479d-8803-443b0ff48581",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution mask_instructions\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 2.280539035797119\n",
    "   Validation loss: 2.262560224533081\n",
    "Ep 1 (Step 000000): Train loss 1.636, Val loss 1.620\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.143, Val loss 0.727\n",
    "...\n",
    "Training completed in 1.77 minutes.\n",
    "Plot saved as loss-plot-mask-instructions.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [02:10<00:00,  1.19s/it]\n",
    "Responses saved as instruction-data-with-response-mask-instructions.json\n",
    "Model saved as gpt2-medium355M-sft-mask-instructions.pth\n",
    "```\n",
    "\n",
    "Next, let's evaluate the performance of the resulting LLM:\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-mask-instructions.json\n",
    "```\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|██████████████████████████████████████████████████████████████████████████████████████| 110/110 [01:23<00:00,  1.31it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 47.73\n",
    "```\n",
    "\n",
    "从得分可以看出，指令掩蔽的效果略差，这与《Instruction Tuning With Loss Over Instructions》论文中的观察结果一致（https://arxiv.org/abs/2405.14394）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 7.3: 在最初的Alpaca数据上进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55",
   "metadata": {},
   "source": [
    "要在原始的斯坦福Alpaca数据集上微调模型（[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)），只需将文件URL更改为：\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
    "```\n",
    "\n",
    "请注意，该数据集包含52k条记录（比第7章多50倍），且记录长度比我们在第7章中使用的要长。因此，强烈建议在GPU上运行训练。\n",
    "\n",
    "如果遇到内存不足的错误，建议将批次大小从8减少到4、2或1。除了降低批次大小外，您还可以考虑将 `allowed_max_length` 从1024降低到512或256。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c",
   "metadata": {},
   "source": [
    "为了方便起见，您可以使用 `exercise_experiments.py` 代码，以批次大小为4和 `allowed_max_length` 为512的设置，在52k的Alpaca数据集上微调模型，按如下方式运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution alpaca_52k\n",
    "```\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 44201\n",
    "Validation set length: 2601\n",
    "Test set length: 5200\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 3.3681655883789063\n",
    "   Validation loss: 3.4122894287109373\n",
    "Ep 1 (Step 000000): Train loss 2.477, Val loss 2.750\n",
    "...\n",
    "Ep 2 (Step 022095): Train loss 0.761, Val loss 1.557\n",
    "...\n",
    "Training completed in 196.38 minutes.\n",
    "Plot saved as loss-plot-alpaca52k.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 5200/5200 [2:56:33<00:00,  2.04s/it]\n",
    "Responses saved as instruction-data-with-response-alpaca52k.json\n",
    "Model saved as gpt2-medium355M-sft-alpaca52k.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7",
   "metadata": {},
   "source": [
    "下面是一点例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8",
   "metadata": {},
   "source": [
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Edit the following sentence to increase readability: \\\"He made a huge effort and was so successful.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"He exerted a tremendous effort, and thus enjoyed great success.\",\n",
    "        \"model_response\": \"He put in an immense effort and was rewarded with success.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the following sentence to make it more concise: \\\"I was displeased with the result of the experiment that I conducted.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I was unhappy with my experiment's outcome.\",\n",
    "        \"model_response\": \"I was displeased with the results of the experiment.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we build a more efficient GPT model?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"We can build a more efficient GPT model by optimizing the architecture of the model, using smaller model sizes and training with fewer parameters. We can also leverage techniques such as knowledge distillation, transfer learning, dynamic sparsity and hybrid computing to further improve the efficiency of the model.\",\n",
    "        \"model_response\": \"Building a more efficient GPT model requires careful planning and optimization. First, it is important to identify the target language and the context in which the model is used. Then, it is important to select the appropriate model architecture, such as backpropagation, hyperparameters, and hyperparameters. Finally, it is important to select the appropriate model weights and optimizers, such as backpropagation, hyperparameters, and hyperparameters.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e2076-f2e6-44a2-86fd-f191f9674267",
   "metadata": {},
   "source": [
    "我们用这个来评估 [ollama_evaluate.py](ollama_evaluate.py)\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-alpaca52k.json\n",
    "```\n",
    "\n",
    "```\n",
    "Scoring entries: 100%|████████████████████| 5200/5200 [1:07:52<00:00, 1.28it/s]\n",
    "Number of scores: 5188 of 5200\n",
    "Average score: 48.16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4",
   "metadata": {},
   "source": [
    "得分略低于我们在本章中使用的数据集上获得的得分。不过，请注意，Alpaca测试集包含比我们在本章中使用的数据集更具多样性和部分更具挑战性的指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30",
   "metadata": {},
   "source": [
    "## 练习 7.4：使用LoRA进行参数高效微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01742cec-1f41-4415-8788-009d31b1ad38",
   "metadata": {},
   "source": [
    "要使用LoRA对模型进行指令微调，请使用附录E中的相关类和函数：\n",
    "\n",
    "```python\n",
    "from appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac",
   "metadata": {},
   "source": [
    "把7.5部分中的代码加进来\n",
    "\n",
    "\n",
    "```python\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")\n",
    "model.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26b925-dc95-4b91-b050-9676dd9608a4",
   "metadata": {},
   "source": [
    "为了方便起见，您可以使用 `exercise_experiments.py` 代码，使用LoRA（秩为16，alpha为16）对模型进行微调，按如下方式运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f02c7e-3b15-44b8-bf41-7892cd755766",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution lora\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
    "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
    "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
    "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Total trainable parameters before: 406,286,336\n",
    "Total trainable parameters after: 0\n",
    "Total trainable LoRA parameters: 7,898,384\n",
    "Initial losses\n",
    "   Training loss: 3.7684114456176756\n",
    "   Validation loss: 3.7619335651397705\n",
    "Ep 1 (Step 000000): Train loss 2.509, Val loss 2.519\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.308, Val loss 0.652\n",
    "...\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [01:52<00:00,  1.03s/it]\n",
    "Responses saved as instruction-data-with-response-lora.json\n",
    "Model saved as gpt2-medium355M-sft-lora.pth\n",
    "```\n",
    "\n",
    "为了比较，您可以通过运行原始第7章微调代码 `python exercise_experiments.py --exercise_solution baseline` 来进行对比。\n",
    "\n",
    "请注意，在Nvidia L4 GPU上，使用LoRA的代码运行时间为1.30分钟，而基线代码运行时间为1.80分钟。因此，LoRA大约快了28%。\n",
    "\n",
    "我们可以使用Ollama Llama 3方法来评估性能，出于方便考虑，该方法也已在 `python exercise_experiments.py` 脚本中实现，您可以按如下方式运行：\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-lora.json\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|████████████████████████| 110/110 [01:13<00:00,  1.50it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 50.23\n",
    "```\n",
    "\n",
    "得分约为50，与原始模型的得分相近。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/01_main-chapter-code/load-finetuned-model.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1545a16b-bc8d-4e49-b9a6-db6631e7483d",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f83194-82b9-4478-9550-5ad793467bd0",
   "metadata": {},
   "source": [
    "# 加载并使用预训练好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e",
   "metadata": {},
   "source": [
    "本笔记本包含 **很少的代码**，用于加载 **在第 7 章** 通过 **指令微调（Instruction Finetuning）** 训练并保存的模型，详见 [ch07.ipynb](ch07.ipynb)。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",    # 分词器\n",
    "    \"torch\",       # 深度学习库\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "finetuned_model_path = Path(\"gpt2-medium355M-sft.pth\")\n",
    "if not finetuned_model_path.exists():\n",
    "    print(\n",
    "        f\"Could not find '{finetuned_model_path}'.\\n\"\n",
    "        \"Run the `ch07.ipynb` notebook to finetune and save the finetuned model.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb02584a-5e31-45d5-8377-794876907bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"gpt2-medium355M-sft.pth\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response \n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "def extract_response(response_text, input_text):\n",
    "    return response_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "response = token_ids_to_text(token_ids, tokenizer)\n",
    "response = extract_response(response, prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/03_model-evaluation/llm-instruction-eval-openai.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用 OpenAI API 评估指令响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- 本笔记本使用 OpenAI 的 GPT-4 API 来评估基于指令微调的 LLMs（大型语言模型）生成的响应，这些响应基于包含生成的模型响应的 JSON 格式数据集，例如：\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"What is the atomic number of helium?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"The atomic number of helium is 2.\",               # <-- 测试集中给出的目标\n",
    "    \"model 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- 由第一个 LLM 生成的响应\n",
    "    \"model 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- 由第二个 LLM 生成的响应\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 1.30.3\n",
      "tqdm version: 4.66.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"openai\",  # OpenAI API\n",
    "        \"tqdm\",    # 进度条\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## 测试 OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- 首先，让我们测试 OpenAI API 是否正确设置\n",
    "- 如果你还没有账户，可以前往 [https://platform.openai.com/](https://platform.openai.com/) 创建一个\n",
    "- 请注意，你还需要为账户充值，因为 GPT-4 API 并非免费（详情见 [https://platform.openai.com/settings/organization/billing/overview](https://platform.openai.com/settings/organization/billing/overview)）\n",
    "- 使用本笔记本中的代码运行实验并创建约 200 个评估的费用大约为 $0.26（即 26 美分），截至目前为止\n",
    "\n",
    "- 请确保不要将这个密钥与任何人共享\n",
    "- 将该密钥（`\"sk-...\"`）添加到此文件夹中的 `config.json` 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# 从 JSON 文件中加载 API 密钥\n",
    "# 请确保将 \"sk-...\" 替换为您在 https://platform.openai.com/api-keys 上的实际 API 密钥\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- 首先，让我们通过一个简单的例子来测试 API，确保它按预期工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e9ef2e-e816-4283-840e-43625791ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 加载 JSON 条目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": [
    "- 在这里，我们假设我们已经将测试数据集和模型的响应保存为一个 JSON 文件，可以通过以下方式加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 100\n"
     ]
    }
   ],
   "source": [
    "json_file = \"eval-example-data.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- 该文件的结构如下，其中包含测试数据集中的给定响应（`'output'`）以及两个不同模型的响应（`'model 1 response'` 和 `'model 2 response'`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Calculate the hypotenuse of a right triangle with legs of 6 cm and 8 cm.',\n",
       " 'input': '',\n",
       " 'output': 'The hypotenuse of the triangle is 10 cm.',\n",
       " 'model 1 response': '\\nThe hypotenuse of the triangle is 3 cm.',\n",
       " 'model 2 response': '\\nThe hypotenuse of the triangle is 12 cm.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- 以下是一个小的实用函数，用于后续可视化时格式化输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 现在，让我们尝试使用OpenAI API来比较模型的响应（我们只评估前5个响应进行可视化比较）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The hypotenuse of the triangle is 10 cm.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The hypotenuse of the triangle is 3 cm.\n",
      "\n",
      "Score:\n",
      ">> The model response \"The hypotenuse of the triangle is 3 cm.\" is incorrect. The correct calculation of the hypotenuse for a right triangle with legs of 6 cm and 8 cm can be found using the Pythagorean theorem, which states that the square of the hypotenuse (c) is equal to the sum of the squares of the other two sides (a and b). Mathematically, this is expressed as:\n",
      "\n",
      "\\[ c = \\sqrt{a^2 + b^2} \\]\n",
      "\\[ c = \\sqrt{6^2 + 8^2} \\]\n",
      "\\[ c = \\sqrt{36 + 64} \\]\n",
      "\\[ c = \\sqrt{100} \\]\n",
      "\\[ c = 10 \\text{ cm} \\]\n",
      "\n",
      "The correct answer should be 10 cm. The response given as 3 cm is not only incorrect but also significantly off from the correct value. This error could lead to misunderstandings or incorrect applications in practical scenarios where precise measurements are crucial.\n",
      "\n",
      "Given the scale from 0 to 100, where 100 is the best score, the response would score very low due to its inaccuracy. However, since the response format is correct (stating the measurement and unit), it does not score the absolute minimum.\n",
      "\n",
      "**Score: 10/100**\n",
      "\n",
      "This score reflects that while the format of the response is correct, the content is highly inaccurate.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> 1. Squirrel\n",
      "2. Eagle\n",
      "3. Tiger\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "1. Squirrel\n",
      "2. Tiger\n",
      "3. Eagle\n",
      "4. Cobra\n",
      "5. Tiger\n",
      "6. Cobra\n",
      "\n",
      "Score:\n",
      ">> The model response lists six animals, three of which (squirrel, tiger, eagle) are indeed active during the day, making them correct responses to the instruction. However, the instruction specifically asked for three different animals, and the model response includes repetitions (tiger and cobra are each listed twice) and also exceeds the requested number of animals.\n",
      "\n",
      "The inclusion of \"cobra\" is incorrect as most cobras are not diurnal (active during the day); they are generally more active during the early morning and late evening, which can be considered crepuscular rather than diurnal.\n",
      "\n",
      "### Scoring Breakdown:\n",
      "- **Relevance to the task**: The response correctly identifies three diurnal animals but also includes additional animals, which was not requested.\n",
      "- **Accuracy**: Including animals not active during the day (cobra) and repeating animals reduces the accuracy.\n",
      "- **Adherence to instructions**: The task was to name three different animals, but the response included six names with repetitions.\n",
      "\n",
      "Given these points, the response partially meets the requirements but also deviates significantly in terms of the number of animals and the inclusion of incorrect and repeated entries.\n",
      "\n",
      "### Score: 50/100\n",
      "This score reflects that while the response did include three correct animals, it failed to strictly follow the instructions by listing only three different animals and included incorrect information.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> I must ascertain what is incorrect.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "What is incorrect?\n",
      "\n",
      "Score:\n",
      ">> The model response \"What is incorrect?\" scores low in terms of fulfilling the instruction to rewrite the sentence in a more formal way. The original sentence \"I need to find out what's wrong.\" expresses a personal obligation and a process of discovery, which is not captured in the model response. The model response turns the sentence into a direct question and loses the nuance of needing to discover or investigate the issue.\n",
      "\n",
      "**Score: 20/100**\n",
      "\n",
      "**Reasoning:**\n",
      "- **Formality:** The response is slightly more formal than casual speech but does not elevate the formality significantly or appropriately. It does use \"incorrect\" which is slightly more formal than \"wrong.\"\n",
      "- **Completeness:** The response fails to include the aspect of needing to find out or ascertain, which is a critical part of the original sentence.\n",
      "- **Accuracy:** The response changes the structure and intent by converting it into a direct question, which does not align with the instruction to rewrite the statement while maintaining its original intent.\n",
      "\n",
      "Overall, the response does not adequately meet the requirements of the task as it significantly alters the meaning and omits key elements of the original sentence.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Score:\n",
      ">> The model response `The interjection in the sentence is 'Wow'.` accurately identifies the interjection in the provided sentence. The response is clear, directly addresses the instruction, and correctly identifies \"Wow\" as the interjection, which is used to express surprise or admiration, fitting the context of the sentence. Therefore, the response is fully correct and meets all the requirements of the task.\n",
      "\n",
      "Score: 100/100\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of sentence is interrogative.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The type of sentence is exclamatory.\n",
      "\n",
      "Score:\n",
      ">> The model response \"The type of sentence is exclamatory.\" is incorrect. The input sentence \"Did you finish the report?\" is clearly an interrogative sentence as it is asking a question, indicated by the question mark at the end and the structure of the sentence.\n",
      "\n",
      "Given the scoring criteria where 100 is the best score and should be awarded to a correct and precise response, the model's response should receive a low score because it incorrectly identifies the type of sentence. An exclamatory sentence typically expresses strong emotion and ends with an exclamation mark, which is not the case here.\n",
      "\n",
      "Therefore, the score for the model response would be 0 out of 100, as it completely misidentifies the type of sentence, providing incorrect information.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[:5]:\n",
    "    prompt = (f\"Given the input `{format_input(entry)}` \"\n",
    "              f\"and correct output `{entry['output']}`, \"\n",
    "              f\"score the model response `{entry['model 1 response']}`\"\n",
    "              f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "              )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model 1 response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- 请注意，响应非常冗长；为了量化哪个模型更好，我们只需要返回分数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_model_scores(json_data, json_key, client):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the number only.\"\n",
    "        )\n",
    "        score = run_chatgpt(prompt, client)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71974dea-31ed-49af-abba-5c858bbbf49c",
   "metadata": {},
   "source": [
    "- 请注意，由于 OpenAI 的 GPT 模型即使设置了随机数种子等，仍然是非确定性的，因此响应分数可能会有所不同。\n",
    "\n",
    "- 我们将把这种评估应用于整个数据集，并计算每个模型的平均分数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:03<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 1 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 74.09\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 2 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 56.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for model in (\"model 1 response\", \"model 2 response\"):\n",
    "\n",
    "    scores = generate_model_scores(json_data, model, client)\n",
    "    print(f\"\\n{model}\")\n",
    "    print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "    print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # 可选：将分数保存到 JSON 文件中\n",
    "    save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "    with open(save_path, \"w\") as file:\n",
    "        json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169d534-1fec-43c4-9550-5cb701ff7f05",
   "metadata": {},
   "source": [
    "- 根据上述评估，我们可以得出结论：第一个模型明显优于第二个模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0_xya1nyDHfY",
   "metadata": {
    "id": "0_xya1nyDHfY"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l62zIRRSBy_R",
   "metadata": {
    "id": "l62zIRRSBy_R"
   },
   "source": [
    "# 把From-Scratch GPT 转换为Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aFmxTQbwCUMl",
   "metadata": {
    "id": "aFmxTQbwCUMl"
   },
   "source": [
    "## 从零构建 Llama 3.2（独立笔记本）\n",
    "\n",
    "### 本笔记本内容\n",
    "在本笔记本中，我们将 **逐步将原始 GPT 架构转换为 Llama 2 模型**（值得注意的是，GPT 和 GPT-2 共享相同的架构）。\n",
    "\n",
    "### 为什么不是 Llama 1 或 Llama 3？\n",
    "- **Llama 1 vs. Llama 2**：\n",
    "  - Llama 1 的架构与 Llama 2 类似，但 **Llama 2 具有更大的上下文窗口**，这一点非常重要。\n",
    "  - Llama 1 的权重 **未公开**，且使用受限，因此更合理的选择是专注于 Llama 2。\n",
    "\n",
    "- **Llama 3**：\n",
    "  - 我将在另一个笔记本中 **演示如何从 Llama 2 迁移到 Llama 3**，其变动较小。\n",
    "\n",
    "### 说明\n",
    "- 本笔记本的解释内容**刻意简化**，以避免冗余，**重点关注代码实现**。\n",
    "- 更多详细信息请参考 **Llama 2 论文**：[Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ohhMKUWvGm9z",
   "metadata": {
    "id": "ohhMKUWvGm9z"
   },
   "source": [
    "<img src=\"../image/gpt2-to-llama2-llama3.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBpQwU89ETA1",
   "metadata": {
    "id": "JBpQwU89ETA1"
   },
   "source": [
    "- 下面是所需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
    "outputId": "8118963b-3c72-43af-874b-439ffebdc94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.24.7\n",
      "sentencepiece version: 0.2.0\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"sentencepiece\",    # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJJneXpTEg4W",
   "metadata": {
    "id": "UJJneXpTEg4W"
   },
   "source": [
    "&nbsp;\n",
    "# 1. 逐步转换 GPT 模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1zpfX2GHBKa",
   "metadata": {
    "id": "v1zpfX2GHBKa"
   },
   "source": [
    "在本节中，我们将基于 [第 4 章](../../ch04/01_main-chapter-code/ch04.ipynb) 的 GPT 模型代码，并 **逐步修改** 以实现 Llama 2 的架构。\n",
    "\n",
    "随后，我们将加载 Meta AI **公开分享的 Llama 2 预训练权重**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f",
   "metadata": {
    "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.1 替换 LayerNorm 为 RMSNorm 层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6",
   "metadata": {
    "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6"
   },
   "source": [
    "首先，我们将 **LayerNorm** 替换为 **均方根层归一化（RMSNorm）**。\n",
    "\n",
    "- **LayerNorm** 使用 **均值和方差** 归一化输入，而 **RMSNorm** 仅使用 **均方根（RMS）**，从而提高计算效率。\n",
    "- RMSNorm 计算公式如下，其中 $x$ 为输入，$\\gamma$ 为可训练参数（向量），$\\epsilon$ 为小常数，用于避免除零错误：\n",
    "\n",
    "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{其中} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$\n",
    "\n",
    "更多细节请参考论文：[Root Mean Square Layer Normalization (2019)](https://arxiv.org/abs/1910.07467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7094381-9499-4e9e-93f9-b79470da3771",
   "metadata": {
    "id": "d7094381-9499-4e9e-93f9-b79470da3771"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtWC8DOmIu0F",
   "metadata": {
    "id": "mtWC8DOmIu0F"
   },
   "source": [
    "以下代码单元用于检查该实现是否与 PyTorch 内置的 RMSNorm："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f",
   "metadata": {
    "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "example_batch = torch.randn(2, 3, 4)\n",
    "\n",
    "rms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\n",
    "rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5)\n",
    "\n",
    "assert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb81f83-c38c-46a4-b763-aa630a32e357",
   "metadata": {
    "id": "5eb81f83-c38c-46a4-b763-aa630a32e357"
   },
   "source": [
    "&nbsp;\n",
    "## 1.2 用SiLU代替GELU作激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa702-f118-4ff6-9135-90725ec8756c",
   "metadata": {
    "id": "0b8aa702-f118-4ff6-9135-90725ec8756c"
   },
   "source": [
    "Llama 采用 **SiLU（Sigmoid Linear Unit）** 作为激活函数，而非 GELU。SiLU 也被称为 **Swish 函数**，其计算公式如下：\n",
    "\n",
    "$$\n",
    "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{其中} \\quad \\sigma(x) \\text{ 为 logistic sigmoid 函数。}\n",
    "$$\n",
    "\n",
    "更多信息请参考论文：[Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning (2017)](https://arxiv.org/abs/1702.03118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe",
   "metadata": {
    "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c",
   "metadata": {
    "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c"
   },
   "outputs": [],
   "source": [
    "silu = SiLU()\n",
    "\n",
    "assert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9",
   "metadata": {
    "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9"
   },
   "source": [
    "&nbsp;\n",
    "## 1.3 更新 FeedForward 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91",
   "metadata": {
    "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91"
   },
   "source": [
    "实际上，Llama 使用的是 SiLU 的 **门控线性单元（Gated Linear Unit, GLU）** 变体，称为 **SwiGLU**。这使得 `FeedForward` 模块的结构略有不同。\n",
    "\n",
    "SwiGLU 在前馈层中引入了 **门控机制**，其计算公式如下：\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
    "\n",
    "其中：\n",
    "- $\\text{Linear}_1$ 和 $\\text{Linear}_2$ 为两个线性变换层\n",
    "- $*$ 表示 **按元素（element-wise）乘法**\n",
    "- **第三个线性层** $\\text{Linear}_3$ 在该门控激活之后应用\n",
    "\n",
    "更多信息请参考论文：[GLU Variants Improve Transformer (2020)](https://arxiv.org/abs/2002.05202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a",
   "metadata": {
    "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "#             GELU(),\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477568cb-03cd-4510-b663-a42ce3ec64a2",
   "metadata": {
    "id": "477568cb-03cd-4510-b663-a42ce3ec64a2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcD8LSHNhBRW",
   "metadata": {
    "id": "qcD8LSHNhBRW"
   },
   "source": [
    "请注意，我们在上述代码中添加了 `dtype=cfg[\"dtype\"]` 设置，使我们能够在 **低精度格式** 下直接加载模型，以减少内存占用（相比于先以 32 位精度实例化后再转换）。\n",
    "\n",
    "此外，我们设置了 `bias=False`，因为 Llama **不使用偏置单元（bias units）**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949",
   "metadata": {
    "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949"
   },
   "source": [
    "&nbsp;\n",
    "## 1.4 实现 RoPE（旋转位置编码）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884",
   "metadata": {
    "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884"
   },
   "source": [
    "在 GPT 模型中，位置嵌入（positional embeddings）实现如下：\n",
    "\n",
    "```python\n",
    "self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "```\n",
    "\n",
    "与传统的 **绝对位置编码** 不同，Llama 采用 **旋转位置编码（RoPE）**，能够同时捕捉 **绝对位置** 和 **相对位置** 信息。\n",
    "\n",
    "RoPE 相关论文：[RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34180fb-448f-44e9-a244-0c736051687b",
   "metadata": {
    "id": "a34180fb-448f-44e9-a244-0c736051687b"
   },
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299",
   "metadata": {
    "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299"
   },
   "source": [
    "- 以下是将 **旋转位置编码（RoPE, Rotary Positional Embedding）** 应用于 `q` 和 `k` 张量的示例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c89f022-7167-4001-8c21-8e012878733f",
   "metadata": {
    "id": "8c89f022-7167-4001-8c21-8e012878733f"
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 2\n",
    "context_len = 5\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297",
   "metadata": {
    "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297"
   },
   "source": [
    "&nbsp;\n",
    "## 1.5 在 `MultiHeadAttention` 模块中添加 RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RnmSHROLhhR3",
   "metadata": {
    "id": "RnmSHROLhhR3"
   },
   "source": [
    "- 需要注意的是，GPT 将 **位置编码（Positional Embedding）** 应用于输入，而 LLaMA 则在 **自注意力机制（Self-Attention）** 中对查询（`q`）和键（`k`）向量执行旋转变换。  \n",
    "- 在此，我们修改 `MultiHeadAttention` 类，以适配 **旋转位置编码（RoPE, Rotary Positional Embedding）**。  \n",
    "- 另外，我们移除 `qkv_bias` 选项，并将 `bias=False` 作为固定设置。  \n",
    "- 此外，我们添加了 `dtype` 设置，以便后续能够使用更低精度的数值格式实例化模型。  \n",
    "- **提示**：由于 `TransformerBlock`（在下一部分）是完全重复的，我们可以优化代码，使每个 `MultiHeadAttention` 模块仅初始化一次缓冲区。然而，我们仍将 **预计算的 RoPE 参数** 添加到 `MultiHeadAttention` 类中，使其可以作为独立模块运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84",
   "metadata": {
    "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # Set bias=False and dtype=dtype for all linear layers below\n",
    "        ###########################################################################\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear layer to combine head outputs\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "        ###########################################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-lt9SfnVioB3",
   "metadata": {
    "id": "-lt9SfnVioB3"
   },
   "source": [
    "- 下面是一个使用 `MultiHeadAttention` 模块处理示例输入的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f15755-0083-483f-963b-99b599651638",
   "metadata": {
    "id": "03f15755-0083-483f-963b-99b599651638"
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 100\n",
    "max_context_len = 4096\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "mha(example_batch)\n",
    "\n",
    "del mha  # delete to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f",
   "metadata": {
    "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.6 更新TransformerBlock 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18",
   "metadata": {
    "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18"
   },
   "source": [
    "- 在此阶段，大部分的核心工作已经完成；现在我们可以更新 `TransformerBlock` 来使用我们在上面实现的代码。  \n",
    "- 具体而言，我们需要：\n",
    "  - 将 **LayerNorm** 替换为 **RMSNorm**  \n",
    "  - 移除 **dropout**  \n",
    "  - 移除 **`qkv_bias` 设置**  \n",
    "  - 添加 **`dtype` 设置**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e110721-bf2b-42b3-989a-1635b1658af0",
   "metadata": {
    "id": "2e110721-bf2b-42b3-989a-1635b1658af0"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]  # NEW\n",
    "            # dropout=cfg[\"drop_rate\"],\n",
    "            # qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "\n",
    "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f",
   "metadata": {
    "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.7 更新模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d991a-559b-47be-96f4-31b881ab2da8",
   "metadata": {
    "id": "ba5d991a-559b-47be-96f4-31b881ab2da8"
   },
   "source": [
    "- 正如你可能还记得的，在 [第 5 章](../01_main-chapter-code/ch05.ipynb) 中，`TransformerBlock` 是主模型中重复使用的基础模块。  \n",
    "- 我们的 LLaMA 模型已接近完成；现在只需更新 `TransformerBlock` 相关的模型代码。  \n",
    "- 具体而言，我们需要：\n",
    "  - 移除 **绝对位置编码**，因为我们已经使用了 **RoPE 编码**  \n",
    "  - 将 **LayerNorm** 替换为 **RMSNorm**  \n",
    "  - 移除 **dropout**  \n",
    "  - 添加 **`dtype` 设置**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79",
   "metadata": {
    "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79"
   },
   "outputs": [],
   "source": [
    "# class GPTModel(nn.Module):\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60",
   "metadata": {
    "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60"
   },
   "source": [
    "&nbsp;\n",
    "## 2. 模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bG--zY-Ljj1f",
   "metadata": {
    "id": "bG--zY-Ljj1f"
   },
   "source": [
    "- 现在模型代码已经完成，我们可以开始初始化它。  \n",
    "- 在 [第 5 章](../01_main-chapter-code/ch05.ipynb) 中，我们使用以下配置文件来指定 **124M 参数规模的 GPT 模型**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f",
   "metadata": {
    "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bVi8uiBjw2T",
   "metadata": {
    "id": "8bVi8uiBjw2T"
   },
   "source": [
    "- 作为参考，下面是 **1.5B 参数规模的 GPT 模型** 配置文件： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tAOojV_mkEnd",
   "metadata": {
    "id": "tAOojV_mkEnd"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_1558M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,           # Number of attention heads\n",
    "    \"n_layers\": 48,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HoGGRAGykQTE",
   "metadata": {
    "id": "HoGGRAGykQTE"
   },
   "source": [
    "- 同样，我们可以定义 **7B 参数规模的 LLaMA 2** 配置文件（为简化起见，这里忽略更大规模的模型）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18",
   "metadata": {
    "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18"
   },
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAP7fiBzkaBz",
   "metadata": {
    "id": "FAP7fiBzkaBz"
   },
   "source": [
    "- 使用这些设置，我们现在可以初始化 **LLaMA 2 7B 模型**（请注意，这需要约 **26GB** 的内存）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7004d785-ac9a-4df5-8760-6807fc604686",
   "metadata": {
    "id": "7004d785-ac9a-4df5-8760-6807fc604686"
   },
   "outputs": [],
   "source": [
    "model = Llama2Model(LLAMA2_CONFIG_7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
    "outputId": "0a0eb34b-1a21-4c11-804f-b40007bda5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bx14NtzWk2wj",
   "metadata": {
    "id": "Bx14NtzWk2wj"
   },
   "source": [
    "- 如上所示，该模型包含 **67 亿个参数**（通常四舍五入后称为 **7B 模型**）。  \n",
    "- 此外，我们可以使用下面的代码计算该模型的 **内存需求**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
    "outputId": "11ced939-556d-4511-d5c0-10a94ed3df32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 52.33 GB\n",
      "bfloat16: 26.17 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zudd-5PulKFL",
   "metadata": {
    "id": "zudd-5PulKFL"
   },
   "source": [
    "- 最后，如果适用，我们还可以将模型部署到 **NVIDIA GPU** 或 **Apple Silicon GPU**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d",
   "metadata": {
    "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34",
   "metadata": {
    "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34"
   },
   "source": [
    "&nbsp;\n",
    "## 3. 加载tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005",
   "metadata": {
    "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005"
   },
   "source": [
    "- 在本节中，我们将加载 **模型的分词器（Tokenizer）**。  \n",
    "- **LLaMA 2** 采用 **Google 的 [SentencePiece](https://github.com/google/sentencepiece) 分词器**，而不是 **OpenAI 的 [Tiktoken](https://github.com/openai/tiktoken)**（但 **LLaMA 3** 采用了 **Tiktoken**）。  \n",
    "- **Meta AI** 在 **Hugging Face Hub** 上分享了 **LLaMA 2 原始模型权重**及 **分词器词汇表**。  \n",
    "- 我们将从 **Hub** 下载 **分词器词汇表**，并将其加载到 **SentencePiece** 中。  \n",
    "- 取消注释并运行以下代码，以安装所需的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422",
   "metadata": {
    "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422"
   },
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KbnlzsbYmJU6",
   "metadata": {
    "id": "KbnlzsbYmJU6"
   },
   "source": [
    "- 请注意，**Meta AI** 要求您在下载文件之前 **接受 LLaMA 2 许可协议**。  \n",
    "  - 为此，您需要创建一个 **Hugging Face Hub 账号**，然后访问 [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) 仓库并接受条款。  \n",
    "- 接下来，您需要创建一个 **访问令牌（Access Token）**。  \n",
    "  - 要生成具有 **READ 权限** 的访问令牌，请点击 **右上角的个人头像**，然后选择 **\"Settings\"（设置）**。  \n",
    "\n",
    "<img src=\"../image/settings.webp\" width=\"300px\">\n",
    "\n",
    "- 然后，创建并复制 **访问令牌**，以便在接下来的代码单元中使用。  \n",
    "\n",
    "<img src=\"../image/access-token.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3357a230-b678-4691-a238-257ee4e80185",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3357a230-b678-4691-a238-257ee4e80185",
    "outputId": "768ed6af-ce14-40bc-ca18-117b4b448269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IxGh6ZYQo0VN",
   "metadata": {
    "id": "IxGh6ZYQo0VN"
   },
   "source": [
    "- 通过 **访问令牌（Access Token）** 登录后，我们的账户将通过验证，确认已接受 **LLaMA 2 许可协议**，然后即可下载 **分词器词汇表**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "e6c75a6aa7b942fe84160e286e3acb3d",
      "08f0bf9459bd425498a5cb236f9d4a72",
      "10251d6f724e43788c41d4b7879cbfd3",
      "53a973c0853b44418698136bd04df039",
      "bdb071e7145a4007ae01599333e72612",
      "6b1821a7f4574e3aba09c1e410cc81e4",
      "8c2873eaec3445888ad3d54ad7387950",
      "0c8f7044966e4207b12352503c67dcbb",
      "8b5951213c9e4798a258146d61d02d11",
      "2c05df3f91e64df7b33905b1065a76f7",
      "742ae5487f2648fcae7ca8e22c7f8db9"
     ]
    },
    "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
    "outputId": "c230fec9-5c71-4a41-90ab-8a34d114ea01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c75a6aa7b942fe84160e286e3acb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gp7iQ8cXAJLv",
   "metadata": {
    "id": "gp7iQ8cXAJLv"
   },
   "source": [
    "- 为了提供更直观的分词器接口，我们定义一个 **`LlamaTokenizer` 包装类**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Ef4WxhjOBOOc",
   "metadata": {
    "id": "Ef4WxhjOBOOc"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class LlamaTokenizer:\n",
    "    def __init__(self, tokenizer_file):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NVhmFeX3pT_M",
   "metadata": {
    "id": "NVhmFeX3pT_M"
   },
   "source": [
    "- 现在，我们可以使用 `generate` 函数，让 **LLaMA 2** 模型生成新的文本。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
    "outputId": "acd5065d-8900-4ba8-ef85-968365f3a0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort movesαllRadius deletingpretcc否']; future eer napulate lackус während inter DES издаSchéon로жа Bass differencespadxsnu ;; ctx始\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93WTtAA5paYV",
   "metadata": {
    "id": "93WTtAA5paYV"
   },
   "source": [
    "- 当然，如上所示，生成的文本目前是 **无意义的**，因为我们尚未训练 **LLaMA 2** 模型。  \n",
    "- 在下一节，我们不会自行训练模型（这将花费 **数万到数十万美元**），而是直接 **加载 Meta AI 提供的预训练权重**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cc248-1d27-4eb6-aa50-173b436652f8",
   "metadata": {
    "id": "f63cc248-1d27-4eb6-aa50-173b436652f8"
   },
   "source": [
    "&nbsp;\n",
    "## 4.加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKeN7rUfqZMI",
   "metadata": {
    "id": "aKeN7rUfqZMI"
   },
   "source": [
    "- 下面我们加载 **[\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b)** 预训练基础模型，该模型在微调之前仅用于 **文本补全**。  \n",
    "- 或者，您可以通过修改下一代码单元中的字符串，加载 **指令微调并对齐的 [\"meta-llama/Llama-2-7b-chat\"](https://huggingface.co/meta-llama/Llama-2-7b-chat) 模型**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "66e777955e8748df878f118f07f38dab",
      "da89ae3ea4d2474e98f64ada608f3cea",
      "93e6da39c25f4edfaa72056c89df1f7f",
      "b628603e4cb0405398c916587ee96756",
      "93bedcb9245e44a0a1eb7e4155070f66",
      "0723f467d37b4904819a8bb33ebda10f",
      "e54928776bc649339002adced63738b0",
      "d8e0f42068af4cb094e2f115f76e06e0",
      "0a939565b6e94f08bee0a66e0f9827d4",
      "a5fedbb7ec2e43d99711bb4cd84b9486",
      "0c186f6539714d8eab023969ce47c500"
     ]
    },
    "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
    "outputId": "0d8942cc-e5e2-4e77-ec41-1ac7bec7d94f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e777955e8748df878f118f07f38dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_file = hf_hub_download(\n",
    "   repo_id=\"meta-llama/Llama-2-7b\",\n",
    "   filename=\"consolidated.00.pth\",\n",
    "   local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701",
   "metadata": {
    "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701"
   },
   "outputs": [],
   "source": [
    "weights = torch.load(weights_file, weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-15SJ7btq2zE",
   "metadata": {
    "id": "-15SJ7btq2zE"
   },
   "source": [
    "- `weights` 变量包含以下张量（仅为简洁起见，展示前 **15 个**）：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
    "outputId": "fa83d38a-bb41-4cb2-d3c7-c573bfe1f8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok_embeddings.weight',\n",
       " 'norm.weight',\n",
       " 'output.weight',\n",
       " 'layers.0.attention.wq.weight',\n",
       " 'layers.0.attention.wk.weight',\n",
       " 'layers.0.attention.wv.weight',\n",
       " 'layers.0.attention.wo.weight',\n",
       " 'layers.0.feed_forward.w1.weight',\n",
       " 'layers.0.feed_forward.w2.weight',\n",
       " 'layers.0.feed_forward.w3.weight',\n",
       " 'layers.0.attention_norm.weight',\n",
       " 'layers.0.ffn_norm.weight',\n",
       " 'layers.1.attention.wq.weight',\n",
       " 'layers.1.attention.wk.weight',\n",
       " 'layers.1.attention.wv.weight']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(weights.keys())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UeeSpnunrDFB",
   "metadata": {
    "id": "UeeSpnunrDFB"
   },
   "source": [
    "- 下面的函数基于 **[第 5 章](../01_main-chapter-code/ch05.ipynb) 中的 `load_weights_into_gpt`**，用于将 **预训练权重加载到 LLaMA 2 模型** 中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65",
   "metadata": {
    "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65"
   },
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"layers.{l}.attention.wq.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"layers.{l}.attention.wk.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"layers.{l}.attention.wv.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"layers.{l}.attention.wo.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"layers.{l}.attention_norm.weight\"]\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
    "        )\n",
    "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"layers.{l}.ffn_norm.weight\"]\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TDuv_Us2rNvk",
   "metadata": {
    "id": "TDuv_Us2rNvk"
   },
   "source": [
    "- 现在，我们已经准备好使用该模型进行 **文本生成**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "240987e8-a023-462e-9376-9edfb27559ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "240987e8-a023-462e-9376-9edfb27559ec",
    "outputId": "044f24b3-4018-4860-834d-6c2731b9e47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort has been made to ensure that the information contained in this website is accurate and up to date and correct at the time of publication\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed949-b6c0-4966-922f-eb0da732c404",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 5. 使用指令微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "akyo7WNyF_YL",
   "metadata": {
    "id": "akyo7WNyF_YL"
   },
   "source": [
    "- 如前所述，我们之前使用的是 **预训练基础模型**。  \n",
    "- 如果您希望使用 **能够遵循指令的模型**，请改用 `\"meta-llama/Llama-2-7b-chat\"`，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nbvAV7vaz6yc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "3b2448a60f5f4ba5b2c686037c8ecd78",
      "60c5932944f24f5fad1d8da89c8e5ae9",
      "aa31aed1b8854a4281fd7e81c60e1205",
      "d4acf06c2414412f8f2fb4f48981c954",
      "693d69251d3d48219c084af17b54b851",
      "ff36d28c55dd4db3a0f76a87640fdfe2",
      "71c49ef820494d5f8908a3daf39f0755",
      "525dc406534f4369b11208816f8fd0d7",
      "865f39213a7341b68f2fe73caaf801b1",
      "eaf4c0231b6d4993b2f8e9e63d8b6921",
      "a11edf3b018e42c88a63a515cf7fe478"
     ]
    },
    "id": "nbvAV7vaz6yc",
    "outputId": "724f5508-d976-4e31-b3d7-95fa65b2c1e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2448a60f5f4ba5b2c686037c8ecd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " What do llamas eat?\n",
      "Llamas and alpacas are herbivores, which means they eat grasses, leaves, grass\n"
     ]
    }
   ],
   "source": [
    "del model  # to free up memory\n",
    "\n",
    "weights_file = hf_hub_download(\n",
    "   repo_id=\"meta-llama/Llama-2-7b-chat\",\n",
    "   filename=\"consolidated.00.pth\",\n",
    "   local_dir=\"Llama-2-7b-chat\"\n",
    ")\n",
    "\n",
    "model = Llama2Model(LLAMA2_CONFIG_7B)\n",
    "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
    "model.to(device);\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"What do llamas eat?\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f693da1-a07c-4e1d-af5a-c3923525f1e2",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 展望"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae93739-ca12-46ba-8ca7-7c07c59f669b",
   "metadata": {},
   "source": [
    "- 本笔记本将原始 **GPT-2** 架构转换为 **LLaMA 2** 模型。  \n",
    "- 如果您对如何将 **LLaMA 2** 转换为 **LLaMA 3**、**LLaMA 3.1** 和 **LLaMA 3.2** 感兴趣，请查看 [converting-llama2-to-llama3.ipynb](converting-llama2-to-llama3.ipynb) 笔记本。  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0723f467d37b4904819a8bb33ebda10f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08f0bf9459bd425498a5cb236f9d4a72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b1821a7f4574e3aba09c1e410cc81e4",
      "placeholder": "​",
      "style": "IPY_MODEL_8c2873eaec3445888ad3d54ad7387950",
      "value": "tokenizer.model: 100%"
     }
    },
    "0a939565b6e94f08bee0a66e0f9827d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c186f6539714d8eab023969ce47c500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c8f7044966e4207b12352503c67dcbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10251d6f724e43788c41d4b7879cbfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8f7044966e4207b12352503c67dcbb",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b5951213c9e4798a258146d61d02d11",
      "value": 499723
     }
    },
    "2c05df3f91e64df7b33905b1065a76f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b2448a60f5f4ba5b2c686037c8ecd78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60c5932944f24f5fad1d8da89c8e5ae9",
       "IPY_MODEL_aa31aed1b8854a4281fd7e81c60e1205",
       "IPY_MODEL_d4acf06c2414412f8f2fb4f48981c954"
      ],
      "layout": "IPY_MODEL_693d69251d3d48219c084af17b54b851"
     }
    },
    "525dc406534f4369b11208816f8fd0d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a973c0853b44418698136bd04df039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c05df3f91e64df7b33905b1065a76f7",
      "placeholder": "​",
      "style": "IPY_MODEL_742ae5487f2648fcae7ca8e22c7f8db9",
      "value": " 500k/500k [00:00&lt;00:00, 3.39MB/s]"
     }
    },
    "60c5932944f24f5fad1d8da89c8e5ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff36d28c55dd4db3a0f76a87640fdfe2",
      "placeholder": "​",
      "style": "IPY_MODEL_71c49ef820494d5f8908a3daf39f0755",
      "value": "consolidated.00.pth: 100%"
     }
    },
    "66e777955e8748df878f118f07f38dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da89ae3ea4d2474e98f64ada608f3cea",
       "IPY_MODEL_93e6da39c25f4edfaa72056c89df1f7f",
       "IPY_MODEL_b628603e4cb0405398c916587ee96756"
      ],
      "layout": "IPY_MODEL_93bedcb9245e44a0a1eb7e4155070f66"
     }
    },
    "693d69251d3d48219c084af17b54b851": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b1821a7f4574e3aba09c1e410cc81e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71c49ef820494d5f8908a3daf39f0755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "742ae5487f2648fcae7ca8e22c7f8db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "865f39213a7341b68f2fe73caaf801b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b5951213c9e4798a258146d61d02d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c2873eaec3445888ad3d54ad7387950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93bedcb9245e44a0a1eb7e4155070f66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93e6da39c25f4edfaa72056c89df1f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8e0f42068af4cb094e2f115f76e06e0",
      "max": 13476925163,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a939565b6e94f08bee0a66e0f9827d4",
      "value": 13476925163
     }
    },
    "a11edf3b018e42c88a63a515cf7fe478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5fedbb7ec2e43d99711bb4cd84b9486": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa31aed1b8854a4281fd7e81c60e1205": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_525dc406534f4369b11208816f8fd0d7",
      "max": 13476925163,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_865f39213a7341b68f2fe73caaf801b1",
      "value": 13476925163
     }
    },
    "b628603e4cb0405398c916587ee96756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5fedbb7ec2e43d99711bb4cd84b9486",
      "placeholder": "​",
      "style": "IPY_MODEL_0c186f6539714d8eab023969ce47c500",
      "value": " 13.5G/13.5G [01:40&lt;00:00, 111MB/s]"
     }
    },
    "bdb071e7145a4007ae01599333e72612": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4acf06c2414412f8f2fb4f48981c954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaf4c0231b6d4993b2f8e9e63d8b6921",
      "placeholder": "​",
      "style": "IPY_MODEL_a11edf3b018e42c88a63a515cf7fe478",
      "value": " 13.5G/13.5G [02:52&lt;00:00, 81.1MB/s]"
     }
    },
    "d8e0f42068af4cb094e2f115f76e06e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da89ae3ea4d2474e98f64ada608f3cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0723f467d37b4904819a8bb33ebda10f",
      "placeholder": "​",
      "style": "IPY_MODEL_e54928776bc649339002adced63738b0",
      "value": "consolidated.00.pth: 100%"
     }
    },
    "e54928776bc649339002adced63738b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6c75a6aa7b942fe84160e286e3acb3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08f0bf9459bd425498a5cb236f9d4a72",
       "IPY_MODEL_10251d6f724e43788c41d4b7879cbfd3",
       "IPY_MODEL_53a973c0853b44418698136bd04df039"
      ],
      "layout": "IPY_MODEL_bdb071e7145a4007ae01599333e72612"
     }
    },
    "eaf4c0231b6d4993b2f8e9e63d8b6921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff36d28c55dd4db3a0f76a87640fdfe2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/03_model-evaluation/llm-instruction-eval-ollama.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用 LLaMA 3 模型和 Ollama 本地评估指令响应  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    " - **本笔记本使用 Ollama 运行 80 亿参数的 LLaMA 3 模型**，  \n",
    "  **用于基于 JSON 格式数据集** 评估 **指令微调 LLM 生成的响应**。  \n",
    "\n",
    "- **示例数据集格式如下**，\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"What is the atomic number of helium?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"The atomic number of helium is 2.\",               # <-- 测试集中给出的目标\n",
    "    \"model 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- 由第一个 LLM 生成的响应\n",
    "    \"model 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- 由第二个 LLM 生成的响应\n",
    "},\n",
    "```\n",
    "\n",
    "- **该代码无需 GPU**，可直接在 **笔记本电脑** 上运行（已在 **M3 MacBook Air** 上测试）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # 进度条\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## 安装 Ollama 并下载 LLaMA 3\n",
    "\n",
    "- **Ollama** 是一个用于高效运行 **LLM（大语言模型）** 的应用。  \n",
    "- 它是 **[llama.cpp](https://github.com/ggerganov/llama.cpp)** 的封装，后者采用 **纯 C/C++ 实现 LLM**，以 **最大化推理效率**。  \n",
    "- **请注意**，Ollama **仅用于 LLM 推理（inference）**，**不支持训练或微调（finetuning）**。  \n",
    "- **在运行下方代码前**，请先访问 **[https://ollama.com](https://ollama.com)** 并按照安装指南完成 **Ollama 安装**（例如，点击 **“Download”** 按钮，下载适用于您的操作系统的 Ollama 应用）。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- **对于 macOS 和 Windows 用户**，点击 **下载的 Ollama 应用**，如果系统提示安装 **命令行工具**，请选择 **“是”**。  \n",
    "- **Linux 用户** 可以使用 **Ollama 官网提供的安装命令** 进行安装。  \n",
    "\n",
    "- **通常，在命令行使用 Ollama 之前**，需要 **启动 Ollama 应用** 或 **在终端运行 `ollama serve`**。  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/21.png\">\n",
    "\n",
    "- **确保 Ollama 运行后**，在 **另一个终端窗口** 执行以下命令，尝试 **80 亿参数的 LLaMA 3 模型**（首次运行时，模型将 **自动下载**，约 **4.7GB 存储空间**）：  \n",
    "\n",
    "\n",
    "```bash\n",
    "# 8B 模型\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "\n",
    "输出如下所示\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest \n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
    "verifying sha256 digest \n",
    "writing manifest \n",
    "removing any unused layers \n",
    "success \n",
    "```\n",
    "\n",
    "- **注意**：`llama3` 指的是 **指令微调后的 80 亿参数 LLaMA 3 模型**。  \n",
    "\n",
    "- **如果您的设备支持**，可以将 `llama3` 替换为 **`llama3:70b`**，以使用 **更大的 700 亿参数 LLaMA 3 模型**。  \n",
    "\n",
    "- **下载完成后**，您将进入 **命令行交互界面**，可与模型进行对话。  \n",
    "\n",
    "- **尝试输入以下提示**：\"What do llamas eat?\"（羊驼吃什么？），  \n",
    "  预计模型会返回类似如下的输出：  \n",
    "\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered \n",
    "stomach and eat plants that are high in fiber. In the wild, llamas \n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall \n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
   "metadata": {},
   "source": [
    "- 输入`/bye`来停止程序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
   "metadata": {},
   "source": [
    "## 使用 Ollama 的 REST API\n",
    "\n",
    "- **另一种与模型交互的方式** 是通过 **REST API** 在 **Python** 中进行调用，具体实现如下。  \n",
    "- **在运行本笔记本中的代码前**，请确保 **Ollama 仍在运行**，可通过以下方式启动：\n",
    "  - 在终端中执行 `ollama serve`\n",
    "  - 使用 **Ollama 应用程序**  \n",
    "\n",
    "- **接下来，运行下方代码单元**，以查询模型并获取响应。  \n",
    "\n",
    "- **首先，我们用一个简单的示例测试 API**，以确保其 **正常运行**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def query_model(prompt, model=\"llama3\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # 创建数据负载作为字典\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {     # 以下设置为确定性响应所需\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 将字典转换为 JSON 格式的字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # 创建请求对象，设置方法为 POST 并添加必要的头信息\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并捕获响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 加载 JSON 数据（Load JSON Entries）\n",
    "\n",
    "- 现在，我们进入 **数据评估** 部分。  \n",
    "- 这里假设我们已将 **测试数据集** 和 **模型响应** 保存为 **JSON 文件**，可以按以下方式加载：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 100\n"
     ]
    }
   ],
   "source": [
    "json_file = \"eval-example-data.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- **该文件的结构如下**，其中包含：  \n",
    "  - **测试数据集中的标准响应**（`'output'`）。  \n",
    "  - **两个不同模型生成的响应**（`'model 1 response'` 和 `'model 2 response'`）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Calculate the hypotenuse of a right triangle with legs of 6 cm and 8 cm.',\n",
       " 'input': '',\n",
       " 'output': 'The hypotenuse of the triangle is 10 cm.',\n",
       " 'model 1 response': '\\nThe hypotenuse of the triangle is 3 cm.',\n",
       " 'model 2 response': '\\nThe hypotenuse of the triangle is 12 cm.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- 下面是一个 **小型工具函数**，用于 **格式化输入**，以便后续 **可视化** 展示。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 现在，我们使用 **Ollama API** 对 **模型生成的响应** 进行比较  \n",
    "  （这里仅评估 **前 5 个响应**，以便进行直观对比）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The hypotenuse of the triangle is 10 cm.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The hypotenuse of the triangle is 3 cm.\n",
      "\n",
      "Score:\n",
      ">> I'd score this response as 0 out of 100.\n",
      "\n",
      "The correct answer is \"The hypotenuse of the triangle is 10 cm.\", not \"3 cm.\". The model failed to accurately calculate the length of the hypotenuse, which is a fundamental concept in geometry and trigonometry.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> 1. Squirrel\n",
      "2. Eagle\n",
      "3. Tiger\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "1. Squirrel\n",
      "2. Tiger\n",
      "3. Eagle\n",
      "4. Cobra\n",
      "5. Tiger\n",
      "6. Cobra\n",
      "\n",
      "Score:\n",
      ">> I'd rate this model response as 60 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies two animals that are active during the day: Squirrel and Eagle.\n",
      "* However, it incorrectly includes Tiger twice, which is not a different animal from the original list.\n",
      "* Cobra is also an incorrect answer, as it is typically nocturnal or crepuscular (active at twilight).\n",
      "* The response does not meet the instruction to provide three different animals that are active during the day.\n",
      "\n",
      "To achieve a higher score, the model should have provided three unique and correct answers that fit the instruction.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> I must ascertain what is incorrect.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "What is incorrect?\n",
      "\n",
      "Score:\n",
      ">> A clever test!\n",
      "\n",
      "Here's my attempt at rewriting the sentence in a more formal way:\n",
      "\n",
      "\"I require an identification of the issue.\"\n",
      "\n",
      "Now, let's evaluate the model response \"What is incorrect?\" against the correct output \"I must ascertain what is incorrect.\".\n",
      "\n",
      "To me, this seems like a completely different question being asked. The original instruction was to rewrite the sentence in a more formal way, and the model response doesn't even attempt to do that. It's asking a new question altogether!\n",
      "\n",
      "So, I'd score this response a 0 out of 100.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The interjection in the sentence is 'Wow'.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1. The instruction asks to identify the interjection in the sentence.\n",
      "2. The input sentence is provided: \"Wow, that was an amazing trick!\"\n",
      "3. The model correctly identifies the interjection as \"Wow\", which is a common English interjection used to express surprise or excitement.\n",
      "4. The response accurately answers the question and provides the correct information.\n",
      "\n",
      "Overall, the model's response perfectly completes the request, making it a 100% accurate answer!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of sentence is interrogative.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "The type of sentence is exclamatory.\n",
      "\n",
      "Score:\n",
      ">> I'd rate this model response as 20 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The input sentence \"Did you finish the report?\" is indeed an interrogative sentence, which asks a question.\n",
      "* The model response says it's exclamatory, which is incorrect. Exclamatory sentences are typically marked by an exclamation mark (!) and express strong emotions or emphasis, whereas this sentence is simply asking a question.\n",
      "\n",
      "The correct output \"The type of sentence is interrogative.\" is the best possible score (100), while the model response is significantly off the mark, hence the low score.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[:5]:\n",
    "    prompt = (f\"Given the input `{format_input(entry)}` \"\n",
    "              f\"and correct output `{entry['output']}`, \"\n",
    "              f\"score the model response `{entry['model 1 response']}`\"\n",
    "              f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "              )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model 1 response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- **注意**：生成的响应较为冗长，为了更清晰地比较模型优劣，  \n",
    "  **我们仅提取评分结果** 进行量化分析。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_model_scores(json_data, json_key):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071ce84-1866-427f-a272-b46700f364b2",
   "metadata": {},
   "source": [
    "- 现在，我们对 **整个数据集** 进行评估，并计算 **每个模型的平均分**（在 **M3 MacBook Air** 上运行 **每个模型约需 1 分钟**）。  \n",
    "- **请注意**，截至目前，Ollama **在不同操作系统上的推理结果并非完全确定性**，  \n",
    "  因此，您的评估分数可能会与下方示例结果 **略有不同**。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 1 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 78.48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:10<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 2 response\n",
      "Number of scores: 99 of 100\n",
      "Average score: 64.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for model in (\"model 1 response\", \"model 2 response\"):\n",
    "\n",
    "    scores = generate_model_scores(json_data, model)\n",
    "    print(f\"\\n{model}\")\n",
    "    print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "    print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # 可选：将分数保存到 JSON 文件中\n",
    "    save_path = Path(\"scores\") / f\"llama3-8b-{model.replace(' ', '-')}.json\"\n",
    "    with open(save_path, \"w\") as file:\n",
    "        json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169d534-1fec-43c4-9550-5cb701ff7f05",
   "metadata": {},
   "source": [
    "- **根据上述评估结果**，可以判断 **第一个模型的表现优于第二个模型**。  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用 LLaMA 3.1 70B 和 Ollama 生成偏好数据集  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- **偏好微调（Preference Finetuning）** 旨在使 **指令微调后的 LLM** 更加符合 **人类偏好**。  \n",
    "- 生成 **偏好微调数据集** 有多种方法：\n",
    "  1. **使用指令微调 LLM 生成多个响应**，并由 **人工根据偏好标准进行排序**。  \n",
    "  2. **使用指令微调 LLM 生成多个响应**，并由 **LLM 根据设定的偏好标准进行排序**。  \n",
    "  3. **使用 LLM 基于特定偏好标准直接生成偏好（Preferred）和非偏好（Dispreferred）响应**。  \n",
    "\n",
    "- **本笔记本采用方法 3**。  \n",
    "- 这里使用 **70B 参数的 LLaMA 3.1-Instruct 模型**（通过 **Ollama** 运行）为 **指令数据集生成偏好标签**。  \n",
    "- **期望的指令数据集格式如下**：\n",
    "\n",
    "### 输入（Input）\n",
    "\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"What is the state capital of California?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The state capital of California is Sacramento.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of Greece?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The capital of Greece is Athens.\",\n",
    "\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "生成的数据集格式如下，其中 **较礼貌的响应** 被标记为 **`'chosen'`（偏好响应）**，**较不礼貌的响应** 被标记为 **`'rejected'`（非偏好响应）**：\n",
    "\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"What is the state capital of California?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The state capital of California is Sacramento.\",\n",
    "        \"rejected\": \"Look, the state capital of California is obviously Sacramento.\",\n",
    "        \"chosen\": \"The state capital of California is Sacramento.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "        \"chosen\": \"A suitable alternative to 'fast' would be 'quick'.\",\n",
    "        \"rejected\": \"A synonym for 'fast' is 'quick'.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of Greece?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The capital of Greece is Athens.\",\n",
    "        \"chosen\": \"I'd be happy to help! The capital of Greece is indeed Athens.\",\n",
    "        \"rejected\": \"The capital of Greece is Athens.\"\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "### 输出（Output）\n",
    "\n",
    "- 该代码 **无需 GPU**，在 **RAM 充足的笔记本电脑** 上即可运行。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # 进度条\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## 安装 Ollama 并下载 LLaMA 3.1\n",
    "\n",
    "- **Ollama** 是一个用于高效运行 **LLM（大语言模型）** 的应用。  \n",
    "- 它是 **[llama.cpp](https://github.com/ggerganov/llama.cpp)** 的封装，后者采用 **纯 C/C++ 实现 LLM**，以 **最大化推理效率**。  \n",
    "- **请注意**，Ollama **仅用于 LLM 推理（inference）**，**不支持训练或微调（finetuning）**。  \n",
    "- **在运行下方代码前**，请先访问 **[https://ollama.com](https://ollama.com)** 并按照安装指南完成 **Ollama 安装**（例如，点击 **“Download”** 按钮，下载适用于您的操作系统的 Ollama 应用）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- **对于 macOS 和 Windows 用户**，点击 **下载的 Ollama 应用**，如果系统提示安装 **命令行工具**，请选择 **“是”**。  \n",
    "- **Linux 用户** 可以使用 **Ollama 官网提供的安装命令** 进行安装。  \n",
    "\n",
    "- **通常，在命令行使用 Ollama 之前**，需要 **启动 Ollama 应用** 或 **在终端运行 `ollama serve`**。  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/21.png\">\n",
    "\n",
    "- **确保 Ollama 运行后**，在 **另一个终端窗口** 执行以下命令，尝试 **700 亿参数的 LLaMA 3.1 模型**：  \n",
    "\n",
    "\n",
    "```bash\n",
    "# 70B 模型\n",
    "ollama run llama3.1:70b\n",
    "```\n",
    "\n",
    "\n",
    "The output looks like as follows:\n",
    "\n",
    "```\n",
    "$ ollama run llama3.1:70b\n",
    "pulling manifest\n",
    "pulling aa81b541aae6... 100% ▕████████████████▏ 39 GB\n",
    "pulling 8cf247399e57... 100% ▕████████████████▏ 1.7 KB\n",
    "pulling f1cd752815fc... 100% ▕████████████████▏ 12 KB\n",
    "pulling 56bb8bd477a5... 100% ▕████████████████▏ 96 B\n",
    "pulling 3c1c2d3df5b3... 100% ▕████████████████▏ 486 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "- **注意**：`llama3.1:70b` 指的是 **指令微调后的 700 亿参数 LLaMA 3.1 模型**。  \n",
    "\n",
    "- **如果您的设备资源有限**，可以选择 **更轻量的 80 亿参数 LLaMA 3.1 模型**，  \n",
    "  **只需将 `llama3.1:70b` 替换为 `llama3.1`**。  \n",
    "\n",
    "- **下载完成后**，您将进入 **命令行交互界面**，可与模型进行对话。  \n",
    "\n",
    "- **尝试输入以下提示**：\"What do llamas eat?\"（羊驼吃什么？），  \n",
    "  预计模型会返回类似如下的输出：  \n",
    "\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered \n",
    "stomach and eat plants that are high in fiber. In the wild, llamas \n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall \n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
   "metadata": {},
   "source": [
    "- 输入`/bye`以结束这一节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
   "metadata": {},
   "source": [
    "## 使用 Ollama 的 REST API\n",
    "\n",
    "- **另一种与模型交互的方式** 是通过 **REST API** 在 **Python** 中进行调用，具体实现如下。  \n",
    "- **在运行本笔记本中的代码前**，请确保 **Ollama 仍在运行**，可通过以下方式启动：\n",
    "  - 在终端中执行 `ollama serve`\n",
    "  - 使用 **Ollama 应用程序**  \n",
    "\n",
    "- **接下来，运行下方代码单元**，以查询模型并获取响应。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- 首先，我们使用 **一个简单示例** 调用 API，以确保其 **正常运行**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
      "\n",
      "1. **Grasses**: Various types of grasses, including timothy grass, orchard grass, and brome grass.\n",
      "2. **Hay**: High-quality hay, such as alfalfa or clover hay, is a staple in a llama's diet.\n",
      "3. **Leaves**: Leaves from trees and shrubs, like willow, cottonwood, and mesquite, are also eaten.\n",
      "4. **Fruits and vegetables**: Llamas enjoy fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
      "5. **Grains**: In moderation, llamas can eat grains like oats, barley, and corn.\n",
      "\n",
      "It's essential to note that llamas have a unique digestive system, with a three-part stomach and a large cecum (a specialized part of the large intestine). This allows them to break down and extract nutrients from plant material more efficiently than many other animals.\n",
      "\n",
      "A typical llama diet might consist of:\n",
      "\n",
      "* 1-2% of their body weight in hay per day\n",
      "* 0.5-1% of their body weight in grains per day (if fed)\n",
      "* Free-choice access to fresh water\n",
      "* Limited amounts of fruits and vegetables as treats\n",
      "\n",
      "It's also important to ensure that llamas have access to a mineral supplement, such as a salt lick or loose minerals, to help maintain optimal health.\n",
      "\n",
      "Remember, every llama is different, and their dietary needs may vary depending on factors like age, size, and activity level. Consult with a veterinarian or experienced llama breeder for specific guidance on feeding your llama.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def query_model(prompt, model=\"llama3.1:70b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # 创建数据负载作为字典\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 将字典转换为 JSON 格式的字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # 创建请求对象，设置方法为 POST 并添加必要的头信息\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并捕获响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 加载 JSON 数据（Load JSON Entries）\n",
    "\n",
    "- 现在，我们进入 **数据生成** 部分。  \n",
    "- **为了直观演示**，我们将使用 **`instruction-data.json`** 文件，  \n",
    "  该文件最初用于 **第 7 章的指令微调（Instruction Finetuning）**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- **该文件的结构如下**，其中：\n",
    "  - `'output'`：测试数据集中提供的 **预期响应**，即模型通过 **指令微调（Instruction Finetuning）** 训练后应生成的内容。  \n",
    "  - `'input'` 和 `'instruction'`：用于指导模型生成 `'output'` 的 **输入数据**。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
       " 'input': 'freind --> friend',\n",
       " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- 下面是一个 **小型工具函数**，用于格式化 **指令（instruction）和输入（input）**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 现在，我们使用 **Ollama API** 生成 **`'chosen'`（偏好）** 和 **`'rejected'`（非偏好）** 响应，  \n",
    "  以进行 **模型的偏好微调（Preference Tuning）**。  \n",
    "- **为了直观演示**，这里生成的回答在 **礼貌程度** 上存在 **明显差异**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "\n",
      "impolite response:\n",
      ">> The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n",
      "\n",
      "Dataset response:\n",
      ">> He goes to the park every day.\n",
      "\n",
      "polite response:\n",
      ">> He goes to the park daily, if I'm not mistaken.\n",
      "\n",
      "Dataset response:\n",
      ">> 45 kilometers is 45000 meters.\n",
      "\n",
      "polite response:\n",
      ">> 45 kilometers is equivalent to 45000 meters.\n",
      "\n",
      "Dataset response:\n",
      ">> Although it was raining, they went for a walk.\n",
      "\n",
      "polite response:\n",
      ">> Although it was raining outside, they still decided to go for a walk.\n",
      "\n",
      "Dataset response:\n",
      ">> 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
      "\n",
      "impolite response:\n",
      ">> Here are your precious square numbers: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for entry in json_data[:5]:\n",
    "    \n",
    "    politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return return the generated response and nothing else.\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(f\"\\n{politeness} response:\")\n",
    "    print(\">>\", query_model(prompt))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- **如果我们认为上面生成的响应较为合理**，可以进入 **下一步**，将该提示（prompt）应用于 **整个数据集**。  \n",
    "- **在数据集中添加**：\n",
    "  - **`'chosen'`**：代表 **偏好（preferred）响应**  \n",
    "  - **`'rejected'`**：代表 **非偏好（dispreferred）响应**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3349dbbc-963f-4af3-9790-12dbfdca63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_responses(json_data):\n",
    "\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entries\")):\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return return the generated response and nothing else.\"\n",
    "        )\n",
    "        response = query_model(prompt)\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071ce84-1866-427f-a272-b46700f364b2",
   "metadata": {},
   "source": [
    "- 现在，我们对 **整个数据集** 进行评估，并计算 **每个模型的平均分**（在 **M3 MacBook Air** 上运行 **每个模型约需 1 分钟**）。  \n",
    "- **请注意**，截至目前，Ollama **在不同操作系统上的推理结果并非完全确定性**，  \n",
    "  因此，您的评估分数可能会与下方示例结果 **略有不同**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing entries: 100%|██████████| 1100/1100 [17:20<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_model_responses(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838d9747-0f7d-46fe-aab5-9ee6b765d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62129596-d10f-45b1-a1af-ee10f358f773",
   "metadata": {
    "id": "62129596-d10f-45b1-a1af-ee10f358f773"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd2379-ed2f-4c77-8b71-f1f0242b9ff9",
   "metadata": {
    "id": "b0bd2379-ed2f-4c77-8b71-f1f0242b9ff9"
   },
   "source": [
    "# 直接偏好优化（DPO）用于 LLM 对齐（从零实现）\n",
    "\n",
    "- 本代码笔记本 **从零实现** **直接偏好优化（Direct Preference Optimization, DPO）**，  \n",
    "  并将其应用于 **大语言模型（LLM）**，以提高其 **生成符合用户偏好的响应能力**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pxMGAf3bnVwn",
   "metadata": {
    "id": "pxMGAf3bnVwn"
   },
   "outputs": [],
   "source": [
    "# !pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb3e145-fbaa-4bb3-9e95-186b4145087f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edb3e145-fbaa-4bb3-9e95-186b4145087f",
    "outputId": "3d449525-76cc-4124-ab30-a93c6a9623ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",    # 分词器\n",
    "    \"torch\",       # 深度学习库\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec20a3-a26c-4f9b-8a33-bfd3d67860e2",
   "metadata": {
    "id": "49ec20a3-a26c-4f9b-8a33-bfd3d67860e2"
   },
   "source": [
    "&nbsp;\n",
    "# 1) 导论·DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17804afd-786b-4600-bad0-f5805454e3d6",
   "metadata": {
    "id": "17804afd-786b-4600-bad0-f5805454e3d6"
   },
   "source": [
    "- **直接偏好优化（DPO）** 是 **论文 [《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》](https://arxiv.org/abs/2305.18290)** 提出的方法，  \n",
    "  它是 **一种替代强化学习（RLHF）** 的方法，可用于 **微调大语言模型（LLM）**。  \n",
    "- **DPO 可用于微调（对齐）模型**，使其生成的响应 **更符合用户期望和指令**。  \n",
    "\n",
    "<img src=\"../image/1.webp\" width=500px>\n",
    "\n",
    "- 在 **指令微调（Instruction Finetuning）** 过程中，我们训练 LLM **根据提示词生成正确答案**。  \n",
    "- **然而，在实际应用中**，正确答案可能有 **多种表达方式**，风格也可能不同：  \n",
    "  - 例如，回答相同的问题，模型可以给出 **技术性解答**，也可以给出 **更友好的用户导向解答**。  \n",
    "  - 下图展示了 **当用户咨询购买笔记本电脑建议时**，LLM 可能生成的两种不同风格的响应：  \n",
    "\n",
    "<img src=\"../image/2.webp\" width=700px>\n",
    "\n",
    "- **RLHF（强化学习人类反馈）** 和 **DPO（直接偏好优化）** 是两种常见的方法，  \n",
    "  它们可以 **引导 LLM 更倾向于某种特定回答风格，以更好地满足用户偏好**。  \n",
    "- **RLHF（强化学习人类反馈）** 需要 **训练一个单独的奖励模型（Reward Model）**，其流程如下图所示：\n",
    "\n",
    "<img src=\"../image/4.webp\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073622f-d537-42bf-8778-43c2adaa2191",
   "metadata": {
    "id": "9073622f-d537-42bf-8778-43c2adaa2191"
   },
   "source": [
    "- **与 RLHF 相比**，DPO **简化了优化流程**，  \n",
    "  **无需构建复杂的奖励模型（Reward Model）和策略优化（Policy Optimization）**，  \n",
    "  **直接优化模型以符合用户偏好**。  \n",
    "- 换句话说，**DPO 直接调整模型的输出**，使其更符合 **人类偏好** 或 **特定目标**。  \n",
    "- 下图概述了 **DPO 的核心思想** 及其 **优化过程**：\n",
    "\n",
    "<img src=\"../image/5.webp\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894134a-315c-453e-bbc1-387794b3f4d6",
   "metadata": {
    "id": "c894134a-315c-453e-bbc1-387794b3f4d6"
   },
   "source": [
    "- 实现 DPO 损失的具体方程如下所示；我们将在本代码笔记本的后面部分用 Python 实现该方程时再次讨论它。\n",
    "\n",
    "<img src=\"../image/3.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7491b5-f619-4501-ad39-2942de57c115",
   "metadata": {
    "id": "dd7491b5-f619-4501-ad39-2942de57c115"
   },
   "source": [
    "- 在上方公式中：\n",
    "  - **“期望值”（Expected Value，$\\mathbb{E}$）** 是统计学术语，表示 **随机变量的平均值**（即括号内的表达式）。  \n",
    "    **优化 $-\\mathbb{E}$ 可使模型更符合用户偏好**。  \n",
    "  - **$\\pi_{\\theta}$ 代表策略（Policy）**，该术语来源于强化学习，  \n",
    "    **在 DPO 中，它表示我们要优化的 LLM**；  \n",
    "    **$\\pi_{ref}$ 是参考模型（Reference LLM）**，通常是优化前的原始 LLM。  \n",
    "    **在训练初期，$\\pi_{\\theta}$ 和 $\\pi_{ref}$ 通常是相同的**。  \n",
    "  - **$\\beta$ 是控制 $\\pi_{\\theta}$ 和参考模型之间散度的超参数**：  \n",
    "    - **增大 $\\beta$ 会加大两者对数概率（Log Probabilities）之间的影响**，  \n",
    "      **从而提高优化后的 LLM 与原始 LLM 之间的差异**。  \n",
    "  - **逻辑 Sigmoid 函数（$\\sigma(\\centerdot)$）** 用于 **将偏好响应与非偏好响应的对数几率（Log-Odds）转换为概率分数**。  \n",
    "\n",
    "- **为了避免代码笔记本过于冗长**，关于这些概念的更详细讨论，  \n",
    "  我可能会在未来撰写 **独立的文章** 进行介绍。  \n",
    "\n",
    "- **如果您想比较 RLHF 与 DPO**，可以参考我的文章：\n",
    "  - **[LLM 预训练与奖励模型评估技巧](https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms)**\n",
    "  - 其中的 **[2.2. RLHF vs 直接偏好优化（DPO）](https://magazine.sebastianraschka.com/i/142924793/rlhf-vs-direct-preference-optimization-dpo)** 部分详细分析了两者的区别。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xqVAgsyQ6LuG",
   "metadata": {
    "id": "xqVAgsyQ6LuG",
    "tags": []
   },
   "source": [
    "&nbsp;  \n",
    "# 2) 准备 DPO 偏好数据集  \n",
    "\n",
    "- **首先，我们加载并准备数据集**，  \n",
    "  这可能会 **回答您在进一步研究 DPO 损失函数之前的许多问题**。  \n",
    "- 这里，我们使用的数据集包含 **对相同指令的两种不同风格的响应**：  \n",
    "  - **较为礼貌的（Preferred）**  \n",
    "  - **较不礼貌的（Dispreferred）**  \n",
    "  - **具体示例将在下一节展示**。  \n",
    "- **该数据集** 通过 **[create-preference-data-ollama.ipynb](create-preference-data-ollama.ipynb)** 生成。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHLB62Nj7haD",
   "metadata": {
    "id": "wHLB62Nj7haD"
   },
   "source": [
    "&nbsp;  \n",
    "## 2.1) 加载偏好数据集  \n",
    "\n",
    "- **该数据集为 JSON 文件**，共包含 **1100 条样本**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5266e66c-5ec0-45e6-a654-148971f6aee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5266e66c-5ec0-45e6-a654-148971f6aee7",
    "outputId": "04e8ee70-3076-441d-d2bf-7641da3d0c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data-with-preference.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/04_preference-tuning-with-dpo/instruction-data-with-preference.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d2b9a-d6d2-46e2-89f8-5ab87e040e3b",
   "metadata": {
    "id": "725d2b9a-d6d2-46e2-89f8-5ab87e040e3b"
   },
   "source": [
    "- 让我们来看两个示例数据: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c11916f-9a26-4367-a16e-7b0c121a20a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c11916f-9a26-4367-a16e-7b0c121a20a6",
    "outputId": "00a432cc-19b1-484f-80e2-e897ee5e4024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Identify the correct spelling of the following word.',\n",
      " 'input': 'Ocassion',\n",
      " 'output': \"The correct spelling is 'Occasion.'\",\n",
      " 'rejected': \"The correct spelling is obviously 'Occasion.'\",\n",
      " 'chosen': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef804a-8c13-4a0b-9b2e-b65a4d0a870d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01ef804a-8c13-4a0b-9b2e-b65a4d0a870d",
    "outputId": "078cd643-83fb-4b42-ecf9-3256e8c9d239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"What is an antonym of 'complicated'?\",\n",
      " 'input': '',\n",
      " 'output': \"An antonym of 'complicated' is 'simple'.\",\n",
      " 'chosen': \"A suitable antonym for 'complicated' would be 'simple'.\",\n",
      " 'rejected': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db5697-a089-4b40-a1f3-e928e8018220",
   "metadata": {
    "id": "56db5697-a089-4b40-a1f3-e928e8018220"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "- 如上所示，数据集包含 **5 个键（keys）**：\n",
    "  - **`'instruction'`** 和 **`'input'`**：用于 **提供给 LLM 的输入**。  \n",
    "  - **`'output'`**：模型在 **第 7 章的指令微调阶段** 训练时生成的 **标准响应**。  \n",
    "  - **`'chosen'`** 和 **`'rejected'`**：DPO 训练所需的 **偏好数据**：\n",
    "    - **`'chosen'`**：**偏好（Preferred）** 响应。  \n",
    "    - **`'rejected'`**：**非偏好（Dispreferred）** 响应。  \n",
    "\n",
    "- **DPO 训练的目标** 是让 **模型更倾向于生成 `'chosen'` 风格的响应**，而 **避免 `'rejected'` 风格**。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86257468-a6ab-4ba3-9c9f-2fdc2c0cc284",
   "metadata": {
    "id": "86257468-a6ab-4ba3-9c9f-2fdc2c0cc284"
   },
   "source": [
    "- 下面是一个 **工具函数**，用于 **格式化模型输入**，  \n",
    "  采用的格式与 **第 7 章（[../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb)）**  \n",
    "  **相同，基于 Alpaca 提示风格（Alpaca Prompt Style）**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4564d55c-1c5d-46a6-b5e8-46ab568ad627",
   "metadata": {
    "id": "4564d55c-1c5d-46a6-b5e8-46ab568ad627"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f38b49f-63fd-48c5-bde8-a4717b7923ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f38b49f-63fd-48c5-bde8-a4717b7923ea",
    "outputId": "9ad07c59-05b3-42ae-c5bc-68780aaf6780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9e4c9-88a3-463a-8c16-c60ed7e6b51e",
   "metadata": {
    "id": "7dd9e4c9-88a3-463a-8c16-c60ed7e6b51e"
   },
   "source": [
    "- 同样，我们可以使用 **Alpaca 提示风格** 格式化 **`chosen`（偏好响应）** 和 **`rejected`（非偏好响应）**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad5831a-e936-44e5-a5cf-02953fe7d848",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ad5831a-e936-44e5-a5cf-02953fe7d848",
    "outputId": "2c0a0cbf-c13d-43cf-fcc1-a4585c21e66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "desired_response = f\"### Response:\\n{data[50]['chosen']}\"\n",
    "print(desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0991f6-fef7-48ab-8dee-fbd2863f784c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc0991f6-fef7-48ab-8dee-fbd2863f784c",
    "outputId": "cd85406c-3470-48f8-9792-63f91affd50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The correct spelling is obviously 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "possible_response = f\"### Response:\\n{data[50]['rejected']}\"\n",
    "print(possible_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6G3j2Q987t_g",
   "metadata": {
    "id": "6G3j2Q987t_g"
   },
   "source": [
    "&nbsp;  \n",
    "## 2.2) 划分训练集、验证集和测试集  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558ecc7",
   "metadata": {},
   "source": [
    "- 接下来，我们将数据集划分为 **3 个子集**：\n",
    "  - **85%** 用作 **训练集（Training Set）**  \n",
    "  - **5%** 用作 **验证集（Validation Set）**  \n",
    "  - **10%** 用作 **测试集（Test Set）**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c7b919-8531-4e33-aebf-aaf8e6dbcfbd",
   "metadata": {
    "id": "36c7b919-8531-4e33-aebf-aaf8e6dbcfbd"
   },
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% 用作训练集\n",
    "test_portion = int(len(data) * 0.1)    # 10% 用作测试集\n",
    "val_portion = len(data) - train_portion - test_portion  # 剩下的 5% 用作验证集\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831a6c1b-119b-4622-9862-87f1db36e066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "831a6c1b-119b-4622-9862-87f1db36e066",
    "outputId": "8e017483-1a75-4336-9540-ac6a69104e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d09f7-66af-49ed-8b9e-484f46e6a68d",
   "metadata": {
    "id": "c07d09f7-66af-49ed-8b9e-484f46e6a68d"
   },
   "source": [
    "&nbsp;\n",
    "## 2.3) 开发 `PreferenceDataset` 类与批量处理函数  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86101174-00c8-485d-8273-d086d5311926",
   "metadata": {
    "id": "86101174-00c8-485d-8273-d086d5311926"
   },
   "source": [
    "- 在本节中，我们基于 **第 7 章的 `InstructionDataset` 类**  \n",
    "  （[../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb)）**重新编写数据集类，以适用于 DPO 训练**。  \n",
    "- **不同之处在于**，我们不再仅关注 **单个输出序列（response）**，  \n",
    "  **而是返回两条响应，其中一条是偏好（\"chosen\"），另一条是非偏好（\"rejected\"）**。  \n",
    "- **整体而言**，`PreferenceDataset` **与** `InstructionDataset` **几乎相同**，  \n",
    "  只是针对 **DPO 训练的需求做了调整**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db08ad74-6dd4-4e40-b1e5-bc5f037d3d27",
   "metadata": {
    "id": "db08ad74-6dd4-4e40-b1e5-bc5f037d3d27"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # 预分词文本\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            prompt = format_input(entry)\n",
    "            rejected_response = entry[\"rejected\"]\n",
    "            chosen_response = entry[\"chosen\"]\n",
    "\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "            chosen_full_text = f\"{prompt}\\n\\n### Response:\\n{chosen_response}\"\n",
    "            rejected_full_text = f\"{prompt}\\n\\n### Response:\\n{rejected_response}\"\n",
    "            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n",
    "            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n",
    "\n",
    "            self.encoded_texts.append({\n",
    "                \"prompt\": prompt_tokens,\n",
    "                \"chosen\": chosen_full_tokens,\n",
    "                \"rejected\": rejected_full_tokens,\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325d183-75b9-400a-80ac-0b8d2f526561",
   "metadata": {
    "id": "2325d183-75b9-400a-80ac-0b8d2f526561"
   },
   "source": [
    "- 除了 **更新 `PreferenceDataset` 类**，我们还需要 **更新批处理（batch collation）函数**，  \n",
    "  该函数的作用是 **对每个批次（batch）中的序列进行填充（padding），使其长度一致**，  \n",
    "  以便将数据 **批量组织** 进行训练。  \n",
    "- **下方代码已添加注释**，帮助理解数据处理流程；  \n",
    "  **不过，最直观的方式是查看后续示例输入和输出**，了解其工作原理。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3a43a6-7704-4bff-9bbc-a38632374f30",
   "metadata": {
    "id": "8d3a43a6-7704-4bff-9bbc-a38632374f30"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    allowed_max_length=None,\n",
    "    mask_prompt_tokens=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 初始化列表以保存批次数据\n",
    "    batch_data = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "        \"rejected_mask\": [],\n",
    "        \"chosen_mask\": []\n",
    "\n",
    "    }\n",
    "\n",
    "    # 确定最长的序列以设置相同的填充长度\n",
    "    max_length_common = 0\n",
    "    if batch:\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            current_max = max(len(item[key])+1 for item in batch)\n",
    "            max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "    # 处理批次中的每个项目\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item[\"prompt\"])\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            # 根据相同的最大长度调整填充\n",
    "            sequence = item[key]\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "\n",
    "            # 将所有填充标记的掩码设置为 False\n",
    "            mask[len(sequence):] = False\n",
    "\n",
    "            # 将所有填充标记的掩码设置为 False\n",
    "            # +2 将 \"### Response\" 之前的 2 个换行符 (\"\\n\") 标记设置为 False\n",
    "            if mask_prompt_tokens:\n",
    "                mask[:prompt.shape[0]+2] = False\n",
    "\n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "            batch_data[f\"{key}_mask\"].append(mask)\n",
    "\n",
    "    # 最终处理\n",
    "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "        # 将所有序列堆叠为给定键的张量\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "\n",
    "        # 可选：截断到最大序列长度\n",
    "        if allowed_max_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
    "\n",
    "        # 移动到指定设备\n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3744b-9bb0-4f1e-b66b-cff35ad8fd9f",
   "metadata": {
    "id": "76f3744b-9bb0-4f1e-b66b-cff35ad8fd9f"
   },
   "source": [
    "- 在正式使用 **自定义批处理函数（collate function）** 之前，  \n",
    "  **我们先创建一个预填充部分参数的版本**，以便后续调用更加方便：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cc137c-7ed7-4758-a518-cc4071b2817a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3cc137c-7ed7-4758-a518-cc4071b2817a",
    "outputId": "598e9def-9768-441a-f886-01f6ba6e250b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,            # 如果有 GPU，直接将数据放在 GPU 上\n",
    "    mask_prompt_tokens=True,  # 这是可选的\n",
    "    allowed_max_length=1024   # 模型支持的上下文长度\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29e996-e267-4348-bc1d-4ac6b725cf6a",
   "metadata": {
    "id": "5d29e996-e267-4348-bc1d-4ac6b725cf6a"
   },
   "source": [
    "- 现在，让我们实际运行 **`customized_collate_fn`**，  \n",
    "  并将其应用于 **偏好数据集中的示例数据**；  \n",
    "  **为此，我们选取数据集的前两条样本进行测试**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1171057d-2a0f-48ff-bad6-4917a072f0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1171057d-2a0f-48ff-bad6-4917a072f0f5",
    "outputId": "3db3eee8-db29-4ff6-8078-6577a05d953a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'instruction': 'Evaluate the following phrase by transforming it into the '\n",
      "                'spelling given.',\n",
      " 'input': 'freind --> friend',\n",
      " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".',\n",
      " 'rejected': 'The spelling of the given phrase \"freind\" is flat out wrong, get '\n",
      "             'it together, the correct spelling is \"friend\".',\n",
      " 'chosen': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".'}\n",
      "\n",
      "{'instruction': 'Edit the following sentence for grammar.',\n",
      " 'input': 'He go to the park every day.',\n",
      " 'output': 'He goes to the park every day.',\n",
      " 'rejected': 'He goes to the stupid park every single day.',\n",
      " 'chosen': 'He goes to the park every day.'}\n"
     ]
    }
   ],
   "source": [
    "example_data = data[:2]\n",
    "\n",
    "for i in example_data:\n",
    "    print()\n",
    "    pprint.pp(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1436cc-fbe5-4581-89d8-1992b5f04042",
   "metadata": {
    "id": "8f1436cc-fbe5-4581-89d8-1992b5f04042"
   },
   "source": [
    "- 接下来，我们实例化 **`example_dataset`**，  \n",
    "  并使用 **PyTorch `DataLoader`** 创建 **`example_dataloader`**，  \n",
    "  **以模拟后续用于模型训练的数据加载器**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db327575-c34b-4fea-b3c7-e30569c9be78",
   "metadata": {
    "id": "db327575-c34b-4fea-b3c7-e30569c9be78"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "example_dataset = PreferenceDataset(example_data, tokenizer)\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a446b7-7037-4d9a-9f14-b4ee0f6f37af",
   "metadata": {
    "id": "43a446b7-7037-4d9a-9f14-b4ee0f6f37af"
   },
   "source": [
    "- 数据集包含以下键（keys）：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ed4cf9-d70a-4bc7-b676-67e76ed3ee10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87ed4cf9-d70a-4bc7-b676-67e76ed3ee10",
    "outputId": "fa724d65-b0e1-4239-8090-9263135ad199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.keys: dict_keys(['prompt', 'chosen', 'rejected', 'rejected_mask', 'chosen_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch in example_dataloader:\n",
    "    break\n",
    "\n",
    "print(\"batch.keys:\", batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda3193-8c68-478c-98d8-0d9d880e7077",
   "metadata": {
    "id": "5bda3193-8c68-478c-98d8-0d9d880e7077"
   },
   "source": [
    "- `prompts` 是一个 **张量（tensor）列表**，其中 **每个张量**  \n",
    "  **包含一个样本的 token ID**；  \n",
    "  **由于我们设定的批量大小（batch size）为 2**，  \n",
    "  **因此这里包含两个 token ID 张量**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468995ce-2906-498f-ac99-0a3f80d13d12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "468995ce-2906-498f-ac99-0a3f80d13d12",
    "outputId": "7f3df961-fcb5-4e49-9b0c-c99447c67cc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198,    36,  2100,  4985,   262,  1708,  9546,\n",
       "           416, 25449,   340,   656,   262, 24993,  1813,    13,   198,   198,\n",
       "         21017, 23412,    25,   198, 19503,   521, 14610,  1545]),\n",
       " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198, 18378,   262,  1708,  6827,   329, 23491,\n",
       "            13,   198,   198, 21017, 23412,    25,   198,  1544,   467,   284,\n",
       "           262,  3952,   790,  1110,    13])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cadebe-2516-4ae0-a71f-a8a623f2e1da",
   "metadata": {
    "id": "89cadebe-2516-4ae0-a71f-a8a623f2e1da"
   },
   "source": [
    "- **训练时，我们实际上不需要 `responses`**，  \n",
    "  **模型训练时需要输入的是 `\"chosen\"`（偏好）和 `\"rejected\"`（非偏好）条目**。  \n",
    "- **`\"chosen\"` 和 `\"rejected\"` 响应序列已进行填充（padding）**，  \n",
    "  **这样它们可以被堆叠为张量（tensor）**；  \n",
    "  **与 `prompts` 类似，这些响应文本已被转换为 token ID**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8f49c56-3989-4fe9-81ac-6bb3cce1a5b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8f49c56-3989-4fe9-81ac-6bb3cce1a5b8",
    "outputId": "ccc0bd06-6e85-4ee9-893b-d985f26a835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198,    36,  2100,  4985,   262,  1708,  9546,\n",
       "           416, 25449,   340,   656,   262, 24993,  1813,    13,   198,   198,\n",
       "         21017, 23412,    25,   198, 19503,   521, 14610,  1545,   198,   198,\n",
       "         21017, 18261,    25,   198,   464, 24993,   286,   262,  1813,  9546,\n",
       "           366, 19503,   521,     1,   318, 11491,    11,   262,  3376, 24993,\n",
       "           318,   366,  6726,  1911, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256],\n",
       "        [21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198, 18378,   262,  1708,  6827,   329, 23491,\n",
       "            13,   198,   198, 21017, 23412,    25,   198,  1544,   467,   284,\n",
       "           262,  3952,   790,  1110,    13,   198,   198, 21017, 18261,    25,\n",
       "           198,  1544,  2925,   284,   262,  3952,   790,  1110,    13, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4cd6d-b2ad-45a6-b00a-ba5b720be4ea",
   "metadata": {
    "id": "35a4cd6d-b2ad-45a6-b00a-ba5b720be4ea"
   },
   "source": [
    "- 上面显示的 token ID 代表了模型的输入，但以这种格式呈现时，对于我们人类来说是很难理解的\n",
    "- 因此，让我们实现一个小工具函数，将它们转换为文本格式，以便我们可以更容易地检查和理解这些内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ea54ba-32cb-4ecb-b38b-923f42fd4615",
   "metadata": {
    "id": "52ea54ba-32cb-4ecb-b38b-923f42fd4615"
   },
   "outputs": [],
   "source": [
    "def decode_tokens_from_batch(token_ids, tokenizer):\n",
    "    ids_in_python_list = token_ids.flatten().tolist()\n",
    "    return tokenizer.decode(ids_in_python_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9dd0ce-1fd4-419c-833f-ea5a1f8d800d",
   "metadata": {
    "id": "bc9dd0ce-1fd4-419c-833f-ea5a1f8d800d"
   },
   "source": [
    "- 让我们对批次中的第一个提示语条目应用 `decode_tokens_from_batch` 工具函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ee481e-3e2c-4ff6-b614-8cb18eb16a41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ee481e-3e2c-4ff6-b614-8cb18eb16a41",
    "outputId": "17ddec15-a09d-45b5-b1e8-600cd59a9600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"prompt\"][0],  # [0]表示批次中的第一个条目\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b95c4-d5c2-4492-9d19-a45b090eee7e",
   "metadata": {
    "id": "637b95c4-d5c2-4492-9d19-a45b090eee7e"
   },
   "source": [
    "- 如上所示，提示语已经正确格式化；现在我们对 `\"chosen\"` 响应做同样的处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33a24f20-5ec3-4a89-b57a-52e997163d07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a24f20-5ec3-4a89-b57a-52e997163d07",
    "outputId": "e04366ee-3719-4b07-fcef-6e9dddc06310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"chosen\"][0],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fbdbd-1cff-401f-8e6c-cd98c134c0f2",
   "metadata": {
    "id": "ac9fbdbd-1cff-401f-8e6c-cd98c134c0f2"
   },
   "source": [
    "- 与指令微调类似，训练过程中传入模型的响应也包含了输入提示。\n",
    "- 另外，我们使用了 `<|endoftext|>` 作为填充token，这样可以将响应扩展到类似的长度，以便将它们堆叠成一个批次。\n",
    "- 不用担心，`<|endoftext|>` token在计算损失时会被忽略，因此不会影响训练结果。\n",
    "- 现在，让我们来看一下对应的拒绝响应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db382be5-c727-4299-8597-c05424ba9308",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db382be5-c727-4299-8597-c05424ba9308",
    "outputId": "edbd8c4a-0528-4361-aeba-9b3c3bbde33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"rejected\"][0],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715dc968-aa64-4388-b577-7c295831bdcf",
   "metadata": {
    "id": "715dc968-aa64-4388-b577-7c295831bdcf"
   },
   "source": [
    "- 在这种情况下，如上所示，拒绝的响应是比选定响应更加不礼貌的版本（我们不希望模型生成不礼貌的回答）。\n",
    "- 最后，我们来讨论一下数据掩码：如果你仔细查看我们上面实现的自定义 collate 函数，你会发现我们为每个数据集条目创建了一个 `\"chosen_mask\"` 和一个 `\"rejected_mask\"`。\n",
    "- 这些掩码与响应条目的形状相同，下面展示的是 `\"chosen\"` 条目的掩码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c324eab-cf1d-4071-b3ba-797d8ec4d1da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c324eab-cf1d-4071-b3ba-797d8ec4d1da",
    "outputId": "742a5742-1bc0-4f74-9eb9-cbf81f936ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen inputs: torch.Size([81])\n",
      "chosen mask:   torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "print(\"chosen inputs:\", batch[\"chosen\"][0].shape)\n",
    "print(\"chosen mask:  \", batch[\"chosen_mask\"][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e95f7-cfc3-4f5f-be5e-c279fba5f674",
   "metadata": {
    "id": "880e95f7-cfc3-4f5f-be5e-c279fba5f674"
   },
   "source": [
    "- 这些掩码的内容是布尔值（`True` 和 `False`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da75b550-5da4-4292-9a7e-a05b842bdcb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da75b550-5da4-4292-9a7e-a05b842bdcb7",
    "outputId": "e5f012c3-33ba-4e6b-aa55-3e331865218f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"chosen_mask\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e67b862-4430-4c99-9157-90955dde29b6",
   "metadata": {
    "id": "0e67b862-4430-4c99-9157-90955dde29b6"
   },
   "source": [
    "- `True` 值表示对应于实际响应的 token ID。\n",
    "- `False` 值表示对应于提示 token（如果我们在 `customized_collate_fn` 函数中设置了 `mask_prompt_tokens=True`，我们之前做过此设置）或填充 token 的 token ID。\n",
    "- 因此，我们可以使用掩码作为选择掩码，只选择对应于响应的 token ID，也就是去掉所有提示和填充 token，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1114c6fe-524b-401c-b9fe-02260e6f0541",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1114c6fe-524b-401c-b9fe-02260e6f0541",
    "outputId": "6d99af1d-940a-4012-c5d9-21d463a66e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"chosen\"][0][batch[\"chosen_mask\"][0]],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a89f83a4-d16e-40d2-ba43-bd410affd967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a89f83a4-d16e-40d2-ba43-bd410affd967",
    "outputId": "1d439c7e-c079-4594-d02a-fa83a3cb275d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"rejected\"][0][batch[\"rejected_mask\"][0]],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525287f-137c-4d71-94ae-cfd6db7b057c",
   "metadata": {
    "id": "e525287f-137c-4d71-94ae-cfd6db7b057c"
   },
   "source": [
    "- 我们将在后续计算DPO损失时使用该掩码，忽略提示符和填充token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jbafhM_R8z5q",
   "metadata": {
    "id": "jbafhM_R8z5q"
   },
   "source": [
    "## 2.4) 创建训练集、验证集和测试集数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c29eb8-d1b9-4abe-a155-52b3270d759a",
   "metadata": {
    "id": "b3c29eb8-d1b9-4abe-a155-52b3270d759a"
   },
   "source": [
    "- 上面我们用了偏好数据集的一个小子集来做示范。\n",
    "- 接下来，我们将创建实际的训练集、验证集和测试集数据加载器。\n",
    "- 这一过程与预训练和指令微调章节中的数据加载器创建方式相同，应该不难理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0068bf-bda0-4d9e-9f79-2fc4b94cbd1c",
   "metadata": {
    "id": "5c0068bf-bda0-4d9e-9f79-2fc4b94cbd1c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = PreferenceDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f4a257b-6835-4194-abe2-5831d6a44885",
   "metadata": {
    "id": "2f4a257b-6835-4194-abe2-5831d6a44885"
   },
   "outputs": [],
   "source": [
    "val_dataset = PreferenceDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = PreferenceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ba19-a6d5-4a77-8283-7a17d7ec06e2",
   "metadata": {
    "id": "1fe1ba19-a6d5-4a77-8283-7a17d7ec06e2"
   },
   "source": [
    "- 让我们遍历数据加载器，查看每个批次的数据形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d61f15-facb-4eb8-a9be-6427887d24b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80d61f15-facb-4eb8-a9be-6427887d24b2",
    "outputId": "dacd3bdf-f069-4b36-da2c-d6c1c6cc5405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 97]) torch.Size([8, 97])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 101]) torch.Size([8, 101])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 100]) torch.Size([8, 100])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 95]) torch.Size([8, 95])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 97]) torch.Size([8, 97])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 104]) torch.Size([8, 104])\n",
      "torch.Size([8, 101]) torch.Size([8, 101])\n",
      "torch.Size([8, 98]) torch.Size([8, 98])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for batch in train_loader:\n",
    "    print(\n",
    "        batch[\"chosen\"].shape,\n",
    "        batch[\"rejected\"].shape,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff958a6-5e61-49f5-9a97-360aa34e3758",
   "metadata": {
    "id": "7ff958a6-5e61-49f5-9a97-360aa34e3758"
   },
   "source": [
    "- 每一行展示了每个批次中 `\"chosen\"` 和 `\"rejected\"` 项目的形状\n",
    "- 由于我们在批次层面上应用了填充，每一行的形状不同\n",
    "- 这样做是出于效率考虑，因为如果将所有样本填充到整个数据集中最长的样本长度，会非常低效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb0543-1142-4374-8825-3384e20c6ac0",
   "metadata": {
    "id": "29cb0543-1142-4374-8825-3384e20c6ac0"
   },
   "source": [
    "&nbsp;\n",
    "# 3) 加载微调后的LLM进行DPO对齐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b08881-b769-4b26-8153-5ec0e8573ed2",
   "metadata": {
    "id": "22b08881-b769-4b26-8153-5ec0e8573ed2"
   },
   "source": [
    "- LLM对齐步骤，如RLHF或DPO，假设我们已经有了一个经过指令微调的模型。\n",
    "- 本节包含加载在第7章中指令微调并保存的模型的最小代码（通过[../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb)）。\n",
    "- 在继续之前，请确保先运行第7章的代码，以创建指令微调的模型。\n",
    "- 以下代码将指令微调的模型复制到当前目录："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3c6d82b-63f7-459a-b901-7125ab225e56",
   "metadata": {
    "id": "b3c6d82b-63f7-459a-b901-7125ab225e56"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "finetuned_model_path = Path(\"gpt2-medium355M-sft.pth\")\n",
    "if not finetuned_model_path.exists():\n",
    "\n",
    "    # 尝试在本地找到模型检查点:\n",
    "    relative_path = Path(\"..\") / \"01_main-chapter-code\" / finetuned_model_path\n",
    "    if relative_path.exists():\n",
    "        shutil.copy(relative_path, \".\")\n",
    "\n",
    "    # 如果此笔记本在 Google Colab 上运行，则从 Google Drive 文件夹中获取\n",
    "    elif \"COLAB_GPU\" in os.environ or \"COLAB_TPU_ADDR\" in os.environ:\n",
    "        from google.colab import drive\n",
    "        drive.mount(\"/content/drive\")\n",
    "        google_drive_path = \"/content/drive/My Drive/Books/LLMs-From-Scratch/ch07/colab/gpt2-medium355M-sft.pth\"  # 读者需要调整此路径\n",
    "        shutil.copy(google_drive_path, \".\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Could not find '{finetuned_model_path}'.\\n\"\n",
    "            \"Run the `ch07.ipynb` notebook to finetune and save the finetuned model.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8585e-4569-4033-84a7-3903d0e8aaf8",
   "metadata": {
    "id": "71c8585e-4569-4033-84a7-3903d0e8aaf8"
   },
   "source": [
    "- 接下来，我们重用前几章中的基本配置来加载模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8333fee-e7fe-4f8c-9411-8c1db6252d98",
   "metadata": {
    "id": "a8333fee-e7fe-4f8c-9411-8c1db6252d98"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout率\n",
    "    \"qkv_bias\": True         # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2821403-605c-4071-a4ff-e23f4c9a11fd",
   "metadata": {
    "id": "c2821403-605c-4071-a4ff-e23f4c9a11fd"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"gpt2-medium355M-sft.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True\n",
    "    )\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61863bec-bd42-4194-b994-645bfe2df8be",
   "metadata": {
    "id": "61863bec-bd42-4194-b994-645bfe2df8be"
   },
   "source": [
    "- 在使用 DPO 训练加载的模型之前，让我们通过在一些样本数据上测试，确保微调后的模型已正确保存和加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4357aec5-0db2-4d73-b37b-539cd8fa80a3",
   "metadata": {
    "id": "4357aec5-0db2-4d73-b37b-539cd8fa80a3"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "541e7988-38d3-47f6-bd52-9da6564479fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "541e7988-38d3-47f6-bd52-9da6564479fa",
    "outputId": "278f7ddf-37c2-4c3a-d069-c510ef6f8d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response\n",
      "that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "response = token_ids_to_text(token_ids, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87ed19-fded-4e56-8585-6c7c0367b354",
   "metadata": {
    "id": "be87ed19-fded-4e56-8585-6c7c0367b354"
   },
   "source": [
    "- 如上所示，模型给出了一个合理且正确的回应。\n",
    "- 如第七章所解释的，实际上，我们会对响应进行清理，只返回响应文本，并去除提示和提示样式（这类似于你在 ChatGPT 中熟悉的方式）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c30c4e2-af84-4ab4-95d0-9641e32c1e7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c30c4e2-af84-4ab4-95d0-9641e32c1e7f",
    "outputId": "70192bbe-fdf6-43eb-c673-f573f8c70156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "def extract_response(response_text, input_text):\n",
    "    return response_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "response = extract_response(response, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442cb9-83b1-46b8-bad0-7d44297ca52d",
   "metadata": {
    "id": "80442cb9-83b1-46b8-bad0-7d44297ca52d"
   },
   "source": [
    "- 现在，我们已经快实现 DPO 部分了。\n",
    "- 正如在本笔记本开头所提到的，DPO 需要使用两个 LLM：一个是策略模型（即我们希望优化的模型），另一个是参考模型（即保持不变的原始模型）。\n",
    "- 在接下来的代码中，我们将 `model` 重命名为 `policy_model`，并实例化第二个模型，称之为 `reference_model`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d88cc3a-312e-4b29-bc6d-de8354c1eb9f",
   "metadata": {
    "id": "5d88cc3a-312e-4b29-bc6d-de8354c1eb9f"
   },
   "outputs": [],
   "source": [
    "policy_model = model\n",
    "\n",
    "reference_model = GPTModel(BASE_CONFIG)\n",
    "reference_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"gpt2-medium355M-sft.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True\n",
    "    )\n",
    ")\n",
    "reference_model.eval()\n",
    "\n",
    "policy_model.to(device)\n",
    "reference_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1469-0038-4914-8aa5-15b1f81877cc",
   "metadata": {
    "id": "9c6c1469-0038-4914-8aa5-15b1f81877cc"
   },
   "source": [
    "&nbsp;\n",
    "# 4) 编写 DPO 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbe60c-e4ce-413e-beec-22eff0237d11",
   "metadata": {
    "id": "75dbe60c-e4ce-413e-beec-22eff0237d11"
   },
   "source": [
    "- 在前面的章节中，我们已经完成了模型加载和数据集准备，现在我们可以进入更有趣的部分，开始编写 DPO 损失函数。\n",
    "- 请注意，下面的 DPO 损失函数代码是基于论文 [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) 中提出的方法。\n",
    "- 作为参考，下面再次展示了核心的 DPO 方程：\n",
    "\n",
    "<img src=\"../image/3.webp\" width=800px>\n",
    "\n",
    "- 在上面的方程中，\n",
    "  - “期望值” $\\mathbb{E}$ 是统计学术语，表示随机变量（括号内的表达式）的平均值或均值；优化 $-\\mathbb{E}$ 可以更好地使模型与用户偏好对齐。\n",
    "  - $\\pi_{\\theta}$ 变量是所谓的策略（这是从强化学习中借用的术语），表示我们希望优化的语言模型（LLM）；$\\pi_{ref}$ 是参考 LLM，通常是优化前的原始 LLM（在训练开始时，$\\pi_{\\theta}$ 和 $\\pi_{ref}$ 通常是相同的）。\n",
    "  - $\\beta$ 是一个超参数，用于控制 $\\pi_{\\theta}$ 和参考模型之间的差异；增大 $\\beta$ 会增加两者在对数概率上的差异对整体损失函数的影响，从而增大两者模型之间的分歧。\n",
    "  - 对数 sigmoid 函数 $\\sigma(\\centerdot)$ 将首选响应和拒绝响应的对数比率（对数几率函数内的项）转换为概率得分。\n",
    "- 在代码中，我们可以按以下方式实现 DPO 损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38CsrrwJIZiV",
   "metadata": {
    "id": "38CsrrwJIZiV"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "      model_chosen_logprobs,\n",
    "      model_rejected_logprobs,\n",
    "      reference_chosen_logprobs,\n",
    "      reference_rejected_logprobs,\n",
    "      beta=0.1,\n",
    "    ):\n",
    "    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "    Args:\n",
    "        policy_chosen_logprobs: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "        policy_rejected_logprobs: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "        reference_chosen_logprobs: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "        reference_rejected_logprobs: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "        beta: Temperature parameter for the DPO loss; typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n",
    "        label_smoothing: conservativeness for DPO loss.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of three tensors: (loss, chosen_rewards, rejected_rewards).\n",
    "    \"\"\"\n",
    "\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    logits = model_logratios - reference_logratios\n",
    "\n",
    "    # DPO（参见 https://arxiv.org/pdf/2305.18290.pdf 中的公式 7）\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "\n",
    "    # 可选值，用于在训练期间跟踪进度\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    # 使用 .mean() 对批次中的样本进行平均\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be65b-38fc-4d18-bf53-a260a15436e1",
   "metadata": {
    "id": "693be65b-38fc-4d18-bf53-a260a15436e1"
   },
   "source": [
    "- 如果你熟悉对数运算，你会注意到我们有一个通用关系 $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$，我们在上面的代码中应用了这个公式。\n",
    "- 牢记这一点，让我们逐步解析一些步骤（稍后我们将使用一个单独的函数来计算 `logprobs`）。\n",
    "- 让我们从以下几行代码开始：\n",
    "\n",
    "    ```python\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    ```\n",
    "\n",
    "- 上面的代码行计算了在政策模型和参考模型中，选择样本和拒绝样本的对数概率（logits）的差异（这就是 $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$ 的应用）：\n",
    "\n",
    "$$\\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_\\theta (y_l \\mid x)} \\right) \\quad \\text{和} \\quad \\log \\left( \\frac{\\pi_{\\text{ref}}(y_w \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right)$$\n",
    "\n",
    "- 接下来，代码 `logits = model_logratios - reference_logratios` 计算了政策模型的对数比率与参考模型的对数比率之间的差异，即：\n",
    "\n",
    "$$\\beta \\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_{\\text{ref}} (y_w \\mid x)} \\right)\n",
    "- \\beta \\log \\left( \\frac{\\pi_\\theta (y_l \\mid x)}{\\pi_{\\text{ref}} (y_l \\mid x)} \\right)$$\n",
    "\n",
    "- 最后，`losses = -F.logsigmoid(beta * logits)` 使用对数- sigmoid 函数计算损失；在原始方程中，期望值内部的项是：\n",
    "\n",
    "$$\\log \\sigma \\left( \\beta \\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_{\\text{ref}} (y_w \\mid x)} \\right)\n",
    "- \\beta \\log \\left( \\frac{\\pi_\\theta (y_l \\mid x)}{\\pi_{\\text{ref}} (y_l \\mid x)} \\right) \\right)$$\n",
    "\n",
    "- 上面我们假设对数概率已经计算出来了；现在让我们定义一个 `compute_logprobs` 函数，用来计算传递给 `compute_dpo_loss` 函数中的对数概率，即值 $\\pi_\\theta (y_w \\mid x)$、$\\pi_\\theta (y_l \\mid x)$ 等等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9",
   "metadata": {
    "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9"
   },
   "outputs": [],
   "source": [
    "def compute_logprobs(logits, labels, selection_mask=None):\n",
    "    \"\"\"\n",
    "    Compute log probabilities.\n",
    "\n",
    "    Args:\n",
    "      logits: Tensor of shape (batch_size, num_tokens, vocab_size)\n",
    "      labels: Tensor of shape (batch_size, num_tokens)\n",
    "      selection_mask: Tensor for shape (batch_size, num_tokens)\n",
    "\n",
    "    Returns:\n",
    "      mean_log_prob: Mean log probability excluding padding tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # 标签是输入向右移动一位\n",
    "    labels = labels[:, 1:].clone()\n",
    "\n",
    "    # 截断 Logits 以匹配标签的token数量\n",
    "    logits = logits[:, :-1, :]\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 收集实际标签的对数概率\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    if selection_mask is not None:\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "\n",
    "        # 应用掩码以过滤掉填充token\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "\n",
    "        # 计算排除填充token的平均对数概率\n",
    "        # 这是在token上取平均，因此形状为 (batch_size, num_tokens)\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "\n",
    "        return avg_log_prob\n",
    "\n",
    "    else:\n",
    "        return selected_log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7",
   "metadata": {
    "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7"
   },
   "source": [
    "- 请注意，以上这个函数由于包含了 `torch.gather` 函数，刚开始可能看起来有点让人畏惧，但它与 PyTorch 的 `cross_entropy` 函数内部的实现非常相似。\n",
    "- 例如，考虑以下示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
    "outputId": "8f7b47d4-73fe-4605-c17d-ad6cfd909a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4185) tensor(1.4185)\n"
     ]
    }
   ],
   "source": [
    "# 示例数据\n",
    "logits = torch.tensor(\n",
    "    [[2.0, 1.0, 0.1],\n",
    "     [0.5, 2.5, 0.3]])  # 形状: (2, 3)\n",
    "targets = torch.tensor([0, 2])  # 形状: (2,)\n",
    "\n",
    "\n",
    "# 使用 torch.gather 手动计算损失\n",
    "log_softmax_logits = F.log_softmax(logits, dim=1)  # 形状: (2, 3)\n",
    "selected_log_probs = torch.gather(\n",
    "    input=log_softmax_logits,\n",
    "    dim=1,\n",
    "    index=targets.unsqueeze(1), # 形状: (2, 1)\n",
    ").squeeze(1)  # 形状: (2,)\n",
    "manual_loss = -selected_log_probs.mean()  # 在批次上取平均\n",
    "\n",
    "\n",
    "# PyTorch 损失函数\n",
    "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "print(manual_loss, cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7",
   "metadata": {
    "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7"
   },
   "source": [
    "- 如上所示，我们可以看到这两种实现是等效的，但让我们进一步聚焦于 `torch.gather` 的工作原理。\n",
    "- 现在考虑以下两个张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "508db6ba-cc40-479f-a996-2250cf862388",
   "metadata": {
    "id": "508db6ba-cc40-479f-a996-2250cf862388"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "  [[1., 2.,],\n",
    "   [3., 4.]]\n",
    ")\n",
    "\n",
    "m = torch.tensor(\n",
    "  [[1, 1],\n",
    "   [0, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979",
   "metadata": {
    "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979"
   },
   "source": [
    "- 上述中，`t` 是我们想要选择的张量，`m` 是一个掩码，用于指定我们如何选择。\n",
    "  - 例如，由于 `m` 的第一行包含 `[1, 1]`，它将两次选择 `t` 中索引位置为 `1` 的值，即值为 `2`。\n",
    "  - `m` 的第二行 `[0, 1]` 选择了 `t` 第二行中索引位置为 `0` 和 `1` 的值，即 `3.` 和 `4.`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fdN5q1YPAbM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fdN5q1YPAbM",
    "outputId": "e935e8ad-1519-4c4b-dbff-65adae0a15a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(input=t, dim=-1, index=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3",
   "metadata": {
    "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3"
   },
   "source": [
    "- 换句话说，`torch.gather` 是一个选择函数。\n",
    "- 当我们之前计算损失时，我们使用它来获取与正确 token 对应的对数概率，这些 token 来自 50,256 个词汇中的正确选项。\n",
    "- “正确” tokens 是响应条目中给出的 tokens。\n",
    "\n",
    "- 关于上面的 `compute_logprobs` 函数，我们在这里使用 `torch.gather`，因为它比 `cross_entropy` 提供了更多的控制，但本质上思想相似。\n",
    "- 我们使用的 `selection_mask` 是为了可选地忽略提示和填充 tokens。\n",
    "- 然后，我们可以如下使用 `compute_logprobs` 函数来计算 `compute_dpo_loss` 损失函数的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318",
   "metadata": {
    "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318"
   },
   "outputs": [],
   "source": [
    "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
    "    \"\"\"Compute the DPO loss on an input batch\"\"\"\n",
    "\n",
    "    # 其中 policy_model(batch[\"chosen\"]) 是 logits\n",
    "    policy_chosen_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"chosen\"]),\n",
    "        labels=batch[\"chosen\"],\n",
    "        selection_mask=batch[\"chosen_mask\"]\n",
    "    )\n",
    "    policy_rejected_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"rejected\"]),\n",
    "        labels=batch[\"rejected\"],\n",
    "        selection_mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ref_chosen_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"chosen\"]),\n",
    "            labels=batch[\"chosen\"],\n",
    "            selection_mask=batch[\"chosen_mask\"]\n",
    "        )\n",
    "        ref_rejected_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"rejected\"]),\n",
    "            labels=batch[\"rejected\"],\n",
    "            selection_mask=batch[\"rejected_mask\"]\n",
    "        )\n",
    "    loss, chosen_rewards, rejected_rewards = compute_dpo_loss(\n",
    "        model_chosen_logprobs=policy_chosen_log_probas,\n",
    "        model_rejected_logprobs=policy_rejected_log_probas,\n",
    "        reference_chosen_logprobs=ref_chosen_log_probas,\n",
    "        reference_rejected_logprobs=ref_rejected_log_probas,\n",
    "        beta=beta\n",
    "    )\n",
    "    return loss, chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28caafb-f378-4332-a142-3e0f9ef67fbb",
   "metadata": {
    "id": "b28caafb-f378-4332-a142-3e0f9ef67fbb"
   },
   "source": [
    "- 上述函数适用于单个批次，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
    "outputId": "65a70828-7dd2-4f72-ffec-45aeaf8afad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.6931, device='cuda:0'), tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = compute_dpo_loss_batch(batch, policy_model, reference_model, beta=0.1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17429cd-2a00-41c8-9f16-38b1c9a5179f",
   "metadata": {
    "id": "b17429cd-2a00-41c8-9f16-38b1c9a5179f"
   },
   "source": [
    "- 我们扩展了这个函数，使其能够在数据加载器中处理指定的 `num_batches`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302",
   "metadata": {
    "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302"
   },
   "outputs": [],
   "source": [
    "def compute_dpo_loss_loader(data_loader, policy_model, reference_model, beta, num_batches=None):\n",
    "    \"\"\"Apply compute_dpo_loss_batch to a whole data loader\"\"\"\n",
    "\n",
    "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        \n",
    "        # 如果指定的批次数量超过了数据加载器中的批次数量，则减少批次数量以匹配数据加载器中的总批次数量\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            total_chosen_rewards += chosen_rewards.item()\n",
    "            total_rejected_rewards += rejected_rewards.item()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 计算平均值\n",
    "    total_loss /= num_batches\n",
    "    total_chosen_rewards /= num_batches\n",
    "    total_rejected_rewards /= num_batches\n",
    "    return total_loss, total_chosen_rewards, total_rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e4c09-d285-44d5-be12-d29769950cb6",
   "metadata": {
    "id": "852e4c09-d285-44d5-be12-d29769950cb6"
   },
   "source": [
    "- 为什么要指定 `num_batches`？这是出于效率考虑（因为每次计算整个数据集的损失会显著减慢训练速度）。\n",
    "\n",
    "- 最后，我们定义了一个便捷函数供后续训练函数使用；这个 `evaluate_dpo_loss_loader` 函数计算训练集和验证集数据加载器的 DPO 损失和奖励，以便进行日志记录："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9",
   "metadata": {
    "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9"
   },
   "outputs": [],
   "source": [
    "def evaluate_dpo_loss_loader(policy_model, reference_model, train_loader, val_loader, beta, eval_iter):\n",
    "    \"\"\"Compute the DPO loss for the training and validation dataset\"\"\"\n",
    "\n",
    "    policy_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_chosen_rewards, train_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=train_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss, val_chosen_rewards, val_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=val_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    res = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_chosen_reward\": train_chosen_rewards,\n",
    "        \"train_rejected_reward\": train_rejected_rewards,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_chosen_reward\": val_chosen_rewards,\n",
    "        \"val_rejected_reward\": val_rejected_rewards\n",
    "    }\n",
    "\n",
    "    policy_model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95ed92-6743-4f13-8b91-0fbf2e540de1",
   "metadata": {
    "id": "6e95ed92-6743-4f13-8b91-0fbf2e540de1"
   },
   "source": [
    "- 在本节中，我们涵盖了很多内容，简要回顾如下：\n",
    "  - 流程是：通过模型计算 `logits` → 从 logits 计算 `compute_logprobs` → 从对数概率计算 `compute_dpo_loss`\n",
    "  - 我们有 `compute_dpo_loss_batch` 函数，简化了上述过程\n",
    "  - `compute_dpo_loss_loader` 工具函数将 `compute_dpo_loss_batch` 函数应用于数据加载器\n",
    "  - `evaluate_dpo_loss_loader` 函数将 `compute_dpo_loss_batch` 应用于训练集和验证集的数据加载器，以便进行日志记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157",
   "metadata": {
    "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157"
   },
   "source": [
    "&nbsp;\n",
    "# 5) 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c",
   "metadata": {
    "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c"
   },
   "source": [
    "- 在上一节设置了 DPO 损失函数后，我们现在可以开始训练模型了。\n",
    "- 请注意，这个训练函数与我们在预训练和指令微调时使用的函数相同，唯一的区别是：\n",
    "  - 我们将交叉熵损失替换为新的 DPO 损失函数\n",
    "  - 我们还跟踪奖励值和奖励边际，这些通常用于 RLHF 和 DPO 环境中，以便跟踪训练进展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f90d9325-77b2-417f-88ff-0a5174889413",
   "metadata": {
    "id": "f90d9325-77b2-417f-88ff-0a5174889413"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import generate_and_print_sample\n",
    "\n",
    "\n",
    "def train_model_dpo_simple(\n",
    "    policy_model, reference_model, train_loader, val_loader,\n",
    "    optimizer, num_epochs, beta,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "\n",
    "    # 初始化列表以跟踪损失和已处理的token\n",
    "    tracking = {\n",
    "        \"train_losses\": [],\n",
    "        \"train_chosen_rewards\": [],\n",
    "        \"train_rejected_rewards\": [],\n",
    "        \"val_losses\": [],\n",
    "        \"val_chosen_rewards\": [],\n",
    "        \"val_rejected_rewards\": [],\n",
    "        \"tokens_seen\": []\n",
    "    }\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 主训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()  # 将模型设置为训练模式\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()  # 重置上一批次的损失梯度\n",
    "\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "\n",
    "            loss.backward()  # 计算损失梯度\n",
    "            optimizer.step()  # 使用损失梯度更新模型权重\n",
    "\n",
    "            tokens_seen += batch[\"chosen\"].numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 可选的评估步骤\n",
    "            if global_step % eval_freq == 0:\n",
    "                res = evaluate_dpo_loss_loader(\n",
    "                    policy_model=policy_model,\n",
    "                    reference_model=reference_model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    beta=beta,\n",
    "                    eval_iter=eval_iter\n",
    "                )\n",
    "                tracking[\"train_losses\"].append(res[\"train_loss\"])\n",
    "                tracking[\"train_chosen_rewards\"].append(res[\"train_chosen_reward\"])\n",
    "                tracking[\"train_rejected_rewards\"].append(res[\"train_rejected_reward\"])\n",
    "                tracking[\"val_losses\"].append(res[\"val_loss\"])\n",
    "                tracking[\"val_chosen_rewards\"].append(res[\"val_chosen_reward\"])\n",
    "                tracking[\"val_rejected_rewards\"].append(res[\"val_rejected_reward\"])\n",
    "                tracking[\"tokens_seen\"].append(tokens_seen)\n",
    "                train_reward_margin = res[\"train_chosen_reward\"] - res[\"train_rejected_reward\"]\n",
    "                val_reward_margin = res[\"val_chosen_reward\"] - res[\"val_rejected_reward\"]\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {res['train_loss']:.3f}, Val loss {res['val_loss']:.3f}, \"\n",
    "                    f\"Train reward margins {train_reward_margin:.3f}, \"\n",
    "                    f\"Val reward margins {val_reward_margin:.3f}\"\n",
    "                )\n",
    "\n",
    "        # 在每个训练周期后打印示例文本\n",
    "        generate_and_print_sample(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=loss.device,\n",
    "            start_context=start_context\n",
    "        )\n",
    "\n",
    "    return tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d4904-f819-4d62-bfb4-85cf28863683",
   "metadata": {
    "id": "820d4904-f819-4d62-bfb4-85cf28863683"
   },
   "source": [
    "- 在开始训练之前，让我们先打印初始的损失和reward："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d53210c5-6d9c-46b0-af22-ee875c2806c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d53210c5-6d9c-46b0-af22-ee875c2806c5",
    "outputId": "8b1d2b39-16c5-4b99-e920-5b33d3c0f34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6931471824645996\n",
      "Validation loss: 0.6931471824645996\n",
      "Train reward margin: 0.0\n",
      "Val reward margin: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # 由于数据加载器中的随机打乱，为了可重复性，设置随机种子\n",
    "\n",
    "res = evaluate_dpo_loss_loader(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    beta=0.1,\n",
    "    eval_iter=5\n",
    ")\n",
    "\n",
    "print(\"Training loss:\", res[\"train_loss\"])\n",
    "print(\"Validation loss:\", res[\"val_loss\"])\n",
    "\n",
    "print(\"Train reward margin:\", res[\"train_chosen_reward\"] - res[\"train_rejected_reward\"])\n",
    "print(\"Val reward margin:\", res[\"val_chosen_reward\"] - res[\"val_rejected_reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a006e91-df94-43ca-8025-1ba791e37bc4",
   "metadata": {
    "id": "4a006e91-df94-43ca-8025-1ba791e37bc4"
   },
   "source": [
    "- 用三个例子测试下模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "q4Ro9DrBa7zH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Ro9DrBa7zH",
    "outputId": "b974d4bd-b92a-4a2a-bb7a-5a2a0d1eca11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "Correct response:\n",
      ">> The meal is cooked by the chef every day.\n",
      "\n",
      "Model response:\n",
      ">> The meal is cooked every day by the chef.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify an input string as either a noun or a verb.\n",
      "\n",
      "### Input:\n",
      "Dance\n",
      "\n",
      "Correct response:\n",
      ">> 'Dance' can be classified as a verb.\n",
      "\n",
      "Model response:\n",
      ">> \"Dance\" can be classified as a verb.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a metaphor.\n",
      "\n",
      "### Input:\n",
      "The book is very interesting.\n",
      "\n",
      "Correct response:\n",
      ">> The book is a page-turner.\n",
      "\n",
      "Model response:\n",
      ">> The book is a treat.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in val_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2386ae-5c4c-448e-bfbf-4ec0604b171e",
   "metadata": {
    "id": "ac2386ae-5c4c-448e-bfbf-4ec0604b171e"
   },
   "source": [
    "- **上方展示了原始模型的响应**。  \n",
    "- **DPO 训练的目标** 是 **引导模型进行轻微的风格调整**，  \n",
    "  例如 **生成与原始响应相似但更礼貌的回答**。  \n",
    "\n",
    "- **在正式启动训练之前**，请注意以下几点设置：\n",
    "  - **我们仅优化政策模型（Policy Model）**，  \n",
    "    因此只将其参数传递给 `AdamW` 优化器；  \n",
    "    **参考模型（Reference Model）不参与训练**。  \n",
    "  - **训练仅进行 1 个 Epoch**，  \n",
    "    **因为 DPO 训练容易“塌陷”（Collapse）**，  \n",
    "    **损失可能继续优化，但模型可能开始生成无意义的文本**。  \n",
    "  - **DPO 训练时，建议使用较小的学习率（Learning Rate）**。  \n",
    "  - **β（Beta）值** 可从 **0.1 增加至 0.5**，  \n",
    "    **以减少 DPO 训练的影响**；  \n",
    "    **我们这里使用 0.1，以确保调整效果更明显**。  \n",
    "  - **训练时长**：\n",
    "    - **在 A100 GPU 上约需 2 分钟**。  \n",
    "    - **在 L4 GPU 上约需 4 分钟**。  \n",
    "    - **在 M3 MacBook Air 上约需 30 分钟**。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
    "outputId": "d98b08b0-c325-411e-a1a4-05e7403f0345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.692, Val loss 0.693, Train reward margins 0.019, Val reward margins 0.009\n",
      "Ep 1 (Step 000005): Train loss 0.690, Val loss 0.691, Train reward margins 0.070, Val reward margins 0.052\n",
      "Ep 1 (Step 000010): Train loss 0.687, Val loss 0.688, Train reward margins 0.126, Val reward margins 0.108\n",
      "Ep 1 (Step 000015): Train loss 0.676, Val loss 0.685, Train reward margins 0.362, Val reward margins 0.173\n",
      "Ep 1 (Step 000020): Train loss 0.676, Val loss 0.680, Train reward margins 0.351, Val reward margins 0.264\n",
      "Ep 1 (Step 000025): Train loss 0.666, Val loss 0.676, Train reward margins 0.564, Val reward margins 0.359\n",
      "Ep 1 (Step 000030): Train loss 0.672, Val loss 0.672, Train reward margins 0.456, Val reward margins 0.441\n",
      "Ep 1 (Step 000035): Train loss 0.663, Val loss 0.669, Train reward margins 0.658, Val reward margins 0.511\n",
      "Ep 1 (Step 000040): Train loss 0.666, Val loss 0.666, Train reward margins 0.597, Val reward margins 0.574\n",
      "Ep 1 (Step 000045): Train loss 0.648, Val loss 0.662, Train reward margins 0.982, Val reward margins 0.660\n",
      "Ep 1 (Step 000050): Train loss 0.648, Val loss 0.659, Train reward margins 0.993, Val reward margins 0.734\n",
      "Ep 1 (Step 000055): Train loss 0.647, Val loss 0.656, Train reward margins 1.014, Val reward margins 0.799\n",
      "Ep 1 (Step 000060): Train loss 0.652, Val loss 0.653, Train reward margins 0.893, Val reward margins 0.870\n",
      "Ep 1 (Step 000065): Train loss 0.631, Val loss 0.650, Train reward margins 1.361, Val reward margins 0.948\n",
      "Ep 1 (Step 000070): Train loss 0.618, Val loss 0.646, Train reward margins 1.699, Val reward margins 1.038\n",
      "Ep 1 (Step 000075): Train loss 0.617, Val loss 0.642, Train reward margins 1.733, Val reward margins 1.121\n",
      "Ep 1 (Step 000080): Train loss 0.592, Val loss 0.639, Train reward margins 2.333, Val reward margins 1.194\n",
      "Ep 1 (Step 000085): Train loss 0.610, Val loss 0.636, Train reward margins 1.907, Val reward margins 1.275\n",
      "Ep 1 (Step 000090): Train loss 0.650, Val loss 0.633, Train reward margins 0.964, Val reward margins 1.353\n",
      "Ep 1 (Step 000095): Train loss 0.607, Val loss 0.630, Train reward margins 1.962, Val reward margins 1.423\n",
      "Ep 1 (Step 000100): Train loss 0.600, Val loss 0.627, Train reward margins 2.127, Val reward margins 1.500\n",
      "Ep 1 (Step 000105): Train loss 0.590, Val loss 0.624, Train reward margins 2.458, Val reward margins 1.564\n",
      "Ep 1 (Step 000110): Train loss 0.607, Val loss 0.622, Train reward margins 1.976, Val reward margins 1.621\n",
      "Ep 1 (Step 000115): Train loss 0.621, Val loss 0.620, Train reward margins 1.605, Val reward margins 1.682\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a metaphor.  ### Input: The book is very interesting.  ### Response: The book is a treat.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The assignment was written by the student.  ### Response\n",
      "Training completed in 1.69 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "tracking = train_model_dpo_simple(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    beta=0.1, # 取值在0.1到0.5之间\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[2]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8ea88-8771-4eb9-855d-2fe1ca2dc2fa",
   "metadata": {
    "id": "eba8ea88-8771-4eb9-855d-2fe1ca2dc2fa"
   },
   "source": [
    "- **从上面的跟踪结果可以看出，损失（Loss）逐步优化**。  \n",
    "- **同时，奖励边际（Reward Margins）** —— 即 **偏好响应（Chosen）与非偏好响应（Rejected）奖励之间的差距**，  \n",
    "  也在 **逐步提升**，这是一个 **积极的信号**。  \n",
    "- **接下来，我们更具体地分析这些结果**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e23989-92bd-4ac2-a4bc-65d4c7ac334e",
   "metadata": {
    "id": "11e23989-92bd-4ac2-a4bc-65d4c7ac334e"
   },
   "source": [
    "&nbsp;\n",
    "# 6) 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7d5fe-c617-45cb-8ea9-ddc7baa22654",
   "metadata": {
    "id": "66d7d5fe-c617-45cb-8ea9-ddc7baa22654"
   },
   "source": [
    "- **首先，我们通过绘制 DPO 损失曲线（Loss Curve）来分析训练结果：**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ddcc66f-cd7c-4f46-96ea-af919ea1a199",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "8ddcc66f-cd7c-4f46-96ea-af919ea1a199",
    "outputId": "c7164b26-8d32-41d1-8c6a-ab835d58d4c5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABs/klEQVR4nO3deVhU5dvA8e/MsO8gsimLKCpuiCCIqGXillmapZmVWtkvxS1b1NdS27TSzErTtFIrTcvSzH3Jpdz3FXFBBRdARXZZ57x/jAySqIDADHh/rutcwpnnnHM/jHDPec6zqBRFURBCCCGEUVIbOgAhhBBC3J0kaiGEEMKISaIWQgghjJgkaiGEEMKISaIWQgghjJgkaiGEEMKISaIWQgghjJgkaiGEEMKISaIWQgghjJgkaiGqsfPnz6NSqTh06JChQxFClJEkaiGMnEqluuc2ceJEQ4cohKhAJoYOQAhxb1euXNF/vWTJEsaPH090dLR+n42NjSHCEkJUErmjFsLIubm56Td7e3tUKpX+excXF6ZNm0bt2rUxNzenefPmrF279q7nys/P5+WXX6Zhw4bExsYC8Oeff9KiRQssLCzw9fXl/fffJy8vT3+MSqXiu+++o2fPnlhZWeHn58eKFSv0r9+4cYN+/fpRs2ZNLC0t8fPzY968eXeNYenSpTRt2hRLS0tq1KhBREQEGRkZ+te/++47/P39sbCwoGHDhnzzzTdFjo+Li6N37944ODjg5OTEU089xfnz5/WvDxgwgB49ejB16lTc3d2pUaMGkZGR5ObmlvhnLoRRUYQQVca8efMUe3t7/ffTpk1T7OzslF9++UU5efKk8s477yimpqbKqVOnFEVRlHPnzimAcvDgQSUrK0vp2bOnEhgYqCQmJiqKoijbtm1T7OzslPnz5ytnz55V1q9fr/j4+CgTJ07UXwNQateurSxatEg5ffq0Mnz4cMXGxka5fv26oiiKEhkZqTRv3lzZu3evcu7cOWXDhg3KihUrio3/8uXLiomJiTJt2jTl3LlzypEjR5SZM2cqaWlpiqIoys8//6y4u7srv//+uxITE6P8/vvvipOTkzJ//nxFURQlJydH8ff3V15++WXlyJEjyokTJ5Tnn39eadCggZKdna0oiqL0799fsbOzU15//XUlKipK+euvvxQrKytlzpw55ftmCFFJJFELUYX8N1F7eHgoH3/8cZEyLVu2VIYMGaIoSmGi/ueff5QOHToobdq0UZKTk/VlO3TooEyaNKnI8T/99JPi7u6u/x5Q3n33Xf336enpCqCsWbNGURRF6d69uzJw4MASxb9//34FUM6fP1/s63Xr1lUWLVpUZN+HH36ohIWF6WNr0KCBotVq9a9nZ2crlpaWyrp16xRF0SVqb29vJS8vT1/m2WefVfr06VOiGIUwNvKMWogqKjU1lcuXLxMeHl5kf3h4OIcPHy6yr2/fvtSuXZu///4bS0tL/f7Dhw+zfft2Pv74Y/2+/Px8srKyyMzMxMrKCoBmzZrpX7e2tsbOzo7ExEQABg8eTK9evThw4ACdOnWiR48etG7dutiYAwIC6NChA02bNqVz58506tSJZ555BkdHRzIyMjh79iyvvPIKgwYN0h+Tl5eHvb29Pt4zZ85ga2tb5LxZWVmcPXtW/33jxo3RaDT6793d3Tl69Og9fppCGC9J1EI8BB5//HF+/vlndu7cyWOPPabfn56ezvvvv8/TTz99xzEWFhb6r01NTYu8plKp0Gq1AHTt2pULFy6wevVqNmzYQIcOHYiMjGTq1Kl3nFOj0bBhwwZ27NjB+vXr+frrrxk3bhy7d+/WfyiYO3cuoaGhdxxXEG9QUBALFy6849w1a9YsUbxCVDWSqIWoouzs7PDw8GD79u088sgj+v3bt28nJCSkSNnBgwfTpEkTnnzySVatWqUv36JFC6Kjo6lXr94DxVKzZk369+9P//79adu2LW+//XaxiRp0STM8PJzw8HDGjx+Pt7c3y5YtY9SoUXh4eBATE0O/fv2KPbZFixYsWbIEFxcX7OzsHihmIaoKSdRCVGFvv/02EyZMoG7dujRv3px58+Zx6NChYu84hw0bRn5+Pk888QRr1qyhTZs2jB8/nieeeAIvLy+eeeYZ1Go1hw8f5tixY3z00UclimH8+PEEBQXRuHFjsrOzWblyJf7+/sWW3b17N5s2baJTp064uLiwe/durl69qi///vvvM3z4cOzt7enSpQvZ2dns27ePGzduMGrUKPr168eUKVN46qmn+OCDD6hduzYXLlzgjz/+4J133qF27dpl/2EKYaQkUQtRhQ0fPpyUlBTefPNNEhMTadSoEStWrMDPz6/Y8iNHjkSr1fL444+zdu1aOnfuzMqVK/nggw/49NNPMTU1pWHDhrz66qsljsHMzIyxY8dy/vx5LC0tadu2LYsXLy62rJ2dHdu2bWP69Omkpqbi7e3N559/TteuXQF49dVXsbKyYsqUKbz99ttYW1vTtGlTRo4cCYCVlRXbtm1j9OjRPP3006SlpVGrVi06dOggd9ii2lIpiqIYOgghhBBCFE8mPBFCCCGMmCRqIYQQwohJohZCCCGMmCRqIYQQwohJohZCCCGMmCRqIYQQwohJoi5nM2fOxMfHBwsLC0JDQ9mzZ0+lXn/btm10794dDw8PVCoVy5cvL/K6oiiMHz8ed3d3LC0tiYiI4PTp00XKJCUl0a9fP+zs7HBwcOCVV14hPT29SJkjR47Qtm1bLCws8PT05LPPPrsjlt9++42GDRtiYWFB06ZNWb16dYnrMXnyZFq2bImtrS0uLi706NGjyBrMoJvfOTIykho1amBjY0OvXr1ISEgoUiY2NpZu3bphZWWFi4sLb7/9dpElHAG2bNlCixYtMDc3p169esyfP/+OeMr6vs6aNYtmzZphZ2eHnZ0dYWFhrFmzpkrVoTiffPIJKpVKP765KtVl4sSJqFSqIlvDhg2rXD0ALl26xAsvvECNGjWwtLSkadOm7Nu3T/96Vfl99/HxueM9UalUREZGAlXrPakQhl0TpHpZvHixYmZmpvzwww/K8ePHlUGDBikODg5KQkJCpcWwevVqZdy4ccoff/yhAMqyZcuKvP7JJ58o9vb2yvLly5XDhw8rTz75pFKnTh3l5s2b+jJdunRRAgIClF27din//POPUq9ePaVv377611NSUhRXV1elX79+yrFjx5RffvlFsbS0VL799lt9me3btysajUb57LPPlBMnTijvvvuuYmpqqhw9erRE9ejcubMyb9485dixY8qhQ4eUxx9/XPHy8lLS09P1ZV5//XXF09NT2bRpk7Jv3z6lVatWSuvWrfWv5+XlKU2aNFEiIiKUgwcPKqtXr1acnZ2VsWPH6svExMQoVlZWyqhRo5QTJ04oX3/9taLRaJS1a9fqyzzI+7pixQpl1apVyqlTp5To6Gjl//7v/xRTU1Pl2LFjVaYO/7Vnzx7Fx8dHadasmTJixAj9/qpSlwkTJiiNGzdWrly5ot+uXr1a5eqRlJSkeHt7KwMGDFB2796txMTEKOvWrVPOnDmjL1NVft8TExOLvB8bNmxQAGXz5s1V6j2pKJKoy1FISIgSGRmp/z4/P1/x8PBQJk+ebJB4/puotVqt4ubmpkyZMkW/Lzk5WTE3N1d++eUXRVEU5cSJEwqg7N27V19mzZo1ikqlUi5duqQoiqJ88803iqOjo379X0VRlNGjRysNGjTQf9+7d2+lW7duReIJDQ1V/ve//5WpLomJiQqgbN26VR+3qamp8ttvv+nLREVFKYCyc+dORVF0H1rUarUSHx+vLzNr1izFzs5OH/s777yjNG7cuMi1+vTpo3Tu3Fn/fXm/r46Ojsp3331XJeuQlpam+Pn5KRs2bFAeeeQRfaKuSnWZMGGCEhAQUOxrVakeo0ePVtq0aXPX16vy7/uIESOUunXrKlqttkq9JxVFmr7LSU5ODvv37yciIkK/T61WExERwc6dOw0YWaFz584RHx9fJEZ7e3tCQ0P1Me7cuRMHBweCg4P1ZSIiIlCr1ezevVtfpl27dpiZmenLdO7cmejoaG7cuKEvc/t1CsqU9WeRkpICgJOTEwD79+8nNze3yDUaNmyIl5dXkbo0bdoUV1fXIjGkpqZy/PjxEsVZnu9rfn4+ixcvJiMjg7CwsCpZh8jISLp163bH9apaXU6fPo2Hhwe+vr7069eP2NjYKlePFStWEBwczLPPPouLiwuBgYHMnTtX/3pV/X3Pycnh559/5uWXX0alUlWp96SiSKIuJ9euXSM/P7/IfxQAV1dX4uPjDRRVUQVx3CvG+Ph4XFxcirxuYmKCk5NTkTLFneP2a9ytTFl+FlqtlpEjRxIeHk6TJk305zczM8PBweGedSlrnKmpqdy8ebNc3tejR49iY2ODubk5r7/+OsuWLaNRo0ZVqg4Aixcv5sCBA0yePPmO16pSXUJDQ5k/fz5r165l1qxZnDt3jrZt25KWllal6hETE8OsWbPw8/Nj3bp1DB48mOHDh7NgwYIisVS13/fly5eTnJzMgAED9OeuKu9JRZFFOYTRi4yM5NixY/z777+GDqVMGjRowKFDh0hJSWHp0qX079+frVu3GjqsUomLi2PEiBFs2LChyDrVVVHBAiAAzZo1IzQ0FG9vb3799VcsLS0NGFnpaLVagoODmTRpEgCBgYEcO3aM2bNn079/fwNHV3bff/89Xbt2xcPDw9ChGA25oy4nzs7OaDSaO3oiJiQk4ObmZqCoiiqI414xurm5kZiYWOT1vLw8kpKSipQp7hy3X+NuZUr7sxg6dCgrV65k8+bNRZYwdHNzIycnh+Tk5HvWpaxx2tnZYWlpWS7vq5mZGfXq1SMoKIjJkycTEBDAl19+WaXqsH//fhITE2nRogUmJiaYmJiwdetWvvrqK0xMTHB1da0ydfkvBwcH6tevz5kzZ6rUe+Lu7k6jRo2K7PP399c341fF3/cLFy6wcePGIqu3VaX3pKJIoi4nZmZmBAUFsWnTJv0+rVbLpk2bCAsLM2BkherUqYObm1uRGFNTU9m9e7c+xrCwMJKTk9m/f7++zN9//41WqyU0NFRfZtu2beTm5urLbNiwgQYNGuDo6Kgvc/t1CsqU9GehKApDhw5l2bJl/P3339SpU6fI60FBQZiamha5RnR0NLGxsUXqcvTo0SJ/iDZs2ICdnZ3+D9z94qyI91Wr1ZKdnV2l6tChQweOHj3KoUOH9FtwcDD9+vXTf11V6vJf6enpnD17Fnd39yr1noSHh98xZPHUqVN4e3sDVev3vcC8efNwcXGhW7du+n1V6T2pMAbtylbNLF68WDE3N1fmz5+vnDhxQnnttdcUBweHIj0RK1paWppy8OBB5eDBgwqgTJs2TTl48KBy4cIFRVF0wzUcHByUP//8Uzly5Ijy1FNPFTtcIzAwUNm9e7fy77//Kn5+fkWGayQnJyuurq7Kiy++qBw7dkxZvHixYmVldcdwDRMTE2Xq1KlKVFSUMmHChFIN1xg8eLBib2+vbNmypciwjczMTH2Z119/XfHy8lL+/vtvZd++fUpYWJgSFhamf71gyEanTp2UQ4cOKWvXrlVq1qxZ7JCNt99+W4mKilJmzpxZ7JCNsr6vY8aMUbZu3aqcO3dOOXLkiDJmzBhFpVIp69evrzJ1uJvbe31Xpbq8+eabypYtW5Rz584p27dvVyIiIhRnZ2clMTGxStVjz549iomJifLxxx8rp0+fVhYuXKhYWVkpP//8s75MVfl9VxRdD2svLy9l9OjRd7xWVd6TiiKJupx9/fXXipeXl2JmZqaEhIQou3btqtTrb968WQHu2Pr3768oim7Ixnvvvae4uroq5ubmSocOHZTo6Ogi57h+/brSt29fxcbGRrGzs1MGDhyopKWlFSlz+PBhpU2bNoq5ublSq1Yt5ZNPPrkjll9//VWpX7++YmZmpjRu3FhZtWpVietRXB0AZd68efoyN2/eVIYMGaI4OjoqVlZWSs+ePZUrV64UOc/58+eVrl27KpaWloqzs7Py5ptvKrm5uXf8zJo3b66YmZkpvr6+Ra5RoKzv68svv6x4e3srZmZmSs2aNZUOHTrok3RVqcPd/DdRV5W69OnTR3F3d1fMzMyUWrVqKX369Cky9riq1ENRFOWvv/5SmjRpopibmysNGzZU5syZU+T1qvL7riiKsm7dOgW4Iz5FqVrvSUVQKYqiGORWXgghhBD3Jc+ohRBCCCMmiVoIIYQwYpKohRBCCCMmiVoIIYQwYpKohRBCCCMmiVoIIYQwYpKoK0B2djYTJ04kOzvb0KE8EKmH8akudaku9YDqU5fqUg+oXnUBkHHUFSA1NRV7e3tSUlKws7MzdDhlJvUwPtWlLtWlHlB96lJd6gHVqy4gd9RCCCGEUZNELYQQQhgxWY+6GHl5eRw8eBBXV1fU6tJ/lklLSwPg0qVLpKamlnd4lUbqYXyqS12qSz2g+tSlutQDqkZdtFotCQkJBAYGYmJy71Qsz6iLsXfvXkJCQgwdhhBCiGpuz549tGzZ8p5l5I66GK6uroDuB+ju7m7gaIQQQlQ3V65cISQkRJ9v7kUSdTEKmrvd3d2pXbu2gaMRQghRXZXk8ap0JhNCCCGMmCRqIYQQwohJohZCCCGMmDyjFkKI2+Tn55Obm2voMEQVZ2pqikajKZdzSaKuaFotXD8NNfygDGOyhRCVQ1EU4uPjSU5ONnQooppwcHDAzc0NlUr1QOeRRF2Bbubks+DPtbx+/HkUcztUtYKgdstbWzBYORk6RCHELQVJ2sXFBSsrqwf+4yoeXoqikJmZSWJiIsADD/OVRF2B9l1IYs/BA/Q3NcMyOxViNuu2Ak6+uqRdK1iXuF2bgImZ4QIW4iGVn5+vT9I1atQwdDiiGrC0tAQgMTERFxeXB2oGl0RdgWpYm+Ma3IMnzrbGIukkzdVnCFSfobnqDPXUlyEpRrcdWaI7QGMOHs3BMwQ6fgjyiV6ISlHwTNrKysrAkYjqpOD/U25uriRqY9XIw47JTzcF4EpKa3aevc7Os9eZHnOd1BtXaa4+S3PVWQLVpwlUn8EhPwPidpOZdgPNY+9jbnLrjf3nc7CuCf7dwdLRgDUSonqT5m5Rnsrr/5Mk6kribm/J0y1q83QL3UxncUmZ7Iy5zq6z1/kt5jpXUm7io4onUHUGJVHFmonrCfJ2pI23Na/v/gx1fpauibwgUacngrkdmFoYsFZCCCEqmiRqA/F0ssLTyYrewZ4oisL565m6O+6YIHaevU52ejY7zl7n6Nk4ckweJ9DkAks3ZfFIg4u0q++My7qxcHIV1GkL9SKgbgeoUVeay4UQD8zHx4eRI0cycuTIEpXfsmUL7du358aNGzg4OFRYXPPnz2fkyJEPXc98SdRGQKVSUcfZmjrO1jwf6oWiKJy9ms7Os9fZcfY6P5x5jtSsPDhyhb+OXAHgb+v9+ObfhNPrdRuAgzfU66BL3HXagbmtAWslhKho92tanTBhAhMnTiz1effu3Yu1tXWJy7du3ZorV65gb29f6muJ+5NEbYRUKhX1XGyp52LLi2E+5OVrORSXzNZTV9kSfZWjl1J4LOMjGqjieER9mMdMjxGkOolp8gXY94NuU5uAZyuo9xjU6whuTeVuW4hq5sqVK/qvlyxZwvjx44mOjtbvs7Gx0X+tKAr5+fn3XfsYoGbNmqWKw8zMDDc3t1IdI0pOZuCoAkw0aoJ9nHizUwP+GtaGfe9GMK13cxo0a8Vv5k/zXNZYmt38loE5bzM/rxOX1O6gzYML/8KmD+DbtvBlAMTtNXRVhBDlyM3NTb/Z29ujUqn03588eRJbW1vWrFlDUFAQ5ubm/Pvvv5w9e5annnoKV1dXbGxsaNmyJRs3bixyXh8fH6ZPn67/XqVS8d1339GzZ0+srKzw8/NjxYoV+te3bNmCSqXSN0nPnz8fBwcH1q1bh7+/PzY2NnTp0qXIB4u8vDyGDx+Og4MDNWrUYPTo0fTv358ePXqU6mcwa9Ys6tati5mZGQ0aNOCnn37Sv6YoChMnTsTLywtzc3M8PDwYPny4/vVvvvkGPz8/LCwscHV15ZlnninVtSuL3FFXQc425vqOaflahSMXC+623Xj/YiATM8FLlUA79RE6mBwhXH0M0+RYVI7ehSc59w8o+eAdDhpTw1VGCCOlKAo3c/MNcm1LU0259RgeM2YMU6dOxdfXF0dHR+Li4nj88cf5+OOPMTc358cff6R79+5ER0fj5eV11/O8//77fPbZZ0yZMoWvv/6afv36ceHCBZycip+4KTMzk6lTp/LTTz+hVqt54YUXeOutt1i4cCEAn376KQsXLmTevHn4+/vz5Zdfsnz5ctq3b1/iui1btowRI0Ywffp0IiIiWLlyJQMHDqR27dq0b9+e33//nS+++ILFixfTuHFj4uPjOXz4MAD79u1j+PDh/PTTT7Ru3ZqkpCT++eefUvxkK48k6ipOo1YR6OVIoJcjIyPqk5SRwz+nr7I1+iprT3vyc3pHLMhmgPd13rJ0LnzDt34K5/+BLp9Aq8GGrIIQRulmbj6Nxq8zyLVPfNAZK7Py+fP8wQcf0LFjR/33Tk5OBAQE6L//8MMPWbZsGStWrGDo0KF3Pc+AAQPo27cvAJMmTeKrr75iz549dOnSpdjyubm5zJ49m7p16wIwdOhQPvjgA/3rX3/9NWPHjqVnz54AzJgxg9WrV5eqblOnTmXAgAEMGTIEgFGjRrFr1y6mTp1K+/btiY2Nxc3NjYiICExNTfHy8iIkJASA2NhYrK2teeKJJ7C1tcXb25vAwMBSXb+ySNN3NeNkbcZTzWsxrU9z9vxfBD+9EgKmlsy+4MGEFcdRFAUUBWrUAytnaPB44cFHl8JvA+H4MshON1wlhBDlJjg4uMj36enpvPXWW/j7++Pg4ICNjQ1RUVHExsbe8zzNmjXTf21tbY2dnZ1+isziWFlZ6ZM06KbRLCifkpJCQkKCPmkCaDQagoKCSlW3qKgowsPDi+wLDw8nKioKgGeffZabN2/i6+vLoEGDWLZsGXl5eQB07NgRb29vfH19efHFF1m4cCGZmZmlun5lkTvqakytVtHWryZfPhfI6z/vZ+HuWLxrWPFau7rQfTp0+xzUt82Wc/Q3OLUWjv+hmyWtXgfwfxIadJGJVsRDx9JUw4kPOhvs2uXlv72333rrLTZs2MDUqVOpV68elpaWPPPMM+Tk5NzzPKamRR+RqVQqtFptqcorilLK6B+Mp6cn0dHRbNy4kQ0bNjBkyBCmTJnC1q1bsbW15cCBA2zZsoX169czfvx4Jk6cyN69eyt0iFlZyB31Q6BzYzfe7dYIgEmrT7L66K0OHer//DF45B0IHwGOdSA/G6JXw/LXYYof/PE/uHK4kiMXwnBUKhVWZiYG2SpyhrTt27czYMAAevbsSdOmTXFzc+P8+fMVdr3i2Nvb4+rqyt69hR1c8/PzOXDgQKnO4+/vz/bt24vs2759O40aNdJ/b2lpSffu3fnqq6/YsmULO3fu5OjRowCYmJgQERHBZ599xpEjRzh//jx///33A9SsYsgd9UPi5XAfYq9nsGDnBd5YcghXOwuCvP9zl1wrSLdFvA8JxyHqL4haAYkn4Mhi3ebTFsIiwa+zLNspRBXk5+fHH3/8Qffu3VGpVLz33nv3vDOuKMOGDWPy5MnUq1ePhg0b8vXXX3Pjxo1SfUh5++236d27N4GBgURERPDXX3/xxx9/6Huxz58/n/z8fEJDQ7GysuLnn3/G0tISb29vVq5cSUxMDO3atcPR0ZHVq1ej1Wpp0KBBRVW5zOQv7UNCpVIxvntjIvxdyM7TMujHfVy4nnG3wuDWBNqPhSE74dW/oUkvUGl0HdB+eQ5mBMOeuZB7s3IrIoR4INOmTcPR0ZHWrVvTvXt3OnfuTIsWLSo9jtGjR9O3b19eeuklwsLCsLGxoXPnzlhYlHxa5B49evDll18ydepUGjduzLfffsu8efN49NFHAd160HPnziU8PJxmzZqxceNG/vrrL2rUqIGDgwN//PEHjz32GP7+/syePZtffvmFxo0bV1CNy06lVPZDgyrg4sWLeHp6EhcXR+3atQ0dTrnKzMmjz7e7OHopBV9na/4Y0hoHqxIurZlyEXZ/C/sXQHYKmFrBG8dlXW1R5WVlZXHu3Dnq1KlTqkQhyo9Wq8Xf35/evXvz4YcfGjqccnGv/1elyTMGv6OeOXMmPj4+WFhYEBoayp49e+5ZPjk5mcjISNzd3TE3N6d+/fpFuvTn5+fz3nvvUadOHSwtLalbty4ffvhhpXdiMFZWZiZ83z+YWg6WxFzL4LUf95OdV8Kxova1odOHMOoEdP0M2o4qmqS3fAKXD1VI3EKI6uXChQvMnTuXU6dOcfToUQYPHsy5c+d4/vnnDR2a0TFool6yZAmjRo1iwoQJHDhwgICAADp37nzXLv85OTl07NiR8+fPs3TpUqKjo5k7dy61atXSl/n000+ZNWsWM2bMICoqik8//ZTPPvuMr7/+urKqZfRc7Cz4YUBLbM1N2HM+iXeWHindBxlzGwj9H7R7u3Bf3F7YMhm+7wiZSeUftBCiWlGr1cyfP5+WLVsSHh7O0aNH2bhxI/7+/oYOzegYtDPZtGnTGDRoEAMHDgRg9uzZrFq1ih9++IExY8bcUf6HH34gKSmJHTt26Lv++/j4FCmzY8cOnnrqKbp166Z//ZdffrnvnfrDpoGbLbNeCGLAvD38eegyXk5WvNnpATpRWDpA095galn0LjvqL6j7GJiVfIJ/IUT15+npeUePbVE8g91R5+TksH//fiIiIgqDUauJiIhg586dxR6zYsUKwsLCiIyMxNXVlSZNmjBp0iTy8wubblu3bs2mTZs4deoUAIcPH+bff/+la9euFVuhKqiNnzOTejYF4Ou/z/Drvriyn8zZD3rNhe5fFu6LPwZLXoBpjWDj+5AW/4ARCyHEw8dgd9TXrl0jPz8fV1fXIvtdXV05efJkscfExMTw999/069fP1avXs2ZM2cYMmQIubm5TJgwAdDNa5uamkrDhg3RaDTk5+fz8ccf069fv7vGkp2dTXZ2tv77tLS0cqhh1dC7pSexSZnM2HyG//vjKB72lrTxcy77CW8fWpF5DZx8ISkG/p0GO2dAs97QejjUNL4hEEIIYYwM3pmsNLRaLS4uLsyZM4egoCD69OnDuHHjmD17tr7Mr7/+ysKFC1m0aBEHDhxgwYIFTJ06lQULFtz1vJMnT8be3l6/3T5Y/mHwZqf6PNXcgzytwuCf9xMdX04fVHwfhaH7oM9C8AyF/Bw4+DPMDIFFfeD8v7rpTIUQQtyVwRK1s7MzGo2GhISEIvsTEhLuuq6pu7s79evXR6MpnFHL39+f+Ph4/fR3b7/9NmPGjOG5556jadOmvPjii7zxxhtMnjz5rrGMHTuWlJQU/XbixIlyqGHVoVKp+OyZZoT4OJGWncfL8/eSmJpVPidXa8D/CXhlPby8Hho+Aah0U5XO7wZzH9PNLa41zCpFQghh7AyWqM3MzAgKCmLTpk36fVqtlk2bNhEWFlbsMeHh4Zw5c6bILDqnTp3C3d0dMzPdWODMzEzU/5kxS6PR3HPmHXNzc+zs7PSbra3tg1StSjI30fDti0H4OltzKfkmryzYR2ZOXvlexCsUnlsIw/ZD8MtgYgGXD8BvA+CrQNg9B3LuMgmLEEI8pAza9D1q1Cjmzp3LggULiIqKYvDgwWRkZOh7gb/00kuMHTtWX37w4MEkJSUxYsQITp06xapVq5g0aRKRkZH6Mt27d+fjjz9m1apVnD9/nmXLljFt2jT9Umri7hytzZg3sCVO1mYcvZTC8F8Okq+tgKbpGnXhiS9g5DF4ZDRYOkHyBVjzNlw7Vf7XE0KIqkwxsK+//lrx8vJSzMzMlJCQEGXXrl361x555BGlf//+Rcrv2LFDCQ0NVczNzRVfX1/l448/VvLy8vSvp6amKiNGjFC8vLwUCwsLxdfXVxk3bpySnZ1d4pji4uIUQImLi3vg+lVF+84nKX7jViveo1cqE/48VvEXzM5QlN1zFGXZkKL7Dy9RlKunK/764qF38+ZN5cSJE8rNmzcNHYpBPPLII8qIESP033t7eytffPHFPY8BlGXLlj3wtcvrPPcyYcIEJSAgoEKvUZx7/b8qTZ4x+KIcQ4cOveti5Vu2bLljX1hYGLt27brr+WxtbZk+fTrTp08vpwgfPkHejnzRuzmRiw4wf8d5ajta0jOwFmlZeaRn55GWlUdaVi7p2bd/n0d6dq7u36w80rIL92m1MPjRurzQyrv4C5pZQcigovvSE+HPoboOaEN2gotMgiDEf3Xv3p3c3FzWrl17x2v//PMP7dq14/Dhw0XWki6JvXv33rE85oOaOHEiy5cv59ChQ0X2X7lyBUdHWUb3XgyeqIVx6tbMnYs3GjJ5zUk+WhXFR6uiHuh87y4/Rm6+loHhdUp2QE66bqKUzGtQs2Hh/ui14B4Adu4PFI8Q1cErr7xCr169uHjx4h3zRc+bN4/g4OBSJ2mAmjVrlleI93W3zsOiUJUaniUq12vtfHmtnS8atW5stLWZBlc7c+q52NDc04G2fs483tSN3sG1eaVNHUZ08OPdbv582qspM59vwYKXQ/hjSGtef6QuAO//dYIFO86X7OJOvvD8Yui/snBsdlYqLB0IXzTSDe+KWgn5uRVQcyGqhieeeIKaNWsyf/78IvvT09P57bffeOWVV7h+/Tp9+/alVq1aWFlZ0bRpU3755Zd7ntfHx6dIq+Tp06dp164dFhYWNGrUiA0bNtxxzOjRo6lfvz5WVlb4+vry3nvvkZur+/2cP38+77//PocPH0alUqFSqfQxq1Qqli9frj/P0aNHeeyxx7C0tKRGjRq89tprpKen618fMGAAPXr0YOrUqbi7u1OjRg0iIyP11yoJrVbLBx98QO3atTE3N6d58+ZFWiVycnIYOnQo7u7uWFhY4O3trR85pCgKEydOxMvLC3Nzczw8PBg+fHiJr10Wckct7kqlUvF/j/szqmN9TDVqfcIurUBPB9Qq+GbLWSasOI5KBS+F+ZTsYNPbVpxJT9TdTcfu1A3vOrUWrF2geV8IfAmc65UpPiHuqSwjETTmoLn15zU/D/KzQaXWTbF7v/OWYrpdExMTXnrpJebPn8+4ceP0azn/9ttv5Ofn07dvX9LT0wkKCmL06NHY2dmxatUqXnzxRerWrUtISMh9r6HVann66adxdXVl9+7dpKSkMHLkyDvK2draMn/+fDw8PDh69CiDBg3C1taWd955hz59+nDs2DHWrl2rXyva3t7+jnNkZGTQuXNnwsLC2Lt3L4mJibz66qsMHTq0yIeRzZs34+7uzubNmzlz5gx9+vShefPmDBo06I5zFufLL7/k888/59tvvyUwMJAffviBJ598kuPHj+Pn58dXX33FihUr+PXXX/Hy8iIuLo64ON3Mjb///jtffPEFixcvpnHjxsTHx3P48OESXbesJFGL+7Iw1dy/0D2oVCre7twArQKzt55l/J/HUalUvHi3Z9Z341wPXl4LV0/BwZ/g8C+QkQjbv9RtXq2hxYvQ6CmZW1yUn0kepT/m2fnQ+NZIk5N/6YYgereBgasKy0xvCpnX7zx2YkqpLvXyyy8zZcoUtm7dql+Hed68efTq1Us/idNbb72lLz9s2DDWrVvHr7/+WqJEvXHjRk6ePMm6devw8ND9LCZNmnTHtMzvvvuu/msfHx/eeustFi9ezDvvvIOlpSU2NjaYmJjcs6l70aJFZGVl8eOPP+qfkc+YMYPu3bvz6aef6meydHR0ZMaMGWg0Gho2bEi3bt3YtGlTiRP11KlTGT16NM899xygW8xp8+bNTJ8+nZkzZxIbG4ufnx9t2rRBpVLh7V34tyo2NhY3NzciIiIwNTXFy8urRD/HByFN36JSqFQqRndpwGvtfAF4b/kxFu2OLdvJata/tdxmlG7Ws/pddHcrsTtg+WCY2gD+GgGX9svMZ6Laa9iwIa1bt+aHH34A4MyZM/zzzz+88sorgG7p3w8//JCmTZvi5OSEjY0N69atIza2ZL9/UVFReHp66pM0UOxcF0uWLCE8PBw3NzdsbGx49913S3yN268VEBBQpCNbeHg4Wq2W6Oho/b7GjRsXmfjK3d39rqsu/ldqaiqXL18mPDy8yP7w8HCionR9cQYMGMChQ4do0KABw4cPZ/369fpyzz77LDdv3sTX15dBgwaxbNky8vLKec6J/5A7alFpVCoVY7s2RKtV+O7fc/zfsqOoVfBciFfZTqgx1c165v8EpF6GQ4t0U5TeOAf75+u2Fv3hya/KsxriYfN/l0t/jMa88OuG3XXnUP3nvmjk0QeL6zavvPIKw4YNY+bMmcybN4+6devyyCOPADBlyhS+/PJLpk+fTtOmTbG2tmbkyJH62RzLw86dO+nXrx/vv/8+nTt3xt7ensWLF/P555+X2zVuV7B6YgGVSnXPSa1Kq0WLFpw7d441a9awceNGevfuTUREBEuXLsXT05Po6Gg2btzIhg0bGDJkiL5F479xlRe5oxaVSqVSMa6bPy/f6v095o+j/Lr3AVbtKmDnAe3egmEHoP9fuiU3TSzQerdhzdErbDt1FZJjYe/3kJZw//MJUcDMuvSb5rZ7II2Jbt/tz6fvdd4y6N27N2q1mkWLFvHjjz/y8ssv659Xb9++naeeeooXXniBgIAAfH199asLloS/vz9xcXFcuXJFv++/Q2R37NiBt7c348aNIzg4GD8/Py5cuFC0umZmRVY6vNu1Dh8+TEZG4fP77du3o1aradCgfBbysbOzw8PD444lNrdv315knQc7Ozv69OnD3LlzWbJkCb///jtJSUkAWFpa0r17d7766iu2bNnCzp07OXq0/D54/ZfcUYtKp1KpeO8Jf7SKwvwd5xn9xxFQQe9gzwc/uVoNddpBnXbsODaaTzbGciT+AAA/N9xJm/NfQ9QKeOnPB7+WEEbCxsaGPn36MHbsWFJTUxkwYID+NT8/P5YuXcqOHTtwdHRk2rRpJCQklHjxoYiICOrXr0///v2ZMmUKqampjBs3rkgZPz8/YmNjWbx4MS1btmTVqlUsW7asSBkfHx/OnTvHoUOHqF27Nra2tpibmxcp069fPyZMmED//v2ZOHEiV69eZdiwYbz44ot3rLT4IN5++20mTJhA3bp1ad68OfPmzePQoUMsXLgQgGnTpuHu7k5gYCBqtZrffvsNNzc3HBwcmD9/Pvn5+YSGhmJlZcXPP/+MpaVlkefY5U3uqIVBqFQqJnRvRP8wbxQFRv9+hKX7L5bLuU9cTuXF73fz/M/RHIm/ieWtznC/ntISa+lPfsMnCwunxcPcDvDvF3D9bLlcXwhDeOWVV7hx4wadO3cu8jz53XffpUWLFnTu3JlHH30UNzc3evToUeLzqtVqli1bxs2bNwkJCeHVV1/l448/LlLmySef5I033mDo0KE0b96cHTt28N577xUp06tXL7p06UL79u2pWbNmsUPErKysWLduHUlJSbRs2ZJnnnmGDh06MGPGjNL9MO5j+PDhjBo1ijfffJOmTZuydu1aVqxYgZ+fH6Drwf7ZZ58RHBxMy5YtOX/+PKtXr0atVuPg4MDcuXMJDw+nWbNmbNy4kb/++osaNWqUa4y3UymK9Lb5r4sXL+Lp6UlcXNwdkwiI8qUoCuP/PM5Puy6gUsHnzwbwdIuy/cwvJ99k6vpolh28hKKAqUbFi618GPZYPVYevcKEP4+hVaB9fWdm9AvC2twE9syF1YU9YnFtAv7dwf9J3WxoqrINSRNVS1ZWFufOnaNOnTpYWFjc/wAhSuBe/69Kk2ek6VsYlEql4oOnGqOg8POuWN787TBqlYoegbVKfI7UrFy+2XyWedvPkZ2n61DyRDN33uncEK8aVgC82MobNzsLhv1ygM2nrvHcnF38MKAlNRv10C3FeWIFnNsGCcd025bJUKOeLmH7dwePQEnaQgiDkDvqYsgddeXTahXe/VM3ZEutgi/6NOep5vdO1jl5Wn7edYGv/z7NjUzdrEQhdZz4v8f9ae7pUOwxB2Nv8MqCfSRl5ODpZMn8gSHUrWmjezEzCaLX6J5hn/1bN894AXtPqN9ZNxTMp23RiVhElSd31KIiyB21qFbUahUfPdUErVZh8d443lhyCLVKRfeAOyebUBSF1Ufj+WzdSS5czwSgbk1rxnb1p4O/i763a3ECvRz5Y3Br+s/bw4XrmfSatYPvXgom2McJrJwgsJ9uy0qF0+t1Sfv0BkiJg73f6TYZ8iWEqESSqIXRUKtVTOrZFK2i8Ou+i4xccgiVCp5oVpis955P4uNVURyKSwbA2cacUR3r0zu4NiaakvWN9HG25o/BrXl5wT4OxyXz/He7+bJPc7o2vW2hDws7aPqMbsvJ1DWLn1oLp9ZBvYjCcpf2w18jocnT0OaNcvgpCCFEUZKohVFRq1V88nQztAos3X+REYt1d9b1XW35dO1JNpzQjYG2MtPwWjtfBrX11XUKK6UaNuYsHtSKYb8cYGNUIkMWHWD8E42KX93LzAoadNFtigLKbRMrnFoH8UfA0afoMdFrwbu1LuELIcQDkEQtjI5areLTXs1QFPj9wEWG/XIQgHytgkatok9LT0Z28MPF7sGeJVqaaZj9QhAT/zrOz7tief+vE1xOvsnYrv6o77YAiUoFqtvmPg95DRzrgP1tz9OvnYFf+oDaFMW7NVdc2hFlF86jYa3KvLCJqBzlObuVEOX1/0kStTBKGrWKz55phqIo/HHwEgAR/q6M6dqAei625XYdE42aD59qgoeDJZ+tjWbuP+e4nJLF588GlGwxEmtn3epdt9GmXiHbzhfL1BhU57bicW4rHsDlXU3xaP8/XTO5LBpiVMzMzFCr1Vy+fJmaNWtiZmZ2z74OQtyLoijk5ORw9epV1Go1ZmZmD3Q+6fVdDOn1bTzytQrLDl7Cp4aVrsNXBVp28CLvLD1Cbr5CSB0n5r4YjL1VyebuzcvXsvtcEmuOXWHd8QSupmXjo7rCY+pDRGgOEqI6gYnq1qdrM1to2gtavAQeLWTYl5HIycnhypUrZGZmGjoUUU1YWVnh7u5ebKIuTZ6RRF0MSdQPr+1nrvH6T/tJy86jnosN8we2pLajVbFls/Py2XHmOmuOXWHDiQT9EDEAWwsTIvxd6dzYjXb1nRn87Rr841cyyPpfauTcNgOba1Ndwm72LFg6VnT1xH0oikJeXt5956QW4n40Gg0mJiZ3bZmRRP2AJFE/3KKupDJw3l7iU7NwsTVn3sCWNPbQLXJ/MyefracSWXMsnr+jEknLLlzeztHKlE6N3OjS1I3wus6YmRT2Qt8Vc53n5uzCRA3/9DbD/eyvcOJPyM/WFQh+BZ6YVqn1FEIYjoyjFuIB+LvbsSyyNQN+2Et0Qhq9Z+9kRIQfB2OT2RJ9lZu5hXdbLrbmdGniRpcmboT4ON11iFgr3xo81tCFv08m8uFxJ77pNxce/wyO/AYHFkCLFwsLXz4EMZuheT+wcang2gohjJ3cURdD7qgFQMrNXF7/aT87Y64X2V/b0ZKut5JzoKfj3XuI/8fJ+FS6fvkPigLLhrQm0OtWU3fBr2BBE9mfkbp1tQOeh56zyqs6QggjUpo8Y/DVs2bOnImPjw8WFhaEhoayZ8+ee5ZPTk4mMjISd3d3zM3NqV+/PqtXry5S5tKlS7zwwgvUqFEDS0tLmjZtyr59+yqyGqIasrc0Zf7LLekb4oW/ux2R7euyclgb/nmnPeO6NSLI26nESRqgoZsdvW4tOPLJmpMotyfo259j1XkUaofonl0XSDgO69+Fi/sLE7sQ4qFg0KbvJUuWMGrUKGbPnk1oaCjTp0+nc+fOREdH4+JyZ5NfTk4OHTt2xMXFhaVLl1KrVi0uXLiAg4ODvsyNGzcIDw+nffv2rFmzhpo1a3L69GkcHaWjjig9cxMNk59uWm7nG9WxPisOX2b3uSQ2RyfyWMNi1tht9qxuu92+ebB3Luz4WjfveKOndFutYN0a3EKIasugTd+hoaG0bNlSv9aoVqvF09OTYcOGMWbMmDvKz549mylTpnDy5ElMTYsfNjNmzBi2b9/OP//8U+a4pOlbVKTJq6P4dlsMDVxtWT2ibckmQTm1Do4s0c14lptRuN/WAxo9CY16gGeoJG0hqogq0fSdk5PD/v37iYgonDdZrVYTERHBzp07iz1mxYoVhIWFERkZiaurK02aNGHSpElFhlKsWLGC4OBgnn32WVxcXAgMDGTu3LkVXh8hSmrIo/WwtzQlOiGNPw5cvP8BoFu565kf4J2z0GchNO2tG4+ddhl2z4Z5XWCaP6x6C87/C1oZXiREdWGwRH3t2jXy8/NxdS3a9Ofq6kp8fHyxx8TExLB06VLy8/NZvXo17733Hp9//jkfffRRkTKzZs3Cz8+PdevWMXjwYIYPH86CBQvuGkt2djapqan6LS0trXwqKUQx7K1MiWxfF4BpG06RlVuKpGpqCf5PQK+58PYZ6LsYAvqCuT2kx+uax+d3g88bwKn1FVQDIURlqlLDs7RaLS4uLsyZMweNRkNQUBCXLl1iypQpTJgwQV8mODiYSZMmARAYGMixY8eYPXs2/fv3L/a8kydP5v3336+0egjxUpgP87ef53JKFgt2nOd/j9Qt/UlMLaBBV92WlwPntsKJ5XByFWRcBUfvwrKXDuj2+bTVLTIihKgyDHZH7ezsjEajISEhocj+hIQE3Nzcij3G3d2d+vXro9EUzsHs7+9PfHw8OTk5+jKNGjUqcpy/vz+xsbF3jWXs2LGkpKTotxMnTpS1WkKUiIWphlGdGgAwc/MZkjNzHuyEJmbg1xGemglvnYYBq6Bmg8LXt38Ji3rDv18U7tNqpQe5EFWAwRK1mZkZQUFBbNq0Sb9Pq9WyadMmwsLCij0mPDycM2fOFFmR5NSpU0XmUg0PDyc6OrrIcadOncLb25u7MTc3x87OTr/Z2pbfog9C3E3PwFo0dLMlNSuPb7acLb8Ta0zBp03RfQ6eYO+lS+YFolfDlwGw6k1dJ7WcDIQQxsegXURHjRrF3LlzWbBgAVFRUQwePJiMjAwGDhwIwEsvvcTYsWP15QcPHkxSUhIjRozg1KlTrFq1ikmTJhEZGakv88Ybb7Br1y4mTZrEmTNnWLRoEXPmzClSRghjoFGrGN2lIQDzd5znUvLNirtYp49g5BGo3bJw35kNkHwB9n6nW5bz0zrwU0/Y+Q1cOy1320IYCYM+o+7Tpw9Xr15l/PjxxMfH07x5c9auXavvYBYbG4v6tuEmnp6erFu3jjfeeINmzZpRq1YtRowYwejRo/VlWrZsybJlyxg7diwffPABderUYfr06fTr16/S6yfE/TzaoCatfJ3YFZPEtPWn+Lx3QMVd7L+LA3T6GPw66xL26Y2QEgtn/9Zt68aCg7fuDtyvkzzbFsKAZArRYsg4alGZDscl89TM7ahUsHp4W/zd7So/CEWBa6fg9AZd4r6wA/Jve26uMdc1p4f+TzdUTAjxQKrEOGohhE6ApwPdmrqjKPDp2pOGCUKl0nU+az0UXvoT3jkHz/0CwS/rnm3nZ8PZTZB8W6fMmzcgMUqayIWoYFVqeJYQ1dVbnRuw7ng8W6KvsuPsNVrXdTZsQOY20PBx3aYocDUaTq2Bht0KyxxfBivf0M2K1vvu8xQIIR6M3FELYQTqOFvTN8QLgE9vX7DDGKhU4NIQ2rwBdh6F+9MTdU3i7rc9V795A5ZHQtRKyMms/FiFqIbkjloIIzG8gx9/HLjI4YsprDp6hSeaedz/IEN6dAyEDQXltpnVTm+AQz/rNhNLqNseGjyum5TF2sCtBEJUUXJHLYSRqGlrzqB2vgBMWRdNbr72PkcYAXMbsLAv/N6lEYQO1j3XzrupG6u9YihM9YMfusLuOZCWcPfzCSHuIIlaCCPyaltfnG3MuHA9k1/23H02PaPl1gS6fqIbs/36v/Do/5Hn0hQULcTugDVvw7SGsKC7bunOjOuGjlgIoyeJWggjYmNuwogOfgB8ufE06dl5Bo6ojFQq8mo25ifzPgRfG0/rrK/4MPcFEu1uJe1z22DlSN2d9ubJho5WCKMmiVoII/NciBc+Nay4npHD3G0xhg6nTHbFXOeJr//lvT+Pk5yZS46NB9/nP85T2e+TO/QQREwEt2a659tOvoUHplyCI79CtqxgJ0QBSdRCGBlTjZq3O+umFp37TwyJaVkGjqjkLiXfJHLRAZ6bs4uT8WnYW5ry4VON2fZOe5xtzLmSksVfsaa6HuSv/wND9xcd8nX0N/hjEPz6kuEqIYSRkUQthBF6vKkbAZ4OZObk89Wm04YO576ycvP5cuNpOny+hVVHrqBWwYutvNny1qO8GOaDlZkJA8N9AJizLaZw+JlzPV2HtAKWDuBUF/y7F+5LS4Clr+iW78x7wFXGHmILdpyn2cR1HLuUYuhQRClJohbCCKlUKsZ21d1V/7Injpir6QaOqHiKorDm6BU6fL6VLzaeIitXS0gdJ1YOa8uHPZrgaG2mL/tCqDdWZhpOxqex9dTV4k8YNACG7YfA2+6oT/wJx5bC4udhmj+sG6ebgEWUysLdF0jNymPZwUuGDkWUkiRqIYxUK98atG9Qk3ytwtT1xpeYouPT6PfdbgYvPMCl5Jt42Fsw4/lAlrzWikYed85Xbm9lynMtdZO6zLnXs3eVCjS3TfHg0wZaRYKNK2Reg50zYGYIfNcRDvwE2cb5IcaYJGXkcCpB93Pad+GGgaMRpSUTnghhxEZ3bciWU1dZfTSe8X8eQ6NWkZevkKdVyMvX6v699XVuvkK+VrcvN19bWE6r+7qmrTm+ztbUdbHB19kG35rWuNtboPrvqlr3kZKZyxcbT/HTrgvkaxXMTNS8/khdBj9SF0szzT2PfbmNDwt2nmfH2escu5RCk1r29ywPgGsj6DIJOn4Ap9fDwZ/g1Dq4uEe3rR0DjXtCi/5QO/jOVcIEe88n6b8+fimFrNx8LEzv/V4J41GmRB0XF4dKpdKv+LFnzx4WLVpEo0aNeO2118o1QCEeZg3d7OjVojZL91/kx50XHuhcJ+PT+Of0tSL7LE011HG2xremNXVr2uj/reNsjbV50T8P+VqFxXtjmboumhuZuQB0aezGuG7+eDqVbAnM2o5WPNHMnT8PXebbbTF83Tew5BXQmBTOP54WD4cWwcGfIemsLnkf/AlqNoTAFyGgL1jXKPm5q7k95woTdZ5W4XBcMqG+8vOpKsqUqJ9//nlee+01XnzxReLj4+nYsSONGzdm4cKFxMfHM378+PKOU4iH1nvdGuFub0F2nhYTtQoTjfrWvypM1Wo0ahWmGt1+/ddqdWFZjQq1SkV8yk1irmZw9moGMdfSib2eyc3cfE5cSeXEldQ7rutmZ4FvTV0S93ayZtnBS/py9V1tmNC9MeH1Sj8t6GvtfPnz0GVWHbnMO50blDjJF2HrBm1H6XqPX9ihS9LHl8PVk7B+HDh6F+2Q9pDbfU43sYy1mYaMnHz2x96QRF2FlClRHzt2jJCQEAB+/fVXmjRpwvbt21m/fj2vv/66JGohypG9lSlvdmpQ7ufNzdcSm5RJzNUMYq6m6/69ls7ZqxkkZeQQn5pFfGoWO84Wzh5mZ2HCGx3r80Irb0w1Zevi0tjDnrZ+zvxz+hrf/3uOiU82LnslVCrwCddtXT+Fo0t1vcPrdykss3sOZFyFFi+Bg2fZr1VFpWblcuKy7gNWv1bezNkWw/7z8py6KilTos7NzcXc3ByAjRs38uSTTwLQsGFDrly5Un7RCSEqjKlGTd2aNtStaQO4FnktOTNHd+d9NZ2Yaxmcu5pBbUdLBj9alxo25g987dfa+fLP6Wss2RvHiA5+RXqHl5mFPbR8RbcV0ObD9i8h9SI4138oE/X+CzfQKuDlZEW3pu66RB17A61WQa2W5/lVQZkSdePGjZk9ezbdunVjw4YNfPjhhwBcvnyZGjWkOUWIqs7ByowgbzOCvB0r5Pxt6jnTyN2OE1dS+WnXBYbfmja13CkKdPoAjv4O/k8U7t/+JcTuhmbP6u6+TS0r5vpGoOD5dEgdJxp52GFhqiY5M5eYaxnUc7G5z9HCGJSp7erTTz/l22+/5dFHH6Vv374EBOjWo12xYoW+SVwIIe5GpVLxv0d0U4cu2HGerNz8+xxRRhoTaNIL+i4qTMaKouuEFr0KfhsAU/xg+RA4u1l3B17NFCTq0DpOmGrUBNR2AGD/haR7HCWMSZkS9aOPPsq1a9e4du0aP/zwg37/a6+9xuzZs8stOCFE9fV4U3dqOVhyPSOH3w9crLwLq1Tw7HwIHwl2tSEnDQ4thJ966CZUWTsWLh3QJfQq7mZOPkcuJgMQWkfX2lnQSrJfxlNXGWVK1Ddv3iQ7OxtHR90bfuHCBaZPn050dDQuLi7lGqAQonoy1ah5pU0dAOZuiyFfW4mJ0bUxdHwfRh6FgWsgaCBYOEB6Auz6Bua2hxktYcunkFQ1F0YBOBh7g9x8BTc7CzyddC0KwT66v9sy8UnVUaZE/dRTT/Hjjz8CkJycTGhoKJ9//jk9evRg1qxZ5RqgEKL66tPSE3tLU85fz2TDifjKD0CtBu/W0H06vHUanvtFN3mKiQVcPw1bJsFXgTC3g27O8Spm923PpwsmtmnhpUvUMbd69wvjV6ZEfeDAAdq2bQvA0qVLcXV15cKFC/z444989dVXpT7fzJkz8fHxwcLCgtDQUPbs2XPP8snJyURGRuLu7o65uTn169dn9erVxZb95JNPUKlUjBw5stRxCSEqlrW5CS+00k0rOnvrbYt1GIKJmW4ylWfnw9tnoMdsqPsYqNSQdgWsaxaWPb0Rrp81WKgldXtHsgIOVmb6TmQH5K66SihTos7MzMTW1haA9evX8/TTT6NWq2nVqhUXLpRu9qQlS5YwatQoJkyYwIEDBwgICKBz584kJiYWWz4nJ4eOHTty/vx5li5dSnR0NHPnzqVWrVp3lN27dy/ffvstzZo1K30lhRCVon9rH8xM1ByKS2avsYzvNbeF5n3hxWUw6iT0+l539w2Qn6dbivPrFrqe40YqOy+fA7G6n2crX6cirwV7S/N3VVKmRF2vXj2WL19OXFwc69ato1OnTgAkJiZiZ3fnZPz3Mm3aNAYNGsTAgQNp1KgRs2fPxsrKqkgntdv98MMPJCUlsXz5csLDw/Hx8eGRRx7R9zwvkJ6eTr9+/Zg7d67+WboQwvi42FrQq4Xug/acbUZ4l2rrCt5hhd9nXgf3Zro77FpBhfu3TYU1o+HcNl0yN7CjF1PIztPiZG12a6x8oRa3ErXcUVcNZUrU48eP56233sLHx4eQkBDCwnT/idevX09gYMnn7s3JyWH//v1EREQUBqRWExERwc6dO4s9ZsWKFYSFhREZGYmrqytNmjRh0qRJ5OcXHVYRGRlJt27dipxbCGGcXm3ri0oFG6MSOZOYZuhw7s3WFV76E944XrjKl6LA/vmwezYs6A5Tbw35Orkacm8aJEz982kfpzsWXim4oz58MZmcPG2lxyZKp0wTnjzzzDO0adOGK1euFLmT7dChAz179izxea5du0Z+fj6urkVnRXJ1deXkyZPFHhMTE8Pff/9Nv379WL16NWfOnGHIkCHk5uYyYcIEABYvXsyBAwfYu3dvieLIzs4mOztb/31ampH/oRCimqlb04aO/q6sP5HA3G3n+PSZKvC4yuS2GdoUBbp+BidXQvRquJmkG/J1aCGYWkG9DtCwO9TvBJaV08JX3PPpAnWcrXGyNiMpI4fjl1MI9JJWR2NW5mUu3dzccHNz4+JF3fjH2rVrV8pkJ1qtFhcXF+bMmYNGoyEoKIhLly4xZcoUJkyYQFxcHCNGjGDDhg1YWFiU6JyTJ0/m/fffr+DIhRD38r9HfFl/IoFlBy/xZqf6uNiV7PfXKKjVhSt75edB7E5d0o5aqZu+NOov3aY21SXtxk/ryprbVkg4efla/TjpUN87E7VKpaKFlyMboxLYf+GGJGojV6amb61WywcffIC9vT3e3t54e3vj4ODAhx9+iFZb8mYUZ2dnNBoNCQlFhz0kJCTg5uZW7DHu7u7Ur18fjaZwLVV/f3/i4+P1TemJiYm0aNECExMTTExM2Lp1K1999RUmJiZ3NJEDjB07lpSUFP124sSJEtdBCFE+grydCPJ2JCdfy7wd5w0dTtlpTKBOW90iIW8cg9e2QLu3dUtwanPh1FpY9hpMqQcpFTPRy4krqaRn52FrYUJDt+L7DcnEJ1VHmRL1uHHjmDFjBp988gkHDx7k4MGDTJo0ia+//pr33nuvxOcxMzMjKCiITZs26fdptVo2bdqkf+79X+Hh4Zw5c6bIB4JTp07h7u6OmZkZHTp04OjRoxw6dEi/BQcH069fPw4dOlQkwRcwNzfHzs5OvxX0aBdCVK7/tdNNK/rzrgukZxu+Q9YDU6nAIxAeexcid8OQXdDuHahRDxx9wL52Ydk9c3Urf+Vl3/V0JVXQ7N3SxwnNXRbeuH3iE4MOixP3Vaam7wULFvDdd9/pV80CaNasGbVq1WLIkCF8/PHHJT7XqFGj6N+/P8HBwYSEhDB9+nQyMjIYOHAgAC+99BK1atVi8uTJAAwePJgZM2YwYsQIhg0bxunTp5k0aRLDhw8HwNbWliZNmhS5hrW1NTVq1LhjvxDCuET4u+Jb05qYqxks3hPLq219DR1S+XLxh8fGQfv/g8zb5trOyYQNEyA3A17dBLWDH+gyu+/xfLpA01r2mGpUXE3L5uKNm2VbF1xUijLdUSclJdGwYcM79jds2JCkpNJN9N6nTx+mTp3K+PHjad68OYcOHWLt2rX6DmaxsbFFls709PRk3bp17N27l2bNmjF8+HBGjBjBmDFjylIVIYQRUatVDLqVnH/49xy5+dW0R7JKBda3rTSYlwVBA8ArrOiQr/XvwYrhELO1xAuGaLUKe8/fP1FbmGpoUssegH2yQIdRUyllaPMIDQ0lNDT0jlnIhg0bxp49e9i923gnASiJixcv4unpSVxcHLVr177/AUKIcpOVm0+bTzdzLT2bL/oE0DPwIf0dzMvRDfPKStZ9b+0CjZ7STXHq1QrUdz7GA4iOT6Pz9G1Ymmo4MrETppq73499tPIE3/17jn6hXnzcs2kFVELcTWnyTJmavj/77DO6devGxo0b9c+Sd+7cSVxc3F2n8hRCiJKwMNUwMNyHKeui+XZrDD2a17pjHPBDQa2B3gvg2B8QtQIyEmHvXN1m4wr+T0LjHrq78NuS9u5z1wFdZ7F7JWnQPaf+7t9z0qHMyJWp6fuRRx7h1KlT9OzZk+TkZJKTk3n66ac5fvw4P/30U3nHKIR4yLwQ6o2VmYaT8WlsO33N0OEYhloDvo/Ck1/pFgzptxSa9wMLe90qX3vnwvxu8HlDWPUmnPsHtPklej5doGCGsuiENFKzciuyNuIBlKnp+24OHz5MixYtih0CVZVI07cQhvfBXyf4Yfs5wuvVYOGrrQwdjvHIy4FzW+H4Mt1Y7awU/UuKjSuPZn3OhXQ1i19rRSvfGvc4kU67zzYTm5TJjy+H0K5+zfuWF+WjNHmmTHfUQghR0V5u44NGrWL7mescu5Ry/wMeFiZm4NcRenwDb525daf9Alg4kG1dmwvpasw0app7OuiGfJ3bds+OaLJAh/GTRC2EMEq1Ha14opk7AN9uizFwNEZKn7RnwttnWN9IN4y1uacDFnmpsHasbu7xeyzJKQt0GD9J1EIIo/XarQlQVh+9QlxSpoGjMXIaU7bE6+YfD6njpBubHdAHvNtAzfqF5Za9Dr/2h0OLIP2qfuKTg7E3yKuuw+GquFL1+n766afv+XpycvKDxCKEEEU09rCnrZ8z/5y+xvf/nmPik40NHZJRK9KRzL4mPDWzaIG8bDjxJ+RmwonlgIoGHi1427wua3KaER0fSuNaMu+3sSnVHbW9vf09N29vb1566aWKilUI8RAquKtesjeOGxk5Bo7GeF28kcml5Jto1Cp9c/Yd1KbQ/y/d3ONuzQAF1eX9RKp+ZaX5u9RZEAx/RuqSeVZqpcYv7q5Ud9Tz5s2rqDiEEKJYbeo508jdjhNXUvl4dRRTnw24/0EPoYLZyJrUssfG/C5/2tVq3fSktYN184+nXoEzGziz/Q/cru3EJucaHPxZt6lNwTsM/DpDixd1w8KEQcgzaiGEUVOpVLz3RCPUKli6/yJL91fMilNV3e4YXaIOLcH4aT07d2jxEgldv6NF9reMMJ0IrYaAU13dSl/ntsHGCUWPSY7VDRETlUYStRDC6IXVrcHICF2HqHeXH+VUQpqBIzI+BStmhfiUIlHfEuDpQJ7KlD/T6hMfNgGGH4BhB6DzZF3ivv1u+rcBMKUunN1cTpGL+5FELYSoEiLb16NNPWeycrUMWXiAzJxqsAxmOUlMyyLmWgYqlW5py9KyMTfB3123brV+OtEadSFsCHT6sLBgToZuDe3sVN1KYAWiVsLObyDp3INUQ9yFJGohRJWgUauY/lxzXGzNOZOYzrvLj8k6yrfsPadLrg3d7LC3Mi3TOQonPrnHSlpm1jDqJLz+L9i63RbAXFg3Fr5qDjNbwcb34eI+0Mpwr/IgiVoIUWU425jzVd9A1Cr448Alftsnz6sB9txaiKNUz6f/o8QTn6jV4PaflbYaPgF12oFKA1ej4N9p8F0H+LwBrBgG0Wt047pFmZRp9SwhhDCUVr41eLNTA6asi+a9P4/RzNOehm52hg7LoEqzEMfdBN9qMj9+OZWbOflYmhW/jGaxQgbptps34PRGiF4NZzbqVvw68KNuM7GEuu2h7mPgHQ41G+qSvrgv+SkJIaqcwY/UpV39mmTn6Z5XZ2Q/vM+rkzNzOBmv61xXlufTBTzsLXCzsyBPq3D4YnLZTmLpCM2ehWfnwdtn4cXlEPI/sPeCvJu6BL76LZgVBhf+LTwuO12aye9BErUQospRq1V80TsANzsLYq5mMG7Z0Yf2efXe87qmat+a1tS0NS/zeVQqFUG3phMtl/WpTcx0d9CPfwYjj8Dr23Vjt30f1fUirxVcWHbzJPisDuz97sGvWw1JohZCVEk1bMz5+vlANGoVyw9dZvHeOEOHZBCFz6fvv6Tl/QR53epQdv4eHcrKQqUCtya6GdFe+hPeOQdmVoWvX9oPWclg4VB038Jn4d/pcHE/5D+8rSbyjFoIUWW19HHirU4N+HTtSSasOE5AbQcaeTxcz6sLnk8/SEeyAgULdByITUarVVCrVQ98zmKp//P8e8AquHIYavgW7ovZAqfX6zYAMxvwDAWvMKjVQrdZPhzzkkuiFkJUaf9r58uec9fZHH2VyEUH+GtYm7tPoVnNpGfn6dfqfpCOZAX83e2wNNWQcjOXs1fT8XO1feBzlojGBGoH/SeYJ8HEAs7/Cxd26O64z27SbQWcfKFWUOHm1hRMLSsn5kokTd9CiCpNrVbxee/muNtbcO5aBmP/eHieV++/cAOtArUdLfFwePAEZapRE+Bprz+3QTn7QVgk9P1F11T++r/Q5VNo8owuQQMkxcDR32DtGPi+IyzqXfQcV09ViyZzSdRCiCrPydqMGc8HYqJW8dfhyyzcHWvokCpFwfPp8ribLhDsrTvXPkMn6tsVjN1u9To88z0MP6hL3i/8Du3fhfpdwdoFPAILj7l5A2a2hE+8IPu2KWez06CKfZAzikQ9c+ZMfHx8sLCwIDQ0lD179tyzfHJyMpGRkbi7u2Nubk79+vVZvXq1/vXJkyfTsmVLbG1tcXFxoUePHkRHR1d0NYQQBhTk7cQ7XRoA8MHKE/om4eqsYH7vVuXQkaxAUEknPjE0KyeoFwGPvA3PL4a3TumSdoGkGDCzBRsXML+tCX9xP5haH355Hv6Zpmtaz8ko8WUN0Vpj8ES9ZMkSRo0axYQJEzhw4AABAQF07tyZxMTEYsvn5OTQsWNHzp8/z9KlS4mOjmbu3LnUqlVLX2br1q1ERkaya9cuNmzYQG5uLp06dSIjo+RvhhCi6hnU1pcIfxdy8rRELjpAWlauoUOqMFm5+RyOK7/n0wVa3Or5HXMtg+vp2eV23gqnUumGhBWoFQRjLsDANYX7FAXij+omYoleBZveh/ndYLInzG4DK0fBoV/g2pli77q3nrpKnzm7iEuq3FnWVIqBH+aEhobSsmVLZsyYAYBWq8XT05Nhw4YxZsyYO8rPnj2bKVOmcPLkSUxNSzan7dWrV3FxcWHr1q20a9fuvuUvXryIp6cncXFx1K5du3QVEkIYVHJmDt2++pdLyTfp1tSdGc8HolJVUO9lA9p59jp95+7Cxdac3f/XoVzr2HHaVk4npjPnxSA6NXa7/wFVSe5NuHIELu4t3FIv3VnO0hFqt9RtTZ8hxcKTTtO3kpCazStt6vDeE40eKIzS5BmD3lHn5OSwf/9+IiIi9PvUajURERHs3Lmz2GNWrFhBWFgYkZGRuLq60qRJEyZNmkR+fv5dr5OSovvU6eRUfp86hRDGycHKjK9vPa9edfQKP+26YOiQKsSe26YNLe8PIgXDtPbHGnnzd1mYWoJXKLQeCr0XwKgT8MYJ6P0jtB6mG/5lYnFrOtT1sPljSI5l/IpjJKRm4+tszVudGlRqyAYdw3Dt2jXy8/NxdXUtst/V1ZWTJ08We0xMTAx///03/fr1Y/Xq1Zw5c4YhQ4aQm5vLhAkT7iiv1WoZOXIk4eHhNGnSpNhzZmdnk51d2MSTliZr3QpRlbXwcmRM14Z8tCqKj1ZGEejpSNPa9vc/sArZc/7WRCe+5fd8ukALL0d+2RPH/vPVMFEXx76Wbmv0lO77vBxIOKZbAeziHtbe8ODPQ6dRq+Dz3gGlmwe9HBj8GXVpabVaXFxcmDNnDkFBQfTp04dx48Yxe/bsYstHRkZy7NgxFi9efNdzTp48GXt7e/3WqNGDNWkIIQzvlTZ16NTIlZx8LUMW7SflZvV5Xp2Tp9UPnyqPiU7+q2CBjiOXUsjOu3trZbVlYqabUCX0NRI7zmDMqvOAbk30QK/Kn2TFoIna2dkZjUZDQkJCkf0JCQm4uRX/XMTd3Z369euj0RR+ovH39yc+Pp6cnJwiZYcOHcrKlSvZvHnzPZ8BjB07lpSUFP124sSJB6iVEMIYqFQqpjwTQG1HS+KSbjJ66ZFqM7766KUUsnK1OFqZUq+mTbmf36eGFTWszcjJ03LsUmq5n7+qUBSFMX8cJTkzl8Yedgx7zM8gcRi06dvMzIygoCA2bdpEjx49AN0d86ZNmxg6dGixx4SHh7No0SK0Wi3qW0uknTp1Cnd3d8zMdD3+FEVh2LBhLFu2jC1btlCnTp17xmFubo65eeFk9qmpD+9/TCGqE3srU2Y+34JnZu9g7fF4Wk3ehKOVGY5WZjhYmeJgZYq9pe5rx9u+drAyxeHW1xamldvMWRIFz6db+jhVyDSfKpWKFt6ObDiRwIELN/RDth42S/bG8ffJRMxM1HzRpzlmJoa5tzX4PHujRo2if//+BAcHExISwvTp08nIyGDgwIEAvPTSS9SqVYvJkycDMHjwYGbMmMGIESMYNmwYp0+fZtKkSQwfPlx/zsjISBYtWsSff/6Jra0t8fHxANjb22NpWf2mlxNC3F2ApwMTn2zM+D+Pk5CaTUJq6YYcWZiq9Um7f2sf+oZ4VVCkJVcRE538V9CtRL3vQhKD8L3/AdVM7PVMPlypa119u1MD6lfWdKrFMHii7tOnD1evXmX8+PHEx8fTvHlz1q5dq+9gFhsbq79zBvD09GTdunW88cYbNGvWjFq1ajFixAhGjx6tLzNr1iwAHn300SLXmjdvHgMGDKjwOgkhjEu/UG86NnIlPiWL5Mxckm/mkpKZw43M3Fvf55Bya/+NzMKv87UKWbla4nOziE/N4qOVJ3gywANrA84lnq9V2Herk1erCuhIViDYu2DJy2QURamWQ9zuJl+r8NZvh8nIySekjhMvt7l3q2xFM3iiBt2z5Ls1dW/ZsuWOfWFhYezateuu56suz6GEEOXHxdYCF1uLEpdXFIX07DxdIs/MZfjig5y7lsHKI5fp09Jwd9VRV1JJy87DxtwEf/eKWymsSS17zDRqrqVnE5uUiXcN6wq7lrH5/t8Y9pxPwtpMw+fPBqCpqFXESqjK9foWQojKoFKpsLUwxdPJiqa17enT0hPA4OteFyxrGezjWKEJxMJUQ5Naug8C+x6WYVpAdHwaU9edAmB890Z4Olnd54iKJ4laCCFKoFeL2pioVRyMTSY63nBzLVTG8+kCBcO0quXEJ8XIydPyxpJD5ORr6dDQhd7BnoYOCZBELYQQJVLT1pwIf13fmcV7DbM6l6Io+h7foeW4EMfdFMz7/bBMfPLVptOcuJKKo5Upk3s1NZrn8pKohRCihPqE6O6wlh28RFZu5U8EciYxnRuZuViYqmlaq+JnWisYlnUqMa1aTRhTnAOxN/hmyxkAJvVsWqr+DBVNErUQQpRQO7+aeNhbkJyZy7rj8ZV+/V237qZbeDlWypjemrbmeNewQlHgYDVu/s7MyePNXw+jVaBnYC26NnU3dEhFSKIWQogS0qhVPHvrueXiPZXfqez2hTgqS5VZn/oBfLLmJOeuZeBmZ8HEJxsbOpw7SKIWQohS6N3SE5UKdsZc5/y1ylvjXvd8uvI6khUoSNT7qmmi3nbqKj/u1K2wNuXZZthblmz55MokiVoIIUqhloMl7fxqArBkX+XdVccmZZKQmo2pRqXv5FUZgr11HwoOxSWTl6+ttOtWhpTMXN5ZegSA/mHetL31vhobSdRCCFFKfW91Klu6/yK5lZS8CsZPB9R2qNT5x/1cbLC1MCEzJ5+TBhyWVhEmrDhGfGoWvs7WjOnqb+hw7koStRBClNJjDV1xtjHjalo2m08mVso1d8dU/vNpALW68A5+3/mkSr12RVp15ArLD11GrYKpBlhjujQkUQshRCmZmajpFaRbOrcyZirLyM5j6yndB4LKTtRw27zfscmVfu2KkJiaxbvLjwIw5NF6lfoooSwkUQshRBn0udX7e0t0IldSblbotb7depZr6Tl417CidV3nCr1WcQo6lO2vBnfUBWtM38jMpZG7HcM7GGaN6dKQRC2EEGXgW9OG0DpOaBX4bd/FCrvOlZSbzPknBoAxXRoaZE3kAE8HNGoVl1OyuJxcsR9KKpp+jWmNYdeYLg3jj1AIIYzUc7c6lS3ZG4dWWzGr9k1ZF01WrpaWPo50aeJWIde4H2tzE/zddesx76/Cw7RuX2P6rc71aeBmuDWmS0MStRBClFHXJu7YWZhwKfkm/565Vu7nP3oxhT8OXALg3W6NDDr3dMEwrW+2nCXmarrB4igrRVF45/dba0z7OPFKG19Dh1RikqiFEKKMLEw19AysBejuqsuToih8tEp399ejuQcBng7lev7S6hvihZ2FCVFXUun21b8s3H0BRamYVoSKsOZYPLtikrAwVTPVCNaYLg1J1EII8QD6tPQCYP2JeK6nZ5fbedefSGD3uSTMTdS83aVhuZ23rBq42bJ2ZDta163Bzdx8xi07xqsL9nE1rfzqXFGycvOZtDoKgP+1q4tXDcOvMV0akqiFEOIBNPKwI6C2Pbn5ir6Z+kHl5Gn5ZM1JAF5tW4daDpblct4H5eFgyc+vhPJuN3/MNGo2nUyky/RtbDyRYOjQ7un7f89x8cZN3Ows+N8jVafJu4AkaiGEeEAFd9W/7I0tl+bgn3dd4Ny1DJxtzBj8aL0HPl95UqtVvNrWlxXDwmnoZsv1jBxe/XEfY/84QkZ2nqHDu0NCahYzN+uWrxzTtSFWZiYGjqj0JFELIcQDerK5B1ZmGmKuZjzw4hXJmTl8uek0AG92aoCNuXEmloZudiyPDGdQ2zqoVPDLnji6ffWP0S2HOWVdNJk5+QR6OfBUcw9Dh1MmkqiFEOIB2Zib8EQz3RrGv+yJfaBzff33GVJu5tLA1ZbetyZVMVYWphrGdWvEwldDcbe34Pz1TJ6ZvZMvNpwyigU8jlxMZul+3Rj38U8Yttf8g5BELYQQ5eC5EF3z9+qjV0i5mVumc5y/lsGPO88DMK6bf5Xpmdy6rjNrR7TjyQAP8rUKX246zTOzd3KuEpcB/S9FUfjgL12v+Z6BtQg08mlC78UoEvXMmTPx8fHBwsKC0NBQ9uzZc8/yycnJREZG4u7ujrm5OfXr12f16tUPdE4hhHgQgZ4O1He1IStXy4pDZetU9smak+TmKzxSvybt6hvnkot3Y29lyld9A/nyuebYWphwKC6Zx7/8h0W7y+e5fWmtPHKFfRduYGmqYbQR9Jp/EAZP1EuWLGHUqFFMmDCBAwcOEBAQQOfOnUlMLH5FmpycHDp27Mj58+dZunQp0dHRzJ07l1q1apX5nEII8aBUKhXP3epUVpaFOnbHXGft8XjUKt3ddFX1VPNarB3Zjla+TtzMzef/lh1l0I/7uFaOQ9fuJys3X99rfvCjdXGzt6i0a1cEgyfqadOmMWjQIAYOHEijRo2YPXs2VlZW/PDDD8WW/+GHH0hKSmL58uWEh4fj4+PDI488QkBAQJnPKYQQ5aFnYC3MNGqOX07l2KWUEh+n1Sp8fGucb98QL+q7Vo2pLe+mloMli15txbjHdcO4NkbphnFtiqqcYVxztsVwKfkmHvYWDGpb9YZj/ZdBE3VOTg779+8nIiJCv0+tVhMREcHOnTuLPWbFihWEhYURGRmJq6srTZo0YdKkSeTn55f5nEIIUR4crc3083GXplPZn4cvceRiCjbmJrzRsX5FhVep1GoVg9r58ufQcBq42nItPYdXFuxj8uqoCm0Kj0/JYtaWswCMedzfqNeZLimDJupr166Rn5+Pq6trkf2urq7Ex8cXe0xMTAxLly4lPz+f1atX89577/H555/z0Ucflfmc2dnZpKam6re0tLRyqJ0Q4mH0XEtdT+0Vhy6TmXP/ccU3c/L5bG00AEPa18XZxrxC46ts/u52/Dk0nFfb1AHg220xTF0fXWHX+2ztSW7m5hPs7Uj3Wz3xqzqDN32XllarxcXFhTlz5hAUFESfPn0YN24cs2fPLvM5J0+ejL29vX5r1KhROUYshHiYtPKtgXcNK9Ky81h15Mp9y3//bwxXUrKo5WDJy+F1KiHCymdhquHdJxrxYY8mAMzcfJZvtpwp9+scjL3BHwd1HfnGd6+6w7H+y6CJ2tnZGY1GQ0JC0ecWCQkJuLkVv5ybu7s79evXR6MpbM7w9/cnPj6enJycMp1z7NixpKSk6LcTJ048YM2EEA8rtVqlH/98v4U6EtOy+OZWM+07XRpgYVr1m2nv5cVW3oztquuB/dnaaBbsOF9u51YUhQ9uLWHZq0VtmtV2KLdzG5pBE7WZmRlBQUFs2rRJv0+r1bJp0ybCwsKKPSY8PJwzZ86g1RYOpj916hTu7u6YmZmV6Zzm5ubY2dnpN1vbqt2RQwhhWM8G1UajVrHvwg1OJ9z9UdoXG06RmZNPc08HngyomrNmldb/HqnL8Md006JOWHGc3/aVz6pjKw5f5mBsMlZmGt7p0qBczmksDN70PWrUKObOncuCBQuIiopi8ODBZGRkMHDgQABeeuklxo4dqy8/ePBgkpKSGDFiBKdOnWLVqlVMmjSJyMjIEp9TCCEqkoudBY81dAHufld9Mj5V/9p7T/hXm2baknijY319M//o34+U6BHBvWTm5OmHY0W2r4erXdUejvVfBp9Etk+fPly9epXx48cTHx9P8+bNWbt2rb4zWGxsLGp14ecJT09P1q1bxxtvvEGzZs2oVasWI0aMYPTo0SU+pxBCVLS+IZ5sOJHA7wcu8naXBpibFDZrK4rCx6ui0CrQrak7Qd5OBoy08qlUKt57wp+M7DyW7Itj5JKDWJlpaH/rw01pfbu18Dn/K22q33N+lVKVVv6uJBcvXsTT05O4uDhq165t6HCEEFVQXr6WNp9uJj41ixnPB/JEs8Km7c3RiQyct1c3xnjUI1VufeTykq9VGLnkEH8dvoy5iZr5A0MIq1ujVOe4nHyTxz7fQlaulpnPt6BbFenpXZo8Y/CmbyGEqI5MNGqeDdb9AV68p7D5Oy9fy8erdJObDAj3eWiTNIBGrWJa7wAi/F3IztPy6oK9pV5969O1J8nK1RLi48TjTYvvMFzVSaIWQogK0jvYE5UK/j1zjbikTEA3veiZxHQcrUyJbG9ca00bgqlGzYznWxBerwYZOfkMmLeXqCupJTp2/4Ub/HnoMipV9RqO9V+SqIUQooJ4OlnRpp4zoOtUlpaVyxcbTgEwMqI+9pamhgzPaFiYapjzYjAtvBxIuZnLi9/v5uzV9Hseo9UWDsd6Nqg2TWrZV0aoBiGJWgghKlDBQh2/7Y/j67/PcD0jB9+a1jwf6mXgyIyLtbkJ8waG0MjdjmvpObzw3W59K0Rxlh+6xOG4ZGzMTXirc/UajvVfkqiFEKICRTRywcnajITUbOZsiwHg/7r6Y6qRP7//ZW9pyk+vhFC3pjVXUrJ44fvdJKZm3VEuIzuPT9cWDsdysa1ew7H+S/6nCCFEBTI30dCrReEyvK3r1qCDf9mGIT0MatiYs/DVVng6WXLheib9vttNUkZOkTLfbj1LQmo2Xk5WvNzGxzCBViJJ1EIIUcH63Gr+Vt1aa7q6dnoqL272Fix6tRWuduacTkyn/w97SM3KBeDijUy+LWiZeLxhkfHp1ZUkaiGEqGD1XGz4pl8Lvn0hiMYe1bfTU3nydLJi4auhOFmbcfRSCq/M36ufgSw7T0srXyc6N66ew7H+SxK1EEJUgsebutPpIUks5aWeiy0/vhyCrYUJe8/foPe3O1l55IpuONYTjR+alglJ1EIIIYxWk1r2zB/YEiszDccu6cZXP9fSk0YedgaOrPJIohZCCGHUgrydmPtSMGYmapyszXizU/UejvVfBl+UQwghhLif8HrO7BjzGADONuYGjqZySaIWQghRJTxsCbqANH0LIYQQRkwStRBCCGHEJFELIYQQRkwStRBCCGHEJFELIYQQRkx6fRdDq9UCcOXKFQNHIoQQojoqyC8F+eZeJFEXIyEhAYCQkBADRyKEEKI6S0hIwMvr3muTqxRFUSopniojLy+PgwcP4urqilr9YE8H0tLSaNSoESdOnMDW1racIhRCCFHZyvPvuVarJSEhgcDAQExM7n3PLIm6gqWmpmJvb09KSgp2dg/P3LRCCFHdGOrvuXQmE0IIIYyYJGohhBDCiEmirmDm5uZMmDABc/OHc45aIYSoLgz191yeUQshhBBGTO6ohRBCCCMmiVoIIYQwYpKohRBCCCMmiboCzZw5Ex8fHywsLAgNDWXPnj2GDkkIIUQpbdu2je7du+Ph4YFKpWL58uWVen1J1BVkyZIljBo1igkTJnDgwAECAgLo3LkziYmJhg5NCCFEKWRkZBAQEMDMmTMNcn3p9V1BQkNDadmyJTNmzAB008V5enoybNgwxowZY+DohBBClIVKpWLZsmX06NGj0q4pd9QVICcnh/379xMREaHfp1ariYiIYOfOnQaMTAghRFUjiboCXLt2jfz8fFxdXYvsd3V1JT4+3kBRCSGEqIokUQshhBBGTBJ1BXB2dkaj0ejXtS6QkJCAm5ubgaISQghRFUmirgBmZmYEBQWxadMm/T6tVsumTZsICwszYGRCCCGqmnuvVi3KbNSoUfTv35/g4GBCQkKYPn06GRkZDBw40NChCSGEKIX09HTOnDmj//7cuXMcOnQIJycnvLy8Kvz6MjyrAs2YMYMpU6YQHx9P8+bN+eqrrwgNDTV0WEIIIUphy5YttG/f/o79/fv3Z/78+RV+fUnUQgghhBGTZ9RCCCGEEZNELYQQQhgxSdRCCCGEEZNELYQQQhgxSdRCCCGEEZNELYQQQhgxSdRCCCGEEZNELYQQQhgxSdRCiEqlUqlYvny5ocMQosqQRC3EQ2TAgAGoVKo7ti5duhg6NCHEXciiHEI8ZLp06cK8efOK7DM3NzdQNEKI+5E7aiEeMubm5ri5uRXZHB0dAV2z9KxZs+jatSuWlpb4+vqydOnSIscfPXqUxx57DEtLS2rUqMFrr71Genp6kTI//PADjRs3xtzcHHd3d4YOHVrk9WvXrtGzZ0+srKzw8/NjxYoV+tdu3LhBv379qFmzJpaWlvj5+d3xwUKIh4kkaiFEEe+99x69evXi8OHD9OvXj+eee46oqCgAMjIy6Ny5M46Ojuzdu5fffvuNjRs3FknEs2bNIjIyktdee42jR4+yYsUK6tWrV+Qa77//Pr179+bIkSM8/vjj9OvXj6SkJP31T5w4wZo1a4iKimLWrFk4OztX3g9ACGOjCCEeGv3791c0Go1ibW1dZPv4448VRVEUQHn99deLHBMaGqoMHjxYURRFmTNnjuLo6Kikp6frX1+1apWiVquV+Ph4RVEUxcPDQxk3btxdYwCUd999V/99enq6Aihr1qxRFEVRunfvrgwcOLB8KixENSDPqIV4yLRv355Zs2YV2efk5KT/OiwsrMhrYWFhHDp0CICoqCgCAgKwtrbWvx4eHo5WqyU6OhqVSsXly5fp0KHDPWNo1qyZ/mtra2vs7OxITEwEYPDgwfTq1YsDBw7QqVMnevToQevWrctUVyGqA0nUQjxkrK2t72iKLi+WlpYlKmdqalrke5VKhVarBaBr165cuHCB1atXs2HDBjp06EBkZCRTp04t93iFqArkGbUQoohdu3bd8b2/vz8A/v7+HD58mIyMDP3r27dvR61W06BBA2xtbfHx8WHTpk0PFEPNmjXp378/P//8M9OnT2fOnDkPdD4hqjK5oxbiIZOdnU18fHyRfSYmJvoOW7/99hvBwcG0adOGhQsXsmfPHr7//nsA+vXrx4QJE+jfvz8TJ07k6tWrDBs2jBdffBFXV1cAJk6cyOuvv46Liwtdu3YlLS2N7du3M2zYsBLFN378eIKCgmjcuDHZ2dmsXLlS/0FBiIeRJGohHjJr167F3d29yL4GDRpw8uRJQNcje/HixQwZMgR3d3d++eUXGjVqBICVlRXr1q1jxIgRtGzZEisrK3r16sW0adP05+rfvz9ZWVl88cUXvPXWWzg7O/PMM8+UOD4zMzPGjh3L+fPnsbS0pG3btixevLgcai5E1aRSFEUxdBBCCOOgUqlYtmwZPXr0MHQoQohb5Bm1EEIIYcQkUQshhBBGTJ5RCyH05EmYEMZH7qiFEEIIIyaJWgghhDBikqiFEEIIIyaJWgghhDBikqiFEEIIIyaJWgghhDBikqiFEEIIIyaJWgghhDBikqiFEEIII/b/ejo2wAhfdUIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(tracking[\"train_losses\"]))\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking[\"tokens_seen\"],\n",
    "    train_losses=tracking[\"train_losses\"],\n",
    "    val_losses=tracking[\"val_losses\"],\n",
    "    label=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bc233-895f-46d5-8e01-202b991cd60c",
   "metadata": {
    "id": "7f8bc233-895f-46d5-8e01-202b991cd60c"
   },
   "source": [
    "- **如上所示，损失（Loss）持续下降，这是一个积极的信号**。  \n",
    "- **从下降趋势来看**，可能会有 **进一步训练的空间**，  \n",
    "  **（建议读者可以尝试继续训练）**，  \n",
    "  **但需要注意 DPO 可能存在“塌陷”（Collapse）风险**，  \n",
    "  **即模型可能会开始生成无意义的响应**。  \n",
    "- **接下来，我们来看奖励边际（Reward Margins）：**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dmbq6ruuf0Cl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "dmbq6ruuf0Cl",
    "outputId": "c2886c16-57da-41bd-c9f0-e936da9d9e4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_reward_margins = [i-j for i,j in zip(tracking[\"train_chosen_rewards\"], tracking[\"train_rejected_rewards\"])]\n",
    "val_reward_margins = [i-j for i,j in zip(tracking[\"val_chosen_rewards\"], tracking[\"val_rejected_rewards\"])]\n",
    "\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking[\"tokens_seen\"],\n",
    "    train_losses=train_reward_margins,\n",
    "    val_losses=val_reward_margins,\n",
    "    label=\"reward margins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69756011-acd6-404c-a5fc-7fe252cf20c8",
   "metadata": {
    "id": "69756011-acd6-404c-a5fc-7fe252cf20c8"
   },
   "source": [
    "- **可以看到，奖励边际（Reward Margins）在优化过程中逐步提升**，  \n",
    "  **这与损失曲线（Loss Curve）相呼应，是一个积极的信号**。  \n",
    "- **需要注意的是**，虽然 **DPO 损失（Loss）和奖励边际（Reward Margins）**  \n",
    "  **是训练过程中重要的衡量指标，但它们不能完全反映优化效果**。  \n",
    "- **最关键的一步** 是 **对生成的响应进行定性分析（Qualitative Evaluation）**。  \n",
    "- **在此，我们直接查看模型的响应质量**，  \n",
    "  **此外，还可以像第 7 章一样使用 LLM 进行自动评分**，  \n",
    "  **进一步量化模型改进情况**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5EfUXJGOali8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EfUXJGOali8",
    "outputId": "7ec7db47-d775-4646-f660-0d7f7e7c8503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "Correct response:\n",
      ">> The meal is cooked by the chef every day.\n",
      "\n",
      "Reference model response:\n",
      ">> The meal is cooked every day by the chef.\n",
      "\n",
      "Policy model response:\n",
      ">> The meal is prepared by the chef.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify an input string as either a noun or a verb.\n",
      "\n",
      "### Input:\n",
      "Dance\n",
      "\n",
      "Correct response:\n",
      ">> 'Dance' can be classified as a verb.\n",
      "\n",
      "Reference model response:\n",
      ">> \"Dance\" can be classified as a verb.\n",
      "\n",
      "Policy model response:\n",
      ">> The input string \"Dance\" could be classified as a verb.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a metaphor.\n",
      "\n",
      "### Input:\n",
      "The book is very interesting.\n",
      "\n",
      "Correct response:\n",
      ">> The book is a page-turner.\n",
      "\n",
      "Reference model response:\n",
      ">> The book is a treat.\n",
      "\n",
      "Policy model response:\n",
      ">> The book is a treat.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in val_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    reference_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nReference model response:\\n>> {reference_response_text.strip()}\")\n",
    "    print(f\"\\nPolicy model response:\\n>> {policy_response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RmcKVg0JlHVF",
   "metadata": {
    "id": "RmcKVg0JlHVF"
   },
   "source": [
    "- 从 **参考模型（Reference Model）** 和 **政策模型（Policy Model）** 的响应对比来看，  \n",
    "  **优化后的策略模型** **在风格上确实发生了微调**，相较于 **原始参考模型** 更贴近偏好。  \n",
    "- 例如，  \n",
    "  - **原始参考模型** 的响应：  \n",
    "    `\"Dance\" can be classified as a verb.`  \n",
    "  - **优化后的策略模型** 变为：  \n",
    "    `\"The input string 'Dance' could be classified as a verb.\"`  \n",
    "  - **优化点**：\n",
    "    - **更礼貌**：使用 `\"could\"` 替代 `\"can\"`，  \n",
    "      **使语气更委婉，更符合人类偏好**（减少了过于武断的表述）。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "jJSwb2hzQwdP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJSwb2hzQwdP",
    "outputId": "6e755db4-9524-42a8-a58b-2218bf03e39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Reference model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Policy model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Reference model response:\n",
      ">> A thunderstorm is a type of storm that typically produces thunder or lightning.\n",
      "\n",
      "Policy model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Reference model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Policy model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    reference_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nReference model response:\\n>> {reference_response_text.strip()}\")\n",
    "    print(f\"\\nPolicy model response:\\n>> {policy_response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/05_dataset-generation/llama3-ollama.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用 LLaMA 3 和 Ollama 生成指令数据集  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- 本笔记本使用 **Ollama 提供的 80 亿参数 LLaMA 3 模型** 生成 **合成数据集**，方法基于 **“Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing”** 论文（[https://arxiv.org/abs/2406.08464](https://arxiv.org/abs/2406.08464)）。  \n",
    "\n",
    "- 生成的数据集将采用 **指令数据集格式**，包含 `\"instruction\"` 和 `\"output\"` 字段，类似于 **Alpaca 数据集**：  \n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"What is the atomic number of helium?\",\n",
    "    \"output\": \"The atomic number of helium is 2.\",\n",
    "},\n",
    "```\n",
    "\n",
    "- **该代码无需 GPU**，可直接在 **笔记本电脑** 上运行（已在 **M3 MacBook Air** 上测试）。  \n",
    "\n",
    "*请注意，本示例生成的指令数据集仅用于**教学目的**。然而，**用户有责任** 确保其使用符合 **Meta AI LLaMA 3** 相关许可协议的规定。*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tqdm\",    # Progress bar\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## 安装 Ollama 并下载 LLaMA 3\n",
    "\n",
    "- **Ollama** 是一个高效运行 **LLM**（大语言模型）的应用。  \n",
    "- 它是 **[llama.cpp](https://github.com/ggerganov/llama.cpp)** 的封装，后者使用 **纯 C/C++ 实现 LLM**，以 **最大化推理效率**。  \n",
    "- **请注意**：Ollama 仅用于 **推理（inference）**，**不支持训练或微调（finetuning）LLM**。  \n",
    "- 在运行下方代码之前，请先访问 **[https://ollama.com](https://ollama.com)** 并按照安装指南完成 **Ollama 的安装**（例如，点击 **“Download”** 按钮并下载适用于您操作系统的 Ollama 应用）。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- **对于 macOS 和 Windows 用户**，点击 **下载的 Ollama 应用**，如果系统提示安装 **命令行工具**，请选择 **“是”**。  \n",
    "- **Linux 用户** 可以使用 **Ollama 官网提供的安装命令** 进行安装。  \n",
    "\n",
    "- **一般来说**，在使用 **Ollama 命令行工具** 之前，需要 **启动 Ollama 应用** 或 **在终端中运行 `ollama serve`**。  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/ch7/21.png\">\n",
    "\n",
    "- **确保 Ollama 运行后**，在 **另一个终端窗口** 执行以下命令，尝试 **8B 参数的 LLaMA 3 模型**（首次执行时，模型将自动下载，占用 **4.7GB 存储空间**）：  \n",
    "\n",
    "\n",
    "```bash\n",
    "# 8B model\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "\n",
    "他的输出如下所示\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest \n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
    "verifying sha256 digest \n",
    "writing manifest \n",
    "removing any unused layers \n",
    "success \n",
    "```\n",
    "\n",
    "- **注意**：`llama3` 指的是 **指令微调后的 80 亿参数 LLaMA 3 模型**。  \n",
    "\n",
    "- **如果您的设备支持**，可以将 `llama3` 替换为 **`llama3:70b`**，以使用 **更大的 700 亿参数 LLaMA 3 模型**。  \n",
    "\n",
    "- **下载完成后**，您将看到 **命令行交互界面**，可以在其中与模型进行对话。  \n",
    "\n",
    "- **尝试输入以下提示**：\"What do llamas eat?\"（羊驼吃什么？），模型的输出应类似如下：  \n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered \n",
    "stomach and eat plants that are high in fiber. In the wild, llamas \n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall \n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
   "metadata": {},
   "source": [
    "- 通过输入`/bye`终止程序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
   "metadata": {},
   "source": [
    "## 使用Ollama's REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": [
    "- 另一种与模型交互的方式是 **通过 Python 调用其 REST API**，可以使用以下函数实现。  \n",
    "- **在运行本笔记本中的代码前**，请确保 **Ollama 仍在运行**，可以通过以下方式启动：\n",
    "  - 在终端中执行 `ollama serve`\n",
    "  - 使用 **Ollama 应用程序**  \n",
    "\n",
    "- 接下来，运行下面的代码单元 **查询模型**。  \n",
    "\n",
    "- **首先，我们用一个简单的示例测试 API**，以确保其 **正常运行**：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def query_model(prompt, model=\"llama3\", url=\"http://localhost:11434/api/chat\", role=\"user\"):\n",
    "    # 创建数据负载作为字典\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"seed\": 123,        # 用于生成确定性响应\n",
    "        \"temperature\": 1.,   # 用于生成确定性响应\n",
    "        \"top_p\": 1,         \n",
    "        \"messages\": [\n",
    "            {\"role\": role, \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 将字典转换为JSON格式的字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # 创建请求对象，设置方法为POST并添加必要的头信息\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并捕获响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb61a4e-2706-431a-835e-7e472b42989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: They enjoy eating hay, such as alfalfa or timothy hay, which provides them with fiber, protein, and other essential nutrients.\n",
      "3. Grains: Llamas may eat grains like oats, barley, or corn as a supplement to their diet.\n",
      "4. Leaves: They will also munch on leaves from trees and shrubs, including clover, alfalfa, and various types of leaves.\n",
      "5. Fruits and vegetables: In the wild, llamas might eat fruits and vegetables that grow in their natural habitat, such as apples, carrots, or potatoes.\n",
      "\n",
      "In general, a llama's diet should consist of:\n",
      "\n",
      "* 50% grasses and hay\n",
      "* 20% grains (like oats or corn)\n",
      "* 10% leaves and other plant material\n",
      "* 5% fruits and vegetables (as treats)\n",
      "\n",
      "It's essential to provide llamas with a balanced diet that meets their nutritional needs, as they can be prone to health issues if they don't receive the right combination of nutrients.\n"
     ]
    }
   ],
   "source": [
    "result = query_model(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c079c6c-5845-4b31-a648-060d0273cd1d",
   "metadata": {},
   "source": [
    "## 提取指令（Extract Instructions）\n",
    "\n",
    "- 现在，让我们使用论文中提出的 **“巧妙方法”**：提供 **空的提示模板** `\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"`，  \n",
    "  这样 **指令微调后的 LLaMA 3 模型** 就会 **自动生成一条指令**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7320a41-ed86-49e9-8eb1-5d609a82ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_instruction(text):\n",
    "    for content in text.split(\"\\n\"):\n",
    "        if content:\n",
    "            return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc41b72f-a8cf-4367-b0ca-0bf8c1f094fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying to find a way to make my child's birthday party more special and unique. What are some creative ideas you have?\n"
     ]
    }
   ],
   "source": [
    "query = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "\n",
    "result = query_model(query, role=\"assistant\")\n",
    "instruction = extract_instruction(result)\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d04ba7-bffc-47f0-87dc-d60fc676b14a",
   "metadata": {},
   "source": [
    "- 如上所示，模型 **准确地生成了一条指令**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 响应生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542f8d3-2db2-4a89-ae50-8825eb19d3b6",
   "metadata": {},
   "source": [
    "- 接下来，我们需要 **生成对应的响应**\n",
    "- 这可以 **直接将指令作为输入** 传递给模型完成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2349eb06-710f-4459-8a03-1a3b2e1e8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an exciting question! I'd be delighted to help you come up with some creative and unique ideas to make your child's birthday party truly special!\n",
      "\n",
      "Here are a few ideas to get you started:\n",
      "\n",
      "1. **Themed Scavenger Hunt**: Plan a scavenger hunt based on the birthday child's favorite theme (e.g., superheroes, animals, or princesses). Hide clues and challenges throughout the party area, leading up to a final surprise.\n",
      "2. **DIY Crafts Station**: Set up a craft station where kids can create their own party favors, such as customized t-shirts, crowns, or jewelry. This activity encourages creativity and makes for a memorable keepsake.\n",
      "3. **Mystery Box Challenge**: Fill mystery boxes with different textures, smells, and sounds. Have the kids guess what's inside each box without looking. This game promotes problem-solving and teamwork.\n",
      "4. **Indoor Camping Adventure**: Set up a cozy indoor \"camping\" area with sleeping bags, flashlights, and s'mores-making stations. Kids can enjoy a camping experience without leaving the party location.\n",
      "5. **Personalized Photo Booth**: Create a customized photo booth with props and backdrops that match the birthday child's theme. This activity allows kids to take home special memories and share them on social media.\n",
      "6. **Foodie Fun**: Plan a cooking or baking station where kids can make their own treats, such as cupcakes, pizzas, or trail mix. This activity teaches valuable skills and lets kids enjoy their creations.\n",
      "7. **Outdoor Movie Night**: Set up an outdoor movie screen (or projector) with cozy seating and snacks. Screen the birthday child's favorite film or a classic kid-friendly movie.\n",
      "8. **Science Experiments**: Host a science-themed party where kids can conduct fun experiments, such as making slime, creating lava lamps, or growing crystals.\n",
      "9. **Karaoke Contest**: Set up a karaoke machine with popular kids' songs and have a singing competition. Offer prizes for the best performances, and provide fun props like microphones and costumes.\n",
      "10. **Time Capsule Ceremony**: Have each guest bring a small item that represents their favorite memory or something they're looking forward to in the future. Bury the time capsule together as a group, with instructions to open it on a specific date (e.g., next year's birthday party).\n",
      "11. **Special Guest Appearance**: Arrange for a special guest, such as a superhero, princess, or even a real-life animal (if feasible), to make an appearance at the party.\n",
      "12. **Customized Games**: Design custom games and activities that fit the birthday child's interests and personality. This could include a customized version of a favorite game or a new game altogether.\n",
      "\n",
      "Remember, the key to making your child's birthday party unique is to incorporate elements that reflect their personality and interests. Mix and match these ideas or come up with something entirely new – the possibilities are endless!\n",
      "\n",
      "What do you think? Is there anything in particular that resonates with you, or would you like more suggestions?\n"
     ]
    }
   ],
   "source": [
    "response = query_model(instruction, role=\"user\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cf92c-3272-4b36-ae30-d1135af56328",
   "metadata": {},
   "source": [
    "## 生成数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470037f3-64f4-4465-9f00-55b69e883a04",
   "metadata": {},
   "source": [
    "- 我们可以 **扩展此方法** 以处理 **任意数量的数据样本**（建议使用 **额外的筛选机制**，例如通过 **另一个 LLM 评估数据质量** 或 **限制数据长度**）。  \n",
    "- **下面的示例** 生成 **5 组合成的指令-响应对**，在 **M3 MacBook Air** 上 **约需 3 分钟**。  \n",
    "- **如果要构建可用于指令微调的数据集**，建议将数据量扩展至 **1k-50k**，并 **使用 GPU 加速数据生成**。  \n",
    "\n",
    "**提示（Tip）**  \n",
    "- 您可以通过将 `model=\"llama3\"` **更改为** `model=\"llama3:70b\"` 来生成 **更高质量的响应**，但这将 **需要更多的计算资源**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9e94ab-02ef-4372-91cd-60128159fd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [02:37<00:00, 31.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dataset_size = 5\n",
    "dataset = []\n",
    "\n",
    "for i in tqdm(range(dataset_size)):\n",
    "\n",
    "    result = query_model(query, role=\"assistant\")\n",
    "    instruction = extract_instruction(result)\n",
    "    response = query_model(instruction, role=\"user\")\n",
    "    entry = {\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": response\n",
    "    }\n",
    "    dataset.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fdbc194-c12a-4138-96d1-51bf66ca1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-llama3-7b.json\", \"w\") as file:\n",
    "    json.dump(dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4027ead-bba4-49b7-9965-47532c3fdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"instruction\": \"What is the significance of the number 7 in various cultures and religions?\",\n",
      "        \"output\": \"The number 7 has been a significant and recurring theme across many cultures and religions, often imbuing it with special meaning and symbolism. Here are some examples:\\n\\n1. **Numerology**: In numerology, the number 7 is considered sacred and mystical, associated with spiritual awakening, introspection, and enlightenment.\\n2. **Judaism**: The Torah has seven days of creation, seven weeks in the wilderness, and seven years of rest (Sabbatical year). Seven is also a symbol of completion or perfection.\\n3. **Christianity**: In Christianity, there are seven deadly sins, seven virtues, and seven sacraments. Jesus was said to have spoken seven sermons, and the number 7 appears in various biblical accounts, such as the seven days of creation and the seven angels who appear before God.\\n4. **Islam**: In Islamic tradition, there are seven heavens, seven earths, and seven veils that separate the physical world from the divine realm. The Quran mentions seven verses (Ayats) that were revealed to Muhammad in a single revelation.\\n5. **Hinduism**: In Hindu mythology, there are seven chakras (energy centers) in the human body, seven colors of the rainbow, and seven planets (including the sun and moon). The number 7 is also associated with the seven aspects of the divine feminine (Saptamatrikas).\\n6. **Buddhism**: In Buddhist cosmology, there are seven levels of existence (dhatus) and seven stages of enlightenment (bodhisattva path). The seven days of meditation retreats in Tibetan Buddhism are also a nod to this number.\\n7. **Ancient Greek and Roman cultures**: The number 7 was considered perfect or complete, with seven being the number of planets known at the time (including Earth, Sun, Moon, Mars, Mercury, Jupiter, and Venus). Seven days were also the typical duration of a week in these cultures.\\n8. **Chinese culture**: In Chinese astrology, there are seven celestial stems and 12 earthly branches, creating a system of cyclical time reckoning. The number 7 is also associated with good luck and prosperity.\\n9. **African cultures**: In some African traditions, the number 7 is associated with spiritual powers, such as divination and healing. Seven is also considered a sacred number in Yoruba culture, representing the seven orishas (deities).\\n10. **Esoteric traditions**: Many esoteric traditions, including Hermeticism, Gnosticism, and Theosophy, have employed the number 7 to represent spiritual principles, such as the seven planes of existence, seven chakras, or seven sacred seals.\\n\\nThese examples illustrate the widespread significance of the number 7 across various cultures and religions. While the specific meanings may vary, the number 7 often represents completion, perfection, spiritual growth, and mystical connections.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"I've been wondering, what is the most important thing you can learn from a child?\",\n",
      "        \"output\": \"What a great question!\\n\\nChildren have so much to teach us, and I think one of the most important things we can learn from them is the power of simplicity, curiosity, and joy. Children are often unencumbered by the complexities and worries that come with adulthood, and they approach life with a fresh perspective.\\n\\nHere are some specific things children can teach us:\\n\\n1. **Simplicity**: Children are masters of simplifying things. They don't get bogged down in details or worry about what others think. They just enjoy the moment and find joy in simple things like playing outside, drawing, or reading.\\n2. **Curiosity**: Kids are naturally curious and love to explore their world. They ask questions, investigate, and learn from experience. This curiosity can be a powerful reminder to us adults to stay open-minded and seek new knowledge and experiences.\\n3. **Joy and playfulness**: Children have an amazing capacity for joy and playfulness. They find happiness in the simplest things, like playing with blocks or having a water balloon fight. We could all benefit from embracing our inner child and finding more joy in life's simple pleasures.\\n4. **Unconditional love**: Children are capable of loving unconditionally, without judgment or expectation. This can be a powerful reminder to us adults to practice self-love, self-acceptance, and kindness towards others.\\n5. **Creativity and imagination**: Kids are incredibly creative and imaginative, often thinking outside the box and coming up with innovative solutions. This creativity can inspire us adults to think differently, try new things, and approach problems from unique angles.\\n6. **Resilience**: Children are surprisingly resilient in the face of challenges and setbacks. They learn to adapt, cope, and move forward, teaching us valuable lessons about perseverance and bouncing back from adversity.\\n\\nIn summary, learning from children can help us regain a sense of simplicity, curiosity, joy, and playfulness, while also reminding us of the importance of unconditional love, creativity, and resilience.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": null,\n",
      "        \"output\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"What is the best way to deal with a difficult person?\",\n",
      "        \"output\": \"Dealing with a difficult person can be challenging, but there are strategies that can help you navigate the situation effectively. Here are some tips:\\n\\n1. **Stay calm**: Take a deep breath and try not to take their behavior personally. A calm demeanor can help de-escalate tensions and prevent misunderstandings.\\n2. **Listen actively**: Sometimes, people act out because they feel unheard or misunderstood. Make an effort to listen carefully to what they're saying, and respond thoughtfully.\\n3. **Set boundaries**: Establish clear limits on what you are and aren't willing to engage in. Be firm but respectful when communicating your needs.\\n4. **Avoid taking the bait**: Don't let their provocations get under your skin. Stay focused on the issue at hand and avoid getting drawn into an argument or debate.\\n5. **Use \\\"I\\\" statements**: When expressing yourself, use \\\"I\\\" statements instead of \\\"you\\\" statements, which can come across as accusatory. This helps to reduce defensiveness and promotes a more constructive conversation.\\n6. **Practice empathy**: Try to understand where the other person is coming from, even if you don't agree with their perspective. Showing that you care about their feelings can help diffuse tension.\\n7. **Don't take it personally**: Remember that the difficult person's behavior is often a reflection of themselves, not you. Keep your self-worth and confidence intact.\\n8. **Seek common ground**: Look for areas of agreement or shared interests. This can help to build bridges and create a more positive atmosphere.\\n9. **Use humor (carefully)**: A well-timed, lighthearted joke or witty remark can help diffuse tension and lighten the mood. However, be cautious not to offend or make light of serious issues.\\n10. **Know when to disengage**: If the situation becomes too heated or toxic, it may be necessary to take a step back and re-engage at a later time when emotions have cooled down.\\n11. **Seek support**: Don't be afraid to ask for help from friends, family, or a professional if you're struggling to manage your interactions with a difficult person.\\n12. **Practice self-care**: Take care of yourself physically, emotionally, and mentally. Engage in activities that bring you joy and help you maintain your energy and resilience.\\n\\nRemember, dealing with a difficult person is not about winning an argument or changing their behavior; it's about maintaining your own emotional well-being and responding constructively to the situation.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"I'm looking for a way to get my cat's attention when they're hiding under the bed or in a closet.\",\n",
      "        \"output\": \"The classic \\\"where'd my cat go?\\\" conundrum! Don't worry, I've got some tips to help you coax your kitty out from their hiding spots:\\n\\n1. **Use their favorite treats**: Cats love food, and familiar treats can be a powerful lure. Try calling your cat's name and saying \\\"treat time\\\" in a playful tone. This might encourage them to emerge and investigate.\\n2. **Make some noise**: Cats have poor eyesight but excellent hearing. Gently knock on the bed frame or closet door with your knuckles, making a soft, rhythmic sound. This can help your cat pinpoint where you are and entice them to come out.\\n3. **Speak softly and calmly**: When speaking to your cat, use a gentle, soothing tone. Avoid loud or harsh voices, as these might scare them even further into hiding.\\n4. **Use verbal cues**: Establish a consistent verbal cue, like \\\"come on out\\\" or \\\"let's play,\\\" which can become associated with the idea of leaving their hiding spot.\\n5. **Create a \\\"safe zone\\\"**: If your cat is hiding due to fear or anxiety (e.g., from loud noises or other pets), try creating a safe, comfortable space for them to emerge into. This could be a cozy blanket or a familiar toy.\\n6. **Patiently wait it out**: Sometimes, cats just need time and space to feel secure enough to come out. Give your cat the opportunity to leave their hiding spot at their own pace.\\n7. **Use a flashlight (carefully)**: If your cat is hiding in a dark space, try using a flashlight to create a gentle beam of light. Be cautious not to shine it directly into their eyes, as this could startle them further.\\n8. **Offer a familiar object**: Place a familiar toy or blanket near the entrance to the hiding spot, which can help your cat feel more comfortable coming out.\\n9. **Make the space inviting**: If your cat is hiding under the bed, try moving any clutter or dust bunnies away from the area. Make the space underneath the bed a pleasant place for them to emerge into.\\n10. **Be patient and don't force it**: Respect your cat's boundaries and allow them to come out when they're ready. Forcing them to leave their hiding spot can create negative associations and make them more likely to hide in the future.\\n\\nRemember, every cat is different, so try a combination of these methods to see what works best for your feline friend.\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat instruction-data-llama3-7b.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch07/05_dataset-generation/reflection-gpt4.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用 GPT-4 进行指令数据优化：基于 Reflection-Tuning 方法  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- 本笔记本使用 **OpenAI 的 GPT-4 API** 来实现 **[Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning](https://arxiv.org/abs/2310.11716)** 论文中的 **数据优化方法**。\n",
    "\n",
    "![](../image/reflection-tuning.webp)\n",
    "\n",
    "- 在 **原始论文** 中，研究人员优化了 **[Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)** 和 **[WizardLM](https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k)** 指令微调数据集。  \n",
    "- 在本笔记本中，我们优化的是 **[第 7 章使用的指令数据集](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)**。  \n",
    "  - **由于其格式与 Alpaca 数据集相同**，本代码同样适用于 **Alpaca 数据集**。  \n",
    "\n",
    "- **期望的数据集格式如下**：\n",
    "\n",
    "\n",
    "```python\n",
    "    {\n",
    "        \"instruction\": \"Edit the following sentence for grammar.\",\n",
    "        \"input\": \"He go to the park every day.\",\n",
    "        \"output\": \"He goes to the park every day.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert 45 kilometers to meters.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"45 kilometers is 45000 meters.\"\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac82b4-e3d5-4ed5-8f46-6c97a9313463",
   "metadata": {},
   "source": [
    "> **请注意**：本笔记本复现了论文中的方法，即使用 **GPT API** 来优化现有数据集。然而，需要注意的是，根据 **[OpenAI 使用条款](https://openai.com/policies/row-terms-of-use/)**，**禁止使用 GPT API 生成的数据来开发与 OpenAI 竞争的模型**。  \n",
    "> **具体条款**：  \n",
    "> *“What you cannot do... Use Output to develop models that compete with OpenAI.”*  \n",
    "> **相关讨论** 可参考 [Reddit 论坛](https://www.reddit.com/r/LocalLLaMA/comments/17vbg1f/does_openai_tos_prohibit_generating_datasets_for/))。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 1.30.3\n",
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"openai\",  # OpenAI API\n",
    "    \"tqdm\",    # 进度条\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## 测试OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- 首先，让我们测试 **OpenAI API 是否正确配置**。  \n",
    "- **如果您还没有账户**，请前往 [OpenAI 平台](https://platform.openai.com/) **注册**。  \n",
    "- **请注意**，您需要 **向账户充值**，因为 **GPT-4 API 不是免费的**（详见 [计费页面](https://platform.openai.com/settings/organization/billing/overview)）。  \n",
    "\n",
    "- **首先，我们需要提供 OpenAI API 密钥**，该密钥可在 [API Keys 页面](https://platform.openai.com/api-keys) 获取。  \n",
    "- **请勿与他人分享您的 API 密钥**。  \n",
    "- **将 API 密钥（`\"sk-...\"`）添加到本文件夹中的 `config.json` 文件中**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# 从JSON文件中加载API密钥\n",
    "# 请确保将\"sk-...\"替换为您在https://platform.openai.com/api-keys上的实际API密钥\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- 首先，我们使用一个 **简单示例** 进行 API 调用，以确保其 **正常运行**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e9ef2e-e816-4283-840e-43625791ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4o-mini\", system_prompt=None):\n",
    "    # 如果提供了系统提示，则定义系统消息\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # 将用户提示添加到消息中\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 调用API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "    \n",
    "    # 返回模型的响应\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## 加载 JSON 数据（Load JSON Entries）\n",
    "\n",
    "- 接下来，我们将 **加载并处理指令数据集**。  \n",
    "- 这里假设我们已将 **测试数据集** 及 **模型生成的响应** 以 **JSON 文件** 形式保存，并可按以下方式加载：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "json_file = Path(\"..\") / \"01_main-chapter-code\" / \"instruction-data.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- 让我们打印 **数据集中的一个条目**，以查看其结构：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce187422-a4e6-4f3c-b0d1-b03257f5bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Evaluate the following phrase by transforming it into the '\n",
      "                'spelling given.',\n",
      " 'input': 'freind --> friend',\n",
      " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pp as pprint\n",
    "\n",
    "pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce41e0-433f-49aa-82b7-a9d1a1d41604",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 优化指令（Improve Instructions）\n",
    "\n",
    "- **Reflection-Tuning** 作者提出了 **两种优化方法**：\n",
    "  1. **优化指令（Instructions）**\n",
    "  2. **优化响应（Responses）**  \n",
    "- 让我们首先 **优化数据集中的指令**。  \n",
    "- 下面是 **[Reflection-Tuning 仓库](https://github.com/tianyi-lab/Reflection_Tuning/blob/main/reflection_code/reflect_response.py)** 提供的 **工具函数**，用于 **格式化输入** 以适配 **GPT-4** 进行数据优化。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d28ada-9e1a-4818-8a49-82be44141533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instr_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of a given instruction. \\n\" + \\\n",
    "                \"1. Why this instruction is not good? First analyse the instruction based on Complexity of the Topic, Level of Detail Required, Knowledge Required, Ambiguity of the Instruction and Logical Reasoning or Problem-Solving Involved. \\n\" + \\\n",
    "                \"Then analyse why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"Finally analyse why this bad instruction lead to a bad answer. \" +\\\n",
    "                \"2. Based on the reason you provided, generate a new and complete instruction which is complex and difficult to answer directly. \" + \\\n",
    "                \"Make sure the new instruction is relevent but independent to the original instruction, which can be answered without knowing the original instruction, put the new instruction in the format of [New Instruction] your instruction [End]\" +\\\n",
    "                \"3. Answer the newly generated instruction as detailed as possible, in the format of [New Answer] your answer [End] \\n\"\n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd8a16-3f96-4662-b8c3-97dced794c6c",
   "metadata": {},
   "source": [
    "- 让我们以 **数据集条目 `json_data[2]`** 为例，查看优化过程。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203807bf-9a76-4e12-b801-b4ae518f30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Convert 45 kilometers to meters.',\n",
      " 'input': '',\n",
      " 'output': '45 kilometers is 45000 meters.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(json_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572a1aa-532a-4a76-9fa3-3b59d996ba13",
   "metadata": {},
   "source": [
    "- 可以使用 **上述 `instr_prompt_no_input` 函数** 来优化指令，如下所示：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a0a76-cc22-4bda-ae26-4b2540afb4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Analysis of the Instruction:**\n",
      "\n",
      "   - **Complexity of the Topic:** The topic of converting kilometers to meters is relatively simple and straightforward, as it involves basic unit conversion.\n",
      "   - **Level of Detail Required:** The instruction does not require much detail; it simply asks for a conversion without any additional context or explanation.\n",
      "   - **Knowledge Required:** Basic knowledge of metric units and their conversions is required, which is common knowledge.\n",
      "   - **Ambiguity of the Instruction:** The instruction is clear and unambiguous; it specifies exactly what needs to be converted.\n",
      "   - **Logical Reasoning or Problem-Solving Involved:** There is minimal logical reasoning involved, as the conversion factor (1 kilometer = 1000 meters) is a standard fact.\n",
      "\n",
      "   **Analysis of the Answer:**\n",
      "\n",
      "   - **Helpfulness:** The answer is helpful in that it provides the correct conversion.\n",
      "   - **Relevance:** The answer is relevant to the instruction, as it directly addresses the conversion requested.\n",
      "   - **Accuracy:** The answer is accurate; 45 kilometers does indeed equal 45,000 meters.\n",
      "   - **Level of Details:** The answer lacks detail. It does not explain the conversion process or provide any context, which could be beneficial for someone unfamiliar with metric conversions.\n",
      "\n",
      "   **Why the Bad Instruction Leads to a Bad Answer:** While the instruction itself is not bad, the simplicity of the task may lead to a lack of depth in the answer. The answer could have been improved by including an explanation of the conversion process, which would enhance understanding.\n",
      "\n",
      "2. **New Instruction:**\n",
      "   [New Instruction] Explain the significance of the metric system in global trade and provide examples of how unit conversions can impact international business transactions. [End]\n",
      "\n",
      "3. **New Answer:**\n",
      "   [New Answer] The metric system, also known as the International System of Units (SI), is a decimal-based system of measurement that is used globally. Its significance in global trade lies in its standardization, which facilitates international communication and commerce. \n",
      "\n",
      "   One of the primary advantages of the metric system is that it is universally recognized, which reduces confusion and errors in measurement. For example, when a company in the United States imports goods from Europe, the specifications for those goods are often provided in metric units. If the U.S. company is accustomed to using imperial units (like inches or pounds), they must convert these measurements to ensure compatibility. \n",
      "\n",
      "   Unit conversions can significantly impact international business transactions. For instance, if a manufacturer orders 100 kilograms of a product but mistakenly interprets it as 100 pounds, they will receive a much smaller quantity than intended, leading to production delays and financial losses. \n",
      "\n",
      "   Additionally, in industries such as pharmaceuticals, precise measurements are critical. A dosage specified in milligrams must be accurately converted to ensure patient safety. \n",
      "\n",
      "   In summary, the metric system's role in global trade is crucial for maintaining consistency and accuracy in measurements, which ultimately supports efficient and effective international business operations. [End]\n"
     ]
    }
   ],
   "source": [
    "entry = json_data[2]\n",
    "\n",
    "system_prompt, prompt = instr_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc4575-caae-45cc-bf9c-fde9322cf3df",
   "metadata": {},
   "source": [
    "- **生成的响应较为冗长**，这对于 **分析** 很有帮助，同时也能利用 **思维链（Chain-of-Thought）提示** 让 **GPT-4** 进行更有效的优化。  \n",
    "- 但是，在构造 **优化后数据集** 时，我们实际只需要 **新的指令和输出**，而 **不需要分析部分**。  \n",
    "- 我们可以使用 **[Reflection-Tuning 仓库](https://github.com/tianyi-lab/Reflection_Tuning/blob/main/reflection_code/reflect_response.py)** 中的 **工具代码**，提取 **模型优化后的指令和输出**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38d406-69a5-448a-8d20-bd48c47eb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ins(text, no_input=True):\n",
    "    if '[New Instruction]' in text:\n",
    "        pattern = r'(\\[New Instruction\\])(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    else:\n",
    "        pattern = r'(New Instruction:)(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_ins = ''\n",
    "    else:\n",
    "        seg_ins = segments[0][1].strip()\n",
    "    if seg_ins.endswith(\"\\n\\n3.\"):\n",
    "        seg_ins = seg_ins[:-4]\n",
    "    return seg_ins\n",
    "\n",
    "\n",
    "def extract_oup(text, no_input=True):\n",
    "    if '[New Answer]' in text:\n",
    "        pattern = r'(\\[New Answer\\])(.*?)(\\[End\\]|$)'\n",
    "    else:\n",
    "        pattern = r'(New Answer:)(.*?)(\\[End\\]|$)'\n",
    "        # pattern = r'(\\[New Answer\\]|New Answer:)(.*?)(\\[End\\]|$)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_oup = ''\n",
    "    else:\n",
    "        seg_oup = segments[0][1].strip()\n",
    "    return seg_oup\n",
    "\n",
    "\n",
    "def extract_instruction(text):\n",
    "    if text == '':\n",
    "        return []\n",
    "    seg_ins = extract_ins(text, no_input=True)\n",
    "    seg_oup = extract_oup(text, no_input=True)\n",
    "    return [seg_ins, seg_oup]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaacf01-6f00-4fa6-9f2c-cf688d58237a",
   "metadata": {},
   "source": [
    "- 现在，我们使用 **这些工具函数** 从 **GPT-4 生成的详细输出** 中 **提取优化后的指令与响应**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699b79b-959e-492e-9fad-f7e451d56777",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_instr, new_outp = extract_instruction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad89ee-9c63-42c8-a113-a98b13a1fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the significance of the metric system in global trade and provide examples of how unit conversions can impact international business transactions.\n"
     ]
    }
   ],
   "source": [
    "print(new_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf3fb6-f572-44ea-aea6-17f64288fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metric system, also known as the International System of Units (SI), is a decimal-based system of measurement that is used globally. Its significance in global trade lies in its standardization, which facilitates international communication and commerce. \n",
      "\n",
      "   One of the primary advantages of the metric system is that it is universally recognized, which reduces confusion and errors in measurement. For example, when a company in the United States imports goods from Europe, the specifications for those goods are often provided in metric units. If the U.S. company is accustomed to using imperial units (like inches or pounds), they must convert these measurements to ensure compatibility. \n",
      "\n",
      "   Unit conversions can significantly impact international business transactions. For instance, if a manufacturer orders 100 kilograms of a product but mistakenly interprets it as 100 pounds, they will receive a much smaller quantity than intended, leading to production delays and financial losses. \n",
      "\n",
      "   Additionally, in industries such as pharmaceuticals, precise measurements are critical. A dosage specified in milligrams must be accurately converted to ensure patient safety. \n",
      "\n",
      "   In summary, the metric system's role in global trade is crucial for maintaining consistency and accuracy in measurements, which ultimately supports efficient and effective international business operations.\n"
     ]
    }
   ],
   "source": [
    "print(new_outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec63dc-eaf1-4bcf-87ab-c63b2924cc67",
   "metadata": {},
   "source": [
    "- **注意**：目前，**指令优化（instruction-refinement）** 仅适用于 **数据集中不包含 `\"input\"` 字段** 的条目。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b1559-61c9-4ab9-a353-4962e4ec6d38",
   "metadata": {},
   "source": [
    "## 优化响应（Improve Responses）\n",
    "\n",
    "- 同样，我们可以将 **反思微调（Reflection-Tuning）** 过程 **应用于数据集的响应部分**（即 `\"output\"` 字段）。  \n",
    "- 下面是 **[Reflection-Tuning 仓库](https://github.com/tianyi-lab/Reflection_Tuning/blob/main/reflection_code/reflect_response.py)** 提供的 **两个工具函数**，用于格式化输入，使其适配 **GPT-4 模型** 进行数据优化。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78806c-abc1-4f38-afc9-9582bb48b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_gen_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of the answer to a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt\n",
    "\n",
    "\n",
    "def res_gen_prompt_input(ins, inp, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful and precise assistant for checking the quality of the answer to a given instruction and its input.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Input]\\n{inp}\\n\\n[The End of Input]\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction and corresponding input. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction and corresponding input? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, inp=inp, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 现在，我们对 **数据集中的一条样本** 进行 **反思微调**，生成 **优化后的响应**，以观察其效果：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126c4aa3-687c-4328-b174-84f1078cac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The answer provided is not good for the given instruction for several reasons:\n",
      "\n",
      "- **Helpfulness**: While the answer does provide the correct conversion, it lacks any explanation or context. A more helpful answer would include a brief explanation of the conversion process, which would aid understanding.\n",
      "\n",
      "- **Relevance**: The answer is relevant in that it addresses the instruction to convert kilometers to meters, but it could be more relevant by including the conversion factor used (1 kilometer = 1000 meters).\n",
      "\n",
      "- **Accuracy**: The answer is accurate in terms of the numerical conversion (45 kilometers = 45000 meters). However, it could be misleading if the reader does not understand how the conversion was derived.\n",
      "\n",
      "- **Level of Details**: The answer is very brief and lacks detail. A more detailed response would include the conversion factor and a step-by-step explanation of how the conversion is performed.\n",
      "\n",
      "2. [Better Answer] To convert kilometers to meters, you can use the conversion factor that 1 kilometer is equal to 1000 meters. Therefore, to convert 45 kilometers to meters, you multiply 45 by 1000. \n",
      "\n",
      "So, 45 kilometers × 1000 meters/kilometer = 45000 meters. \n",
      "\n",
      "Thus, 45 kilometers is equal to 45000 meters. [End]\n"
     ]
    }
   ],
   "source": [
    "entry = json_data[2]\n",
    "\n",
    "system_prompt, prompt = res_gen_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206abe9-5a64-4532-90d5-661d63670531",
   "metadata": {},
   "source": [
    "- 如上所示，**生成的响应** 包含了对 **原始响应的分析**；  \n",
    "- 我们可以使用 **[Reflection-Tuning 仓库](https://github.com/tianyi-lab/Reflection_Tuning/blob/main/reflection_code/reflect_response.py)** 提供的 **工具函数** 提取 **优化后的新响应**：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "164cb816-f7dd-4399-a0be-300f4518cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(text):\n",
    "    if text.count('[Better Answer]') >= 2:\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|\\[Better Answer\\]|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    else:\n",
    "        # pattern = r'\\[(Better Answer)\\](.*?)\\[End\\]'\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|End|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    return [segment[1].strip() for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95174f90-dc02-483d-a335-8b448d1b1e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To convert kilometers to meters, you can use the conversion factor that 1 kilometer is equal to 1000 meters. Therefore, to convert 45 kilometers to meters, you multiply 45 by 1000. \n",
      "\n",
      "So, 45 kilometers × 1000 meters/kilometer = 45000 meters. \n",
      "\n",
      "Thus, 45 kilometers is equal to 45000 meters.\n"
     ]
    }
   ],
   "source": [
    "response = extract_response(output)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf583fb-7e18-4b84-89dc-1c5d162c67ea",
   "metadata": {},
   "source": [
    "## 提升数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- 现在，让我们将 **指令反思（instruction-reflection）** 和 **响应反思（response-reflection）** 技术 **应用到实际数据集**。  \n",
    "- **注意**：此处仅对 **小规模数据子集** 进行 **演示**，如果要应用到 **完整数据集**，请修改相关参数。  \n",
    "\n",
    "```python\n",
    "data_to_process = json_data[:3]\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "data_to_process = json_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb631a-cde5-4f5c-8eae-0065b4723abb",
   "metadata": {},
   "source": [
    "### Reflect instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc4fb6-9c95-4999-ba1f-c333e701d779",
   "metadata": {},
   "source": [
    "- 以下代码使用 **反思微调（Reflection-Tuning）** 方法，对 **原始数据集中的指令** 进行 **优化与精炼**。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4564aa-2d3e-46a8-a339-295c5ff177b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reflect_instructions(json_data, client):\n",
    "    new_json_data = [] \n",
    "    \n",
    "    for entry in tqdm(json_data):\n",
    "        \n",
    "        if not entry[\"input\"]:\n",
    "            system_prompt, prompt = instr_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_instr, new_outp = extract_instruction(output)\n",
    "            new_entry = {\"instruction\": new_instr, \"input\": \"\", \"output\": new_outp}\n",
    "            new_json_data.append(new_entry)\n",
    "        else:\n",
    "            new_json_data.append(entry)\n",
    "\n",
    "    return new_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d897eda7-ebd6-4a09-a3ae-8d05a2f234dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:06<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "data_to_process = json_data[:3]\n",
    "\n",
    "new_json_data = reflect_instructions(data_to_process, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a677a2-d590-4ffb-a202-5fe79a317d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Evaluate the following phrase by transforming it into the '\n",
      "                'spelling given.',\n",
      " 'input': 'freind --> friend',\n",
      " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".'}\n",
      "\n",
      "\n",
      "\n",
      "{'instruction': 'Edit the following sentence for grammar.',\n",
      " 'input': 'He go to the park every day.',\n",
      " 'output': 'He goes to the park every day.'}\n",
      "\n",
      "\n",
      "\n",
      "{'instruction': 'Explain the significance of understanding metric conversions '\n",
      "                'in scientific research, and provide an example of how a '\n",
      "                'miscalculation in unit conversion could impact experimental '\n",
      "                'results.',\n",
      " 'input': '',\n",
      " 'output': 'Understanding metric conversions is crucial in scientific research '\n",
      "           'because accurate measurements are fundamental to the validity of '\n",
      "           'experimental results. The metric system is widely used in '\n",
      "           'scientific disciplines due to its ease of use and universal '\n",
      "           'acceptance, allowing researchers from different countries to '\n",
      "           'communicate their findings effectively.\\n'\n",
      "           '\\n'\n",
      "           '   For example, consider a scenario in a chemistry experiment '\n",
      "           'where a researcher needs to prepare a solution with a specific '\n",
      "           'concentration. If the researcher intends to prepare a 1 molar (1 '\n",
      "           'M) solution of sodium chloride (NaCl) in 1 liter of water, they '\n",
      "           'must accurately measure the mass of NaCl required. The molar mass '\n",
      "           'of NaCl is approximately 58.44 grams per mole. Therefore, to '\n",
      "           'prepare 1 liter of a 1 M solution, the researcher needs to '\n",
      "           'dissolve 58.44 grams of NaCl in water.\\n'\n",
      "           '\\n'\n",
      "           '   However, if the researcher mistakenly converts the volume from '\n",
      "           'liters to milliliters and uses 1 mL instead of 1 L, they would '\n",
      "           'only need 0.05844 grams of NaCl. This significant error in unit '\n",
      "           'conversion would lead to a solution that is 1,000 times more '\n",
      "           'concentrated than intended. Such a miscalculation could result in '\n",
      "           'erroneous experimental outcomes, potentially leading to incorrect '\n",
      "           'conclusions about the behavior of the solution in reactions or '\n",
      "           'biological systems. This example highlights the importance of '\n",
      "           'precise unit conversions in ensuring the accuracy and reliability '\n",
      "           'of scientific research.'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in new_json_data[:3]:\n",
    "    pprint(i)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17840244-a7f9-47e4-8551-671fdedfc856",
   "metadata": {},
   "source": [
    "- 保存新数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9710e60-6c3a-42ab-ab66-24005db2e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-reflected.json\", \"w\") as file:\n",
    "    json.dump(new_json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a081ff5-7aa7-4651-934a-34ce56b7ee5e",
   "metadata": {},
   "source": [
    "### 响应反思调整（Reflecting on Responses）\n",
    "\n",
    "- 现在，我们对 **模型生成的响应** 进行 **反思优化**(response-reflection)：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "835da869-965a-4a4c-9799-56dbcd559d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38f436b6-1b6c-45e7-a538-a47021607ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_responses(json_data, client):\n",
    "    new_json_data = [] \n",
    "    \n",
    "    for entry in tqdm(json_data):\n",
    "        \n",
    "        if not entry[\"input\"]:\n",
    "            system_prompt, prompt = res_gen_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_response = extract_response(output)\n",
    "\n",
    "            if not len(new_response):\n",
    "                new_response = entry[\"output\"]\n",
    "                      \n",
    "            new_entry = {\"instruction\": entry[\"instruction\"], \"input\": \"\", \"output\": new_response[0]}\n",
    "            new_json_data.append(new_entry)\n",
    "\n",
    "        else:\n",
    "            system_prompt, prompt = res_gen_prompt_input(ins=entry[\"instruction\"], inp=entry[\"input\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_response = extract_response(output)\n",
    "\n",
    "            if not len(new_response):\n",
    "                new_response = entry[\"output\"]\n",
    "\n",
    "            new_entry = {\"instruction\": entry[\"instruction\"], \"input\": entry[\"input\"], \"output\": new_response[0]}\n",
    "            new_json_data.append(new_entry)\n",
    "\n",
    "    return new_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0168fb7e-bef4-43e1-967d-f294909b6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:07<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "new_json_data = reflect_responses(data_to_process, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a0949dc-70f3-4adb-9d0a-7f387c0702c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Evaluate the following phrase by transforming it into the '\n",
      "                'spelling given.',\n",
      " 'input': 'freind --> friend',\n",
      " 'output': 'The input phrase \"freind\" contains a spelling error. The correct '\n",
      "           'transformation of the word is as follows: \"freind\" should be '\n",
      "           'corrected to \"friend.\" Therefore, the correct spelling is '\n",
      "           '\"friend.\"'}\n",
      "\n",
      "\n",
      "\n",
      "{'instruction': 'Edit the following sentence for grammar.',\n",
      " 'input': 'He go to the park every day.',\n",
      " 'output': 'The original sentence \"He go to the park every day\" contains a '\n",
      "           'grammatical error in the verb form. The correct form should be \"He '\n",
      "           'goes to the park every day.\" This is because the subject \"He\" is '\n",
      "           'third person singular, and in English, the verb \"to go\" changes to '\n",
      "           '\"goes\" when used with third person singular subjects. Therefore, '\n",
      "           'the corrected sentence is grammatically accurate and maintains the '\n",
      "           'original meaning.'}\n",
      "\n",
      "\n",
      "\n",
      "{'instruction': 'Convert 45 kilometers to meters.',\n",
      " 'input': '',\n",
      " 'output': 'To convert kilometers to meters, you can use the conversion factor '\n",
      "           'that 1 kilometer is equal to 1,000 meters. Therefore, to convert '\n",
      "           '45 kilometers to meters, you multiply 45 by 1,000. \\n'\n",
      "           '\\n'\n",
      "           'So, 45 kilometers is equal to 45,000 meters (45 km × 1,000 m/km = '\n",
      "           '45,000 m). \\n'\n",
      "           '\\n'\n",
      "           'This conversion is useful in various contexts, such as distance '\n",
      "           'measurement in travel or scientific calculations.'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in new_json_data[:3]:\n",
    "    pprint(i)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603159d-e9fa-42cd-a5ab-b528e534e103",
   "metadata": {},
   "source": [
    "- 保存最新的数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e763966-6a43-4706-879d-1f413a85ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"response-reflected.json\", \"w\") as file:\n",
    "    json.dump(new_json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc19b4-926b-4497-8370-496efd970366",
   "metadata": {},
   "source": [
    "## 创建改进的指令数据集\n",
    "\n",
    "- **应用上述两种方法** 到 **第 7 章指令数据集** 的 **1100 条数据**，成本约 **$0.60（60 美分）**。  \n",
    "- **为了避免 GitHub 仓库体积过大**，处理后的数据集文件可从 **Google Drive** 下载：\n",
    "  - [instruction-reflected.json](https://drive.google.com/file/d/1c1QnuTdt9nP1u51vBn4_b05mWR_ZNGBv/view?usp=sharing)  \n",
    "  - [response-reflected.json](https://drive.google.com/file/d/1RNckTZ2ELcdUoJtaylao6NvyZPMtNv1v/view?usp=sharing)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="appendix-E/01_main-chapter-code/appendix-E.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff",
   "metadata": {
    "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff"
   },
   "source": [
    "# Appendix E: 使用LoRA进行参数高效微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "316166b4-027a-4756-e9b4-fe88ae75dd4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.7.2\n",
      "numpy version: 1.25.2\n",
      "tiktoken version: 0.5.1\n",
      "torch version: 2.2.2\n",
      "tensorflow version: 2.15.0\n",
      "pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", \n",
    "        \"pandas\"      \n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21532056-0ef4-4c98-82c7-e91f61c6485e",
   "metadata": {
    "id": "21532056-0ef4-4c98-82c7-e91f61c6485e"
   },
   "source": [
    "## E.1 LoRA简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc999-3d91-4a1c-a157-9d056392e8d8",
   "metadata": {
    "id": "66edc999-3d91-4a1c-a157-9d056392e8d8"
   },
   "source": [
    "- 本节无代码。\n",
    "- 低秩适配（Low-rank Adaptation, LoRA）是一种机器学习技术，通过仅调整预训练模型中一小部分低秩参数，使其更好地适应特定（通常较小）数据集。\n",
    "- 这种方法的重要性在于，它能够高效地微调大型模型以适应特定任务的数据，大大降低了微调所需的计算成本和时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1",
   "metadata": {
    "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1"
   },
   "source": [
    "- 假设我们有一个大型权重矩阵 $W$，对应某一层。\n",
    "- 在反向传播过程中，我们学习一个矩阵 $\\Delta W$，它包含了如何更新原始权重以在训练过程中最小化损失函数的信息。\n",
    "- 在常规训练和微调中，权重更新定义如下：\n",
    "\n",
    "$$W_{\\text{updated}} = W + \\Delta W$$\n",
    "\n",
    "- [Hu 等人](https://arxiv.org/abs/2106.09685) 提出的 LoRA 方法提供了一种更高效的方式来计算权重更新 $\\Delta W$，通过学习其近似值 $\\Delta W \\approx AB$。\n",
    "- 换句话说，在 LoRA 方法中，我们有以下公式，其中 $A$ 和 $B$ 是两个较小的权重矩阵：\n",
    "\n",
    "$$W_{\\text{updated}} = W + AB$$\n",
    "\n",
    "- 下图并排展示了参数的全部微调和 LoRA 的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b",
   "metadata": {
    "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/E1/1.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc",
   "metadata": {
    "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc"
   },
   "source": [
    "- 如果你仔细观察，会发现上图中关于全量微调和 LoRA 的描述与我之前展示的公式略有不同。\n",
    "- 这是由于矩阵乘法的分配律：我们不需要将权重与更新后的权重相加，而是可以将它们分开处理。\n",
    "- 例如，如果输入数据为 $x$，那么对于常规微调，可以写为：\n",
    "\n",
    "$$x (W+\\Delta W) = x W + x \\Delta W$$\n",
    "\n",
    "- 类似地，对于 LoRA，可以写为：\n",
    "\n",
    "$$x (W+A B) = x W + x A B$$\n",
    "\n",
    "- LoRA 的优势在于，我们可以将 LoRA 的权重矩阵分开存储。\n",
    "- 实际上，这意味着我们完全不需要修改预训练模型的权重（仅更新$A$和$B$矩阵即可），因此可以动态地应用 LoRA 矩阵。\n",
    "- 在设置好数据集并加载模型之后，我们将在代码中实现 LoRA，使这些概念更加直观。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840b001",
   "metadata": {},
   "source": [
    "## E.2 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c64df-4431-4d27-834d-2bb38a01fc02",
   "metadata": {
    "id": "669c64df-4431-4d27-834d-2bb38a01fc02"
   },
   "source": [
    "- 本节复用了第6章中的代码，用于加载和准备数据集。\n",
    "- 为避免重复代码，可以打开并运行第6章的笔记本，然后将附录 E.4 中的 LoRA 代码插入到其中。\n",
    "- （LoRA 代码原本是第6章的最后一节，但由于篇幅原因被移至附录中。）\n",
    "- 同样，我们也可以将 LoRA 应用于第7章中的模型，以实现指令微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "a67a7afe-b401-4463-c731-87025d20f72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {
    "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57"
   },
   "source": [
    "- 在验证步骤中，每个批次包含8个样本，且每个样本由120个词元组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
    "outputId": "2ae34de1-dd01-4f99-d2c8-ba4dca400754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {
    "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1"
   },
   "source": [
    "- 查看一下几个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "4d19ed61-cf7a-4ec4-b822-c847dd1c5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604",
   "metadata": {
    "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604"
   },
   "source": [
    "## E.3 模型的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1",
   "metadata": {
    "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1"
   },
   "source": [
    "- 这一部分跟第六章很像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
    "outputId": "b8c9b125-bb52-45d3-8071-fa5054dbf5a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 45.0kiB/s]\n",
      "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 2.15MiB/s]\n",
      "hparams.json: 100%|█████████████████████████| 90.0/90.0 [00:00<00:00, 54.5kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:12<00:00, 6.86MiB/s]\n",
      "model.ckpt.index: 100%|███████████████████| 5.21k/5.21k [00:00<00:00, 2.99MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 1.32MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.48MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout比率\n",
    "    \"qkv_bias\": True         # 查询-键-值 偏置\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252614cd-7ce6-4908-83e6-3761f519904e",
   "metadata": {
    "id": "252614cd-7ce6-4908-83e6-3761f519904e"
   },
   "source": [
    "- 用文本内容进行模型加载的二次确认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
    "outputId": "28ccbca5-8de9-41a0-c093-da00fcbaa91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174b31b-1ab5-4115-b01c-245369da5af3",
   "metadata": {
    "id": "8174b31b-1ab5-4115-b01c-245369da5af3"
   },
   "source": [
    "- 我们将改变输出层作为分类模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e255ce91-d73a-4854-90a4-95804928eb16",
   "metadata": {
    "id": "e255ce91-d73a-4854-90a4-95804928eb16"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e6f057-1383-4ece-8444-0a88e71ac75d",
   "metadata": {
    "id": "02e6f057-1383-4ece-8444-0a88e71ac75d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意：\n",
    "# 如果适用，取消注释以下代码可在 Apple Silicon 芯片上运行，\n",
    "# 经测试，在 M3 MacBook Air 上运行速度比 Apple CPU 快约 1.2 倍。\n",
    "# 但可能导致损失值略有不同。\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device);  # 对于 nn.Module 类，无需 model = model.to(device) 赋值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe",
   "metadata": {
    "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe"
   },
   "source": [
    "- 最后，让我们计算未微调模型的初始分类准确率（预计约为 50%，表明模型尚无法可靠地区分垃圾消息和非垃圾消息）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
    "outputId": "74848515-5a49-4125-fecb-9f4bac23f812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b",
   "metadata": {
    "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b"
   },
   "source": [
    "## E.4 使用LoRA进行参数高效微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4a82-61ef-4d0a-9858-8988e844f12c",
   "metadata": {
    "id": "652a4a82-61ef-4d0a-9858-8988e844f12c"
   },
   "source": [
    "- 我们首先初始化一个 LoRA Layer，它会创建矩阵 $A$ 和 $B$，同时设置缩放超参数 `alpha` 和秩超参数 `rank` ($r$)。\n",
    "- 该层可以接受输入并计算相应的输出，如下图所示：\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/E4/1.png\" width=\"350px\">\n",
    "\n",
    "代码中，上图展示的 LoRA 层实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ds9ywjMwvIW",
   "metadata": {
    "id": "2ds9ywjMwvIW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # 在PyTorch中对线性层进行相同的初始化\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21faa8-0614-4257-93cd-68952193e14a",
   "metadata": {
    "id": "ad21faa8-0614-4257-93cd-68952193e14a"
   },
   "source": [
    "- 上述代码中的 `rank` 是一个超参数，用于控制矩阵 $A$ 和 $B$ 的内维度。换句话说，该参数决定了 LoRA 引入的额外参数数量，是平衡模型适应性和参数效率的关键因素。\n",
    "- 第二个超参数 `alpha` 是一个缩放超参数，作用于低秩适配的输出。它主要控制适配层输出对原始层输出的影响程度，可视为调节低秩适配对层输出影响的一种方式。\n",
    "- 目前，我们实现的 `LoRALayer` 类能够对层的输入 $x$ 进行变换。然而，在 LoRA 中，我们通常希望替换现有的 `Linear` 层，以便权重更新可以作用于现有的预训练权重，如下图所示：\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/E4/2.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f",
   "metadata": {
    "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f"
   },
   "source": [
    "- 为了整合如上图所示的原始 `Linear` 层权重，我们实现了一个 `LinearWithLoRA` 层。该层使用之前实现的 LoRALayer，可以替换神经网络中的现有 `Linear` 层，例如在 LLM 的自注意力模块或前馈模块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127d3a64-8359-4b21-b056-78d58cc75fe8",
   "metadata": {
    "id": "127d3a64-8359-4b21-b056-78d58cc75fe8"
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1145a90-35ff-462c-820b-15483fa5b051",
   "metadata": {
    "id": "e1145a90-35ff-462c-820b-15483fa5b051"
   },
   "source": [
    "- 由于我们在 LoRA 层中将权重矩阵 $B$（`LoRALayer` 中的 `self.B`）初始化为零，矩阵 $A$ 和 $B$ 的乘积结果为一个全零矩阵，因此不会影响原始权重（因为将 0 加到原始权重上不会对其产生修改）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d",
   "metadata": {
    "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d"
   },
   "source": [
    "- 为了在之前定义的 GPT 模型上尝试使用 LoRA，我们定义了一个 `replace_linear_with_lora` 函数，用于将模型中的所有 `Linear` 层替换为新的 `LinearWithLoRA` 层。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/E4/3.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "WlQZ8ygqzN_g",
   "metadata": {
    "id": "WlQZ8ygqzN_g"
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            #用LinearWithLoRA替代先前的线性输出层\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 对于子模块也这么用\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2",
   "metadata": {
    "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2"
   },
   "source": [
    "- 接着，我们冻结原始模型参数，并使用 `replace_linear_with_lora` 函数替换这些 `Linear` 层，代码如下。\n",
    "- 这样，LLM 中的 `Linear` 层将被替换为 `LinearWithLoRA` 层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
    "outputId": "fd4c208f-854a-4701-d9d3-9d73af733364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mLk_fPq0yz_u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLk_fPq0yz_u",
    "outputId": "0a93b8fc-05d7-4ace-ee47-e2fc6bdd7d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9",
   "metadata": {
    "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9"
   },
   "source": [
    "- 使用 LoRA 时，可训练参数的数量减少了近 50 倍。\n",
    "- 现在通过输出模型架构来再次确认各层是否已按预期修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
    "outputId": "acff8eca-3775-45a2-b62d-032a986ef037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55",
   "metadata": {
    "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55"
   },
   "source": [
    "- 从上面的模型架构可以看出，模型现在包含了新的 `LinearWithLoRA` 层。\n",
    "- 由于矩阵 $B$ 初始化为 0，我们预计模型的初始性能与之前相比没有变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "DAlrb_I00VEU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAlrb_I00VEU",
    "outputId": "3da44ac4-230b-4358-d996-30b63f0d962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101",
   "metadata": {
    "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101"
   },
   "source": [
    "- 在 M3 MacBook Air 笔记本电脑上训练大约需要 15 分钟，而在 V100 或 A100 GPU 上不到半分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wCParRvr0eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCParRvr0eff",
    "outputId": "ce910a9c-ee89-48bb-bfa6-49c6aee1e450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.008, Val loss 0.052\n",
      "Ep 2 (Step 000250): Train loss 0.021, Val loss 0.179\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.096, Val loss 0.080\n",
      "Ep 3 (Step 000350): Train loss 0.010, Val loss 0.116\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.003, Val loss 0.151\n",
      "Ep 4 (Step 000450): Train loss 0.008, Val loss 0.077\n",
      "Ep 4 (Step 000500): Train loss 0.001, Val loss 0.147\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.007, Val loss 0.094\n",
      "Ep 5 (Step 000600): Train loss 0.000, Val loss 0.056\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 12.10 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c",
   "metadata": {
    "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c"
   },
   "source": [
    "- 对模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bawWGijA0iF3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "bawWGijA0iF3",
    "outputId": "af70782a-d605-4376-fa6c-d33b38979cfa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa074723-e3f7-4f7e-a267-855531a037dc",
   "metadata": {
    "id": "aa074723-e3f7-4f7e-a267-855531a037dc"
   },
   "source": [
    "- 之前我们通过 `eval_iter=5` 设置仅计算了 5 个批次的准确率\n",
    "- 下面我们将在整个数据集上计算准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1D2awlEq0gZi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D2awlEq0gZi",
    "outputId": "d603eda1-d912-43eb-ec9c-af6a622510a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.00%\n",
      "Validation accuracy: 96.64%\n",
      "Test accuracy: 97.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d",
   "metadata": {
    "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d"
   },
   "source": [
    "- 从上述较高的准确率值可以看出，LoRA 微调是成功的。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch03/01_main-chapter-code/ch03.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae38945-39dd-45dc-ad4f-da7a4404241f",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
   "metadata": {},
   "source": [
    "# 第三章: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bcbe8-a034-43a2-b557-997b03c9882d",
   "metadata": {},
   "source": [
    "本章所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# 打印 PyTorch 版本，便于复现本章输出\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4474d-7c68-4846-8702-37906cf08197",
   "metadata": {},
   "source": [
    "- LLM的核心:Attention\n",
    "- 译者:可以直接看论文呀!\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
   "metadata": {},
   "source": [
    "<img src=\"../image/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e020fd-9690-4343-80df-da96678bef5e",
   "metadata": {},
   "source": [
    "<img src=\"../image/02.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
   "metadata": {},
   "source": [
    "## 3.1 长序列的建模"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
   "metadata": {},
   "source": [
    "- 没有代码\n",
    "- 逐字翻译文本通常不可行，因为源语言和目标语言在语法结构上存在差异："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0c433-aa4b-491e-848a-54905ebb05ad",
   "metadata": {},
   "source": [
    "<img src=\"../image/03.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
   "metadata": {},
   "source": [
    "- 在Transformer模型出现之前，机器翻译任务主要依赖于编码器(encoder)-解码器(decoder)架构的循环神经网络（RNNs）。\n",
    "- 在这种架构中，编码器逐词处理源语言序列，并通过隐藏状态（神经网络中的中间层）生成输入序列的表示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2",
   "metadata": {},
   "source": [
    "<img src=\"../image/04.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602c585-b87a-41c7-a324-c5e8298849df",
   "metadata": {},
   "source": [
    "## 3.2 注意力机制高效捕获数据关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde64c-6034-421d-81d9-8244932086ea",
   "metadata": {},
   "source": [
    "- 本节不涉及代码。\n",
    "- 借助注意力机制，文本生成解码器能够选择性地关注所有输入token，从而在生成特定输出token时，动态分配不同输入token的重要性系数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1",
   "metadata": {},
   "source": [
    "<img src=\"../image/05.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e",
   "metadata": {},
   "source": [
    "- Transformer中的自注意力机制是一种关键技术，它通过让序列中的每个位置与其他所有位置交互并计算相关性，从而增强输入表示的上下文信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565dc9f-b1be-4c78-b503-42ccc743296c",
   "metadata": {},
   "source": [
    "<img src=\"../image/06.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
   "metadata": {},
   "source": [
    "## 3.3 自注意力关注的不同部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
   "metadata": {},
   "source": [
    "### 3.3.1 无可变参数的自注意力模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269e9f1-df11-4644-b575-df338cf46cdf",
   "metadata": {},
   "source": [
    "- 本节介绍了一种高度简化的自注意力变体，不包含任何可训练的权重。\n",
    "- 该变体仅用于说明目的，并非Transformer中实际使用的注意力机制。\n",
    "- 下一节（3.3.2节）将扩展此简易模型，实现真正的自注意力机制。\n",
    "- 假设给定一个输入序列 $x^{(1)}$ 到 $x^{(T)}$：\n",
    "  - 输入是一个文本（例如，一句已被处理为token嵌入的句子，如“Your journey starts with one step”），具体处理方法在第2章中已有描述。\n",
    "  - 例如，$x^{(1)}$ 是表示单词“Your”的d维向量，以此类推。\n",
    "\n",
    "- **目标：** 为输入序列中的每个元素 $x^{(i)}$（从 $x^{(1)}$ 到 $x^{(T)}$）计算上下文向量 $z^{(i)}$（$z$ 和 $x$ 的维度相同）。\n",
    "    - 上下文向量 $z^{(i)}$ 是对输入 $x^{(1)}$ 到 $x^{(T)}$ 的加权求和。\n",
    "    - 上下文向量是针对特定输入的“上下文”相关表示。\n",
    "      - 以第二个输入 $x^{(2)}$ 为例，说明具体计算过程。\n",
    "      - 第二个上下文向量 $z^{(2)}$ 是对所有输入 $x^{(1)}$ 到 $x^{(T)}$ 的加权求和，权重由相对于 $x^{(2)}$ 的注意力权重决定。\n",
    "      - 注意力权重决定了每个输入元素对 $z^{(2)}$ 的贡献程度。\n",
    "      - 简而言之，$z^{(2)}$ 是 $x^{(2)}$ 的增强版本，融合了与当前任务相关的所有其他输入元素的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
   "metadata": {},
   "source": [
    "<img src=\"../image/07.webp\" width=\"400px\">\n",
    "\n",
    "- （请注意，此图中的数字已截断至小数点后一位，以减少视觉干扰；其他图表中的数值也可能经过类似处理。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff856c58-8382-44c7-827f-798040e6e697",
   "metadata": {},
   "source": [
    "- 按照惯例，未归一化的注意力值称为 **“注意力得分”**，而归一化后总和为1的注意力得分称为 **“注意力权重”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b10344-128d-462a-823f-2178dff5fd58",
   "metadata": {},
   "source": [
    "- 下方代码逐步演示了上图的操作过程\n",
    "\n",
    "<br>\n",
    "\n",
    "- **步骤 1：** 计算未归一化的注意力得分 $\\omega$\n",
    "- 假设使用第二个输入token作为查询，即 $q^{(2)} = x^{(2)}$，通过点积计算未归一化的注意力得分：\n",
    "    - $\\omega_{21} = x^{(1)} \\cdot q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} \\cdot q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} \\cdot q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} \\cdot q^{(2)\\top}$\n",
    "- 其中，$\\omega$ 是希腊字母“欧米伽”，表示未归一化的注意力得分。\n",
    "    - 在 $\\omega_{21}$ 中，下标“21”表示以第2个元素为查询，与第1个元素计算得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3",
   "metadata": {},
   "source": [
    "- 假设我们有以下输入句子，该句子已根据第3章的描述嵌入到3维向量中（此处我们使用了一个非常小的嵌入维度进行说明，以便内容可以显示在页面上）： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 构造一个玩具输入序列 inputs：共有 6 个 token，每个 token 用 3 维向量表示\n",
    "# inputs.shape == (num_tokens=6, d_in=3)\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "# 每一行对应一个 token 的输入向量 x^(i)，每一列对应一个特征维度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
   "metadata": {},
   "source": [
    "- （在本书中，我们遵循机器学习和深度学习的常见惯例：训练样本以行表示，特征值以列表示；对于上述张量，每一行表示一个词，每一列表示一个嵌入维度。）\n",
    "\n",
    "- 本节的主要目标是演示如何以第二个输入序列 $x^{(2)}$ 作为查询，计算其上下文向量 $z^{(2)}$。\n",
    "\n",
    "- 图中展示了该过程的第一步，即通过点积操作计算 $x^{(2)}$ 与所有其他输入元素之间的注意力得分 $\\omega$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3453a-58fa-42c4-b225-86850bc856f8",
   "metadata": {},
   "source": [
    "<img src=\"../image/08.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22",
   "metadata": {},
   "source": [
    "- 我们以输入序列中的第2个元素 $x^{(2)}$ 为例，计算其上下文向量 $z^{(2)}$；稍后会将此方法推广至计算所有上下文向量。\n",
    "- 第一步是通过计算查询 $x^{(2)}$ 与所有输入token的点积，得到未归一化的注意力得分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# 步骤 1：计算未归一化注意力分数 omega\n",
    "# 这里我们选择第 2 个 token（索引为 1）作为 query，意思是“以它为中心，关注其它单词”\n",
    "query = inputs[1]  # 选取第2个 token（journey）作为查询序列（Query 向量）\n",
    "\n",
    "# 创建一个空的 Tensor（与输入 token 数量相同），用于存放所有注意力分数 ω_{2,i}\n",
    "# 这里 attn_scores_2 最终会存储 query（第2个 token）与每个输入 x^(i) 的点积（共 num_tokens 个）\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])  # shape: (num_tokens,)\n",
    "\n",
    "# 针对每一个输入 token x^(i)，计算它与 query 的点积，作为未归一化的相关性分数\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # torch.dot(x_i, query)：计算第 i 个 token 和 query 的向量点积，结果为一个标量\n",
    "    # 这个标量反映了 x^(i) 与 query 的“相似程度”，值越大代表越相关\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    # 例如 i=0 时，是 x^(1) 与 x^(2) 做点积，对应 ω_{2,1}\n",
    "\n",
    "# 打印所有注意力分数，结果为 shape=(6,) 的一维张量\n",
    "# 这些分数就是“以第2个 token 作为 query”时，模型对其它每个 token 的初步关注度（未归一化）\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df09ae0-199f-4b6f-81a0-2f70546684b8",
   "metadata": {},
   "source": [
    "- 补充说明：点积本质上是逐元素相乘并将所得积相加的一种简写表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# 补充：点积就是逐元素相乘再求和（这里示例计算 ω_21 = x^(1) · q^(2)）\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "    # 累加 x^(1)[idx] * q^(2)[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))\n",
    "# 两种写法结果应一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d444d76-e19e-4e9a-a268-f315d966609b",
   "metadata": {},
   "source": [
    "- **步骤 2：** 将未归一化的注意力得分（“欧米伽”，$\\omega$）归一化，使其总和为1。\n",
    "- 以下是一种简单的方法，用于将未归一化的注意力得分归一化："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b",
   "metadata": {},
   "source": [
    "<img src=\"../image/09.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Step 2（简化版）：把分数除以总和，让权重之和为 1（注意：这不是 softmax）\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum() \n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc0a57-f53e-41bf-8793-daa77a819431",
   "metadata": {},
   "source": [
    "- 然而，在实践中，使用softmax函数进行归一化更为常见，因为它能够更好地处理极端值，并且在训练过程中具有更理想的梯度特性，因此推荐使用。\n",
    "- 下面是一个简单的softmax函数实现，用于缩放并对向量元素进行归一化，使它们的和为1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 一个“朴素版”的 softmax：用于演示概念（实际工程中建议直接用 torch.softmax）\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "# softmax 会把分数映射成概率分布（和为 1），并且在反向传播中性质更好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
   "metadata": {},
   "source": [
    "- 上述简单实现可能会因输入值过大或过小而遭遇数值不稳定问题，导致溢出或下溢。\n",
    "- 因此，在实践中，建议使用PyTorch内置的softmax函数，因为它经过高度优化，性能更佳："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 使用 PyTorch 内置 softmax（数值更稳定、实现也更高效）\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n",
    "# dim=0 表示对这个一维向量做 softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
   "metadata": {},
   "source": [
    "- **步骤 3**：通过将嵌入的输入标记 $x^{(i)}$ 与注意力权重相乘，并对结果向量求和，计算上下文向量 $z^{(2)}$："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d",
   "metadata": {},
   "source": [
    "<img src=\"../image/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: 用注意力权重对输入向量加权求和，得到上下文向量 z^(2)\n",
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "# 初始化为 0 向量（shape 与单个输入 token 向量一致，这里是 (3,)）\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    # 把每个 x_i 按权重加到一起；这里相当于 values = inputs\n",
    "\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
   "metadata": {},
   "source": [
    "### 3.3.2 计算所有token的attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8",
   "metadata": {},
   "source": [
    "#### 将其推广到所有输入序列标记：\n",
    "\n",
    "- 上面，我们为输入2计算了注意力权重和上下文向量（如下图中高亮的行所示）。\n",
    "- 接下来，我们将此计算推广到所有输入序列标记，计算对应的注意力权重和上下文向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
   "metadata": {},
   "source": [
    "<img src=\"../image/11.webp\" width=\"400px\">\n",
    "\n",
    "- （请注意，图中的数字已四舍五入到小数点后两位；每一行的数值应相加为1.0或100%；其他图中的数字也进行了类似处理。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789b990-fb51-4beb-9212-bf58876b5983",
   "metadata": {},
   "source": [
    "- 在自注意力机制中，首先计算注意力得分，随后对这些得分进行归一化，得到总和为1的注意力权重。\n",
    "- 接着，利用这些注意力权重对输入进行加权求和，生成上下文向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
   "metadata": {},
   "source": [
    "<img src=\"../image/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652506-f2c8-473c-a905-85c389c842cc",
   "metadata": {},
   "source": [
    "- 对所有成对元素应用之前的**步骤 1**，计算未归一化的注意力得分矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04004be8-07a1-468b-ab33-32e16a551b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# 将 Step 1 推广到所有 token：计算所有成对元素的注意力得分矩阵 ω（shape: (T, T)）\n",
    "attn_scores = torch.empty(6, 6)\n",
    "#建立个空表来储存相关联程度\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "        # attn_scores[i, j] 表示第 i 个 token 对第 j 个 token 的相似度（未归一化分数）\n",
    "print(attn_scores)\n",
    "# 每一行对应一个 query token，每一列对应一个 key token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
   "metadata": {},
   "source": [
    "- 如果是矩阵相乘那么更有效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# 上面的双重循环等价于一次矩阵乘法（更快、更简洁）\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)\n",
    "# (T, d_in) @ (d_in, T) -> (T, T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
   "metadata": {},
   "source": [
    "- 与**第二步**相似, 我们对每一行都要归一化操作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Step 2 推广到矩阵：对每一行做 softmax，使每个 query 的注意力权重和为 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)\n",
    "# dim=-1 表示沿最后一维（列）归一化：每一行都是一个概率分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
   "metadata": {},
   "source": [
    "- 一个快速验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))\n",
    "#验证一下大家加起来都是1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
   "metadata": {},
   "source": [
    "- 用**step 3** 计算所有的向量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Step 3 推广到所有 token：一次矩阵乘法得到所有上下文向量（shape: (T, d_in)）\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "# (T, T) @ (T, d_in) -> (T, d_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
   "metadata": {},
   "source": [
    "-  作为合理性检查，之前计算的上下文向量 $z^{(2)} = [0.4419, 0.6515, 0.5683]$ 可以在上图的第二行找到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# 与之前手工计算的 z^(2)（context_vec_2）应一致\n",
    "print(\"Previous 2nd context vector:\", context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
   "metadata": {},
   "source": [
    "## 3.4 可调整参数的自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88363117-93d8-41fb-8240-f7cfe08b14a3",
   "metadata": {},
   "source": [
    "- 以下的概念框架展示了本节中开发的自注意力机制,以及这种机制是如何融入本书和本章的整体叙述与结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928",
   "metadata": {},
   "source": [
    "<img src=\"../image/13.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
   "metadata": {},
   "source": [
    "### 3.4.1 手把手的计算attention的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
   "metadata": {},
   "source": [
    "- 在本节中，我们实现了原始 Transformer 架构、GPT 模型以及大多数流行 LLM 中使用的自注意力机制。  \n",
    "- 这种自注意力机制被称为“缩放点积注意力”（scaled dot-product attention）。  \n",
    "- 整体思路与之前相似：  \n",
    "  - 我们希望计算针对特定输入元素的上下文向量，即输入向量的加权和。  \n",
    "  - 为此，我们需要生成注意力权重。  \n",
    "- 如你所见，与之前介绍的基本注意力机制相比，只有一些细微差异：  \n",
    "  - 最显著的区别是引入了在模型训练过程中更新的权重矩阵。  \n",
    "  - 这些可训练的权重矩阵至关重要，它们使模型（尤其是注意力模块）能够学习生成“优质”的上下文向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db4093-93e8-4bee-be8f-c8fac8a08cdd",
   "metadata": {},
   "source": [
    "<img src=\"../image/14.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
   "metadata": {},
   "source": [
    "- 按照步骤实现自注意力机制，我们将首先介绍三个训练权重矩阵 $W_q$、$W_k$ 和 $W_v$。  \n",
    "- 这三个矩阵用于通过矩阵乘法将嵌入的输入标记 $x^{(i)}$ 映射到查询向量、键向量和值向量：\n",
    "- (译者: 分别是Query、Key、Value,专有名词)   \n",
    "\n",
    "  - 查询向量：$q^{(i)} = W_q \\,x^{(i)}$  \n",
    "  - 键向量：$k^{(i)} = W_k \\,x^{(i)}$  \n",
    "  - 值向量：$v^{(i)} = W_v \\,x^{(i)}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334313-5fd0-477b-8728-04080a427049",
   "metadata": {},
   "source": [
    "- 输入 $x$ 和查询向量 $q$ 的嵌入维度可以相同，也可以不同，具体取决于模型的设计和实现方式。\n",
    "- 在 GPT 模型中，输入和输出维度通常是相同的，但为了便于示范并更好地理解计算过程，这里我们选择了不同的输入和输出维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出第 2 个 token 的输入向量 x^(2) 作为示例\n",
    "x_2 = inputs[1] # second input element\n",
    "# d_in: 输入向量维度（这里是 3）；d_out: Q/K/V 投影后的维度（这里设为 2，便于展示）\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
   "metadata": {},
   "source": [
    "- 下面，我们初始化三个权重矩阵；请注意，为了简化输出并便于示范，我们将 `requires_grad=False`\n",
    "- 但如果我们要在模型训练中使用这些权重矩阵，应将 `requires_grad=True`，以便在训练过程中更新这些矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# 固定随机种子确保可复现性\n",
    "\n",
    "# 三个可学习的投影矩阵：把输入 x 映射到 query/key/value 空间\n",
    "# 形状都是 (d_in, d_out)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# 注意：这里为了让输出更易读，requires_grad=False；真实训练中应为 True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
   "metadata": {},
   "source": [
    "- 计算这三个向量值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cedd62-01e1-4196-a575-baecc6095601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# 计算第 2 个 token 对应的 q^(2), k^(2), v^(2)（此处用矩阵乘法实现线性投影）\n",
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "# 结果向量维度为 d_out（这里是 2）\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
   "metadata": {},
   "source": [
    "- 我们可以清晰地看到,embedding被降维了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 对所有 token 做投影：得到 keys/values 矩阵\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "# keys.shape == values.shape == (T, d_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
   "metadata": {},
   "source": [
    "- 在下一步 **步骤 2** 中，我们通过计算查询向量和每个键向量之间的点积来计算未归一化的注意力得分："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
   "metadata": {},
   "source": [
    "<img src=\"../image/15.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cbc253-a182-4490-a765-246979ea0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# 计算一个单独的注意力得分：ω_22 = q^(2) · k^(2)\n",
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
   "metadata": {},
   "source": [
    "- 因为我们有六个输入,所以我们有六个attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14e44b5-d170-40f9-8847-8990804af26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# 一次性计算 q^(2) 对所有 key 的注意力得分：ω_2j（shape: (T,)）\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)\n",
    "# (d_out,) @ (d_out, T) -> (T,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
   "metadata": {},
   "source": [
    "<img src=\"../image/16.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
   "metadata": {},
   "source": [
    "- 接下来，在 **步骤 3** 中，我们使用之前提到的 softmax 函数计算注意力权重（归一化后的注意力得分，总和为 1）。\n",
    "- 与之前的不同之处在于，我们现在通过将注意力得分除以嵌入维度的平方根 $\\sqrt{d_k}$（即 `d_k**0.5`）来对注意力得分进行缩放："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146f5587-c845-4e30-9894-c7ed3a248153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# 对注意力得分做缩放 softmax（scaled dot-product attention 的关键一步）\n",
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "# 除以 sqrt(d_k) 可以避免 d_k 较大时 softmax 进入饱和区导致梯度不稳定\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
   "metadata": {},
   "source": [
    "<img src=\"../image/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
   "metadata": {},
   "source": [
    "- 在**第四步**, 我们可以计算每一个token的向量了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# 用注意力权重对 values 加权求和，得到上下文向量 z^(2)（shape: (d_out,)）\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
   "metadata": {},
   "source": [
    "### 3.4.2 自注意模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313410-3155-4d90-a7a3-2f3386e73677",
   "metadata": {},
   "source": [
    "- 下面是代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# SelfAttention_v1: 最简自注意力层（单头）\n",
    "# 输入 x.shape == (T, d_in)，输出 context_vec.shape == (T, d_out)\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        # 三个可学习权重矩阵：W_q, W_k, W_v（形状均为 (d_in, d_out)）\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        # 这里使用 rand 只是为了演示；实际训练时会从更合适的初始化开始\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性投影：把输入从 d_in 映射到 d_out\n",
    "        # keys/queries/values.shape == (T, d_out)\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        # 注意力得分矩阵 omega：attn_scores.shape == (T, T)\n",
    "        # 其中 attn_scores[i, j] 表示第 i 个 token 作为 query 对第 j 个 token 的打分\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        # dim=-1：对每一行做 softmax，使每个 query 的注意力权重和为 1\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
   "metadata": {},
   "source": [
    "<img src=\"../image/18.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
   "metadata": {},
   "source": [
    "- 我们可以使用 PyTorch 的 `Linear` 层简化上述实现，禁用偏置项后，`Linear` 层相当于矩阵乘法。\n",
    "- 使用 `nn.Linear` 替代手动使用 `nn.Parameter(torch.rand(...))` 的一个主要优势是，`nn.Linear` 具有推荐的权重初始化方案，这有助于模型训练更加稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# SelfAttention_v2: 用 nn.Linear 重写 v1（更符合 PyTorch 常见写法）\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # nn.Linear 会学习一个线性变换（可选 bias），用于生成 Q/K/V\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # 使用 Linear 的好处：自带更推荐的权重初始化方案\n",
    "\n",
    "    def forward(self, x):\n",
    "        # keys/queries/values.shape == (T, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T \n",
    "        # 归一化前先缩放（除以 sqrt(d_k)）再做 softmax\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        # context_vec.shape == (T, d_out)\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
   "metadata": {},
   "source": [
    "- `SelfAttention_v1` 和 `SelfAttention_v2` 会给出不同的输出，因为它们使用了不同的初始权重矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
   "metadata": {},
   "source": [
    "## 3.5 对未出现的信息的隐藏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
   "metadata": {},
   "source": [
    "- 在casual attention，对角线以上的注意力权重被掩蔽，确保在计算上下文向量时，LLM 无法利用位置的信息来调整注意力权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
   "metadata": {},
   "source": [
    "<img src=\"../image/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
   "metadata": {},
   "source": [
    "### 3.5.1 因果自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
   "metadata": {},
   "source": [
    "- 在这一节中，我们将把之前的自注意力机制转换为因果自注意力机制。\n",
    "- 因果自注意力确保模型在预测序列中某个位置的值时，仅依赖于前面已知位置的输出，而不依赖于后续位置。\n",
    "- 换句话说，这确保了每个下一个词的预测仅依赖于前面的词。\n",
    "- 为了实现这一点，对于每个给定的标记，我们会将“未知的信息”（即输入文本中当前token之后的token）掩蔽掉："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a",
   "metadata": {},
   "source": [
    "<img src=\"../image/20.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9",
   "metadata": {},
   "source": [
    "- 为了说明和实现因果自注意力，让我们使用上一节中的注意力得分和权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 为了演示因果 mask，这里复用上一节 sa_v2 的投影层来生成 Q/K\n",
    "# 注意：这一步还没有应用 mask，只是在计算未掩蔽的注意力权重\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)\n",
    "# attn_weights.shape == (T, T)，每一行是一个 query 的注意力分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
   "metadata": {},
   "source": [
    "- 隐藏未知信息的attention score最简单的方法是通过 PyTorch 的 `tril` 函数进行掩蔽，其中主对角线以下的元素（包括对角线本身）设置为 1，主对角线以上的元素设置为 0："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 构造一个最简单的因果 mask：下三角为 1（允许关注自己及之前），上三角为 0（屏蔽未来）\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "# mask_simple.shape == (T, T)\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
   "metadata": {},
   "source": [
    "- 然后，我们可以将注意力权重与这个mask相乘，以将对角线以上的注意力得分置为零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 如果在 softmax 之后再乘 mask，上三角会变成 0，但行和不再是 1（概率分布被破坏）\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)\n",
    "# 下面会展示如何重新归一化，或更高效地在 softmax 之前掩蔽\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb35787-cf12-4024-b66d-e7215e175500",
   "metadata": {},
   "source": [
    "- 如果在 softmax 之后进行掩蔽，它会破坏 softmax 所创建的概率分布。\n",
    "- softmax 确保所有输出值的总和为 1。\n",
    "- 如果在 softmax 之后进行掩蔽，就需要重新归一化输出，确保其总和为 1，这会使过程更加复杂，并可能带来意想不到的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
   "metadata": {},
   "source": [
    "- 我们可以用以下方式确保所有的数据都是归一化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 重新归一化：让每一行的注意力权重和恢复为 1\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "# masked_simple_norm 的每一行再次是概率分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
   "metadata": {},
   "source": [
    "- 尽管我们在技术上已经完成了因果注意力机制的编码，但让我们简要地探讨一种更高效的方法，以实现与上述相同的效果。\n",
    "- 因此，在注意力得分进入 softmax 函数之前，我们可以将对角线以上的未归一化注意力得分用负无穷大进行掩蔽，而不是将其置零并重新归一化："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb682900-8df2-4767-946c-a82bee260188",
   "metadata": {},
   "source": [
    "<img src=\"../image/21.webp\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 更常见/高效的做法：在 softmax 之前把“未来位置”的得分直接设为 -inf\n",
    "# 这样 softmax 后这些位置会变成 0，并且无需额外再归一化\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# mask.shape == (T, T)：对角线以上（未来 token）为 1，其余为 0\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# masked[i, j] 在 j>i 的位置被置为 -inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
   "metadata": {},
   "source": [
    "- 结果显然是归一化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 在掩蔽后的得分上做 scaled softmax，得到真正的因果注意力权重\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
   "metadata": {},
   "source": [
    "### 3.5.2 使用Dropout防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
   "metadata": {},
   "source": [
    "- 此外，我们还应用了丢弃（Dropout）来减少训练过程中的过拟合。\n",
    "- dropout可以应用于多个位置：\n",
    "  - 例如，在计算注意力权重之后；\n",
    "  - 或在将注意力权重与值向量相乘之后。\n",
    "- 在这里，我们选择在计算注意力权重之后应用丢弃掩码，因为这种做法更为常见。\n",
    "\n",
    "- 另外，在此示例中，我们使用了50%的丢弃率，这意味着随机屏蔽掉一半的注意力权重。（在后续训练GPT模型时，我们会使用更低的丢弃率，例如0.1或0.2。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee799cf6-6175-45f2-827e-c174afedb722",
   "metadata": {},
   "source": [
    "<img src=\"../image/22.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575458-a6da-4e54-8688-83e155f2de06",
   "metadata": {},
   "source": [
    "- 如果我们应用0.5（50%）的丢弃率，未被抛弃的值将相应地被缩放一个因子，1/0.5 = 2。\n",
    "- 这种缩放通过公式 1 / (1 - `dropout_rate`) 计算得出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de578db-8289-41d6-b377-ef645751e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) \n",
    "# dropout rate=0.5：随机将一半元素置 0；未被置 0 的元素会乘以 1/(1-0.5)=2 进行缩放\n",
    "example = torch.ones(6, 6) \n",
    "# 构造一个全 1 矩阵用来观察 dropout 的“置零 + 缩放”效果\n",
    "\n",
    "print(dropout(example))\n",
    "# 注意：dropout 只在训练模式下生效（nn.Module 默认是 train()）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 把 dropout 应用到注意力权重上：会随机屏蔽部分注意力连接（常见正则化手段）\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269df5c8-3e25-49d0-95d3-bb232287404f",
   "metadata": {},
   "source": [
    "- 生成的输出可能会因操作系统的不同而有所不同；\n",
    "- 你可以在 [PyTorch 问题追踪器](https://github.com/pytorch/pytorch/issues/121595) 上了解更多内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
   "metadata": {},
   "source": [
    "### 3.5.3 实现一个简洁的因果自注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
   "metadata": {},
   "source": [
    "- 现在，我们准备实现一个完整的自注意力机制，包含因果掩码和dropout。\n",
    "- 另一项任务是实现代码以处理包含多个输入的批次，确保我们的 `CausalAttention` 类能够支持第二章中实现的数据加载器所生成的批量输出。\n",
    "- 为了简化起见，我们通过复制输入文本示例来模拟批量输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 模拟一个 batch：把同一个序列复制两份堆叠在 batch 维度上\n",
    "# batch.shape == (batch_size=2, num_tokens=6, d_in=3)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "# torch.stack 会新增一个维度；dim=0 表示放在最前面作为 batch 维\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de8253-5387-4dd1-b53b-72f8f10b9ecb",
   "metadata": {},
   "source": [
    "- 缩放因子  \\sqrt{d}  的引入解决了注意力机制中的数值不稳定问题。\n",
    "- 它确保了即使嵌入维度  d  较大，点积得分也能被合理地控制在一个适当范围，方便 Softmax 生成平滑的注意力分布，且梯度不会过大或过小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        # 单头因果自注意力：带 causal mask + dropout，并支持 batch 输入\n",
    "        # 输入 x.shape == (b, T, d_in)；输出 context_vec.shape == (b, T, d_out)\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        # register_buffer: mask 不是可训练参数，但会随模型一起保存，并自动跟随 device 移动\n",
    "        # mask.shape == (context_length, context_length)，对角线以上为 1（未来位置），其余为 0\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # b: batch_size，num_tokens: 序列长度 T，d_in: 输入维度\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        # keys/queries/values.shape == (b, T, d_out)\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        # attn_scores.shape == (b, T, T)\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1## 缩放因子 √d，用于稳定梯度\n",
    "        )\n",
    "        # dim=-1：对每个 query 的最后一维做 softmax，得到注意力权重（每行和为 1）\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "        context_vec = attn_weights @ values\n",
    "        # context_vec.shape == (b, T, d_out)\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd",
   "metadata": {},
   "source": [
    "- Dropout仅在训练时要使用,验证时不需要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50",
   "metadata": {},
   "source": [
    "<img src=\"../image/23.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
   "metadata": {},
   "source": [
    "## 3.6 拓展单头至多方注意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
   "metadata": {},
   "source": [
    "### 3.6.1 堆叠多个单头注意力层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
   "metadata": {},
   "source": [
    "- 以下是之前实现的自注意力机制总结（为简化起见，未展示因果和dropout掩码）：\n",
    "\n",
    "- 这种机制也称为单头注意力：\n",
    "\n",
    "<img src=\"../image/24.webp\" width=\"400px\">\n",
    "\n",
    "- 我们通过堆叠多个单头注意力模块来构建多头注意力模块：\n",
    "\n",
    "<img src=\"../image/25.webp\" width=\"400px\">\n",
    "\n",
    "- 多头注意力的核心思想是使用不同的学习到的线性投影，并行地多次运行注意力机制。这使得模型能够在不同位置同时关注来自不同表示子空间的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a341bf-122a-4e31-99db-39259cf8d1b2",
   "metadata": {},
   "source": [
    "在 Python 中，super().__init__() 是一种调用父类（基类）构造函数的方法，常用于类继承的场景中。它确保子类能够正确初始化父类的属性和方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadAttentionWrapper: 用多个 CausalAttention 头并行计算，然后在最后一维拼接\n",
    "# 注意：这是“直观实现”，便于理解；下一节会实现更紧凑高效的 MultiHeadAttention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__() \n",
    "        # 创建 num_heads 个注意力头，每个头输出维度为 d_out\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # 拼接后输出维度为 num_heads * d_out\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
   "metadata": {},
   "source": [
    "- 在上述实现中，嵌入维度为4，因为我们将 `d_out=2` 作为键、查询和值向量以及上下文向量的嵌入维度。由于使用了2个注意力头，输出嵌入维度为 2 * 2 = 4。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
   "metadata": {},
   "source": [
    "### 3.6.2 利用权重拆分实现多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
   "metadata": {},
   "source": [
    "- 尽管上述实现是一种直观且功能完整的多头注意力机制（通过封装之前的单头注意力 `CausalAttention` 实现），我们仍可以编写一个独立的 `MultiHeadAttention` 类来实现相同的功能。\n",
    "\n",
    "- 在这个独立的 `MultiHeadAttention` 类中，我们不会将单个注意力头进行拼接。\n",
    "- 相反，我们会创建独立的 W_query、W_key 和 W_value 权重矩阵，并将它们拆分为每个注意力头的单独矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadAttention: 将所有头的 Q/K/V 一次性算出，再 reshape/transpose 拆成多头并行计算\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        # 确保每个头的维度 head_dim 是整数\n",
    "            \n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "        # head_dim: 每个注意力头的通道数；d_out = num_heads * head_dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        # out_proj: 将拼接后的多头输出再做一次线性变换（常见做法）\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        # 上三角掩码，确保因果性\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        # 下面把最后一维 d_out 拆成 (num_heads, head_dim)，让每个头独立做 attention\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # 把 num_heads 维度提前：便于并行计算每个 head 的注意力\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # 计算缩放点积注意力\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        # 将掩码缩减到当前 token 数量，并转换为布尔型\n",
    "        # 进而实现动态遮蔽,所以不用另开好几个数组\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # 遮蔽矩阵\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        #归一化\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        # 把 head 维度再换回到 (b, num_tokens, num_heads, head_dim)\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        #对上下文向量的形状进行调整，确保输出的形状\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # contiguous(): 确保内存连续，便于 view 正确工作\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
   "metadata": {},
   "source": [
    "- 请注意，上述实现本质上是 `MultiHeadAttentionWrapper` 的重写版本，并且更加高效。  \n",
    "- 生成的输出看起来略有不同，因为随机权重初始化有所不同，但两者都是完全可用的实现，可以在我们将在后续章节中实现的 GPT 类中使用。  \n",
    "- 另外，值得注意的是，我们在上面的 `MultiHeadAttention` 类中添加了一个线性投影层（`self.out_proj`）。这只是一个线性变换，不改变维度。在 LLM 实现中，使用这样的投影层是标准做法，但它并非严格必要（近期的研究表明，去除该层不会影响模型的表现；请参阅本章末尾的进一步阅读部分）。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5d396-c990-45dc-9908-2c621461f851",
   "metadata": {},
   "source": [
    "<img src=\"../image/26.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
   "metadata": {},
   "source": [
    "- 请注意，如果你对上述内容的紧凑和高效实现感兴趣，可以考虑使用PyTorch中的 [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) 类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363701ad-2022-46c8-9972-390d2a2b9911",
   "metadata": {},
   "source": [
    "- 由于上述实现初看起来可能有些复杂，我们来看一下执行 `attn_scores = queries @ keys.transpose(2, 3)` 时会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a @ a.transpose(2, 3))\n",
    "# 对每个 head 单独计算相关性矩阵：(num_tokens, head_dim) @ (head_dim, num_tokens)\n",
    "# 输出 shape 为 (b, num_heads, num_tokens, num_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b",
   "metadata": {},
   "source": [
    "- 在这种情况下，PyTorch 中的矩阵乘法实现将处理四维输入张量，使得矩阵乘法在最后两个维度（`num_tokens`, `head_dim`）之间进行，然后对各个头进行重复计算。  \n",
    "\n",
    "- 例如，以下方法提供了一种更紧凑的方式来分别计算每个头的矩阵乘法：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "# 取出第一个 head：shape (num_tokens, head_dim)\n",
    "first_res = first_head @ first_head.T\n",
    "# 得到该 head 的相关性矩阵：shape (num_tokens, num_tokens)\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec671bf-7938-4304-ad1e-75d9920e7f43",
   "metadata": {},
   "source": [
    "# 总结与收获"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25",
   "metadata": {},
   "source": [
    "- 请参阅 [./multihead-attention.ipynb](./multihead-attention.ipynb) 代码笔记本，它是数据加载器（第2章）的简洁版本，加上我们在本章实现的多头注意力类，后续章节中训练GPT模型时将需要使用。\n",
    "- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到习题解答。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="ch02/01_main-chapter-code/ch02.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"../image/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "# 第二章:处理文本数据 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "本章节需要安装的包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a1d01-7ca9-44a5-b8ce-00624232ac0b",
   "metadata": {},
   "source": [
    "pip3 install importlib.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "# 确认库已安装并显示当前安装的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "- 本章节已经为LLM的实现构建了数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"../image/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7",
   "metadata": {},
   "source": [
    "## 2.1 理解文字embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6",
   "metadata": {},
   "source": [
    "- 无代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69dab7-a433-427a-9e5b-b981062d6296",
   "metadata": {},
   "source": [
    "- 在众多形式的embedding中,我们只讨论text embedding\n",
    "- embedding含义丰富,而且是常用词汇,所以以下皆不做翻译,多加体会!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4",
   "metadata": {},
   "source": [
    "<img src=\"../image/02.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba",
   "metadata": {},
   "source": [
    "- LLM从高纬空间视角理解文字(i.e., 上千个dimension)\n",
    "- 虽然人类只能想象低维的视角,我们无法描绘计算机所理解的embedding\n",
    "- 但是下图我们粗浅的从二维上模拟计算机的视角"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6",
   "metadata": {},
   "source": [
    "<img src=\"../image/03.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "## 2.2 文本标签化(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4876035",
   "metadata": {},
   "source": [
    "- 关于embedding token,实在是不好翻译,于是有时候我会选择不去翻译这两个专有名词\n",
    "- 本节中,我们将tokenize文字信息. 这会把文字拆解为更多小的理解单元 例如单个词或者字节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d73d0",
   "metadata": {},
   "source": [
    "(这也有点抽象，事实上可以粗略理解为将单词拆分为词根、词源和词缀)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {},
   "source": [
    "<img src=\"../image/04.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {},
   "source": [
    "- 载入源文件\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) 一本无版权的短篇小说"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e9b441-cf4e-4a4e-8e3e-44be25354259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os##导入os库    \n",
    "import urllib.request ##导入request库\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):##如果文件不存在则创建，防止因文件已存在而报错\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)##从指定的地点读取文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce",
   "metadata": {},
   "source": [
    "- 如果在执行前面的代码单元时遇到 `ssl.SSLCertVerificationError` 错误可能是由于使用了过时的 Python 版本；\n",
    "- 你可以在 [GitHub 上查阅更多信息](https://github.com/rasbt/LLMs-from-scratch/pull/403)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read() ##读入文件按照utf-8\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))##先输出总长度\n",
    "print(raw_text[:99])##输出前一百个内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {},
   "source": [
    "- 目标是对这段文本进行分词和嵌入处理，以便用于大语言模型（LLM）。\n",
    "- 我们将基于一些简单的示例文本开发一个简单的分词器，之后可以将其应用于上述文本。\n",
    "- 以下正则表达式将基于空格进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)##正则表达式按照空白字符进行分割\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3",
   "metadata": {},
   "source": [
    "- 优化正则表达,可以分割更多的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)##只是按照, .分割\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad",
   "metadata": {},
   "source": [
    "- 移除空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "##把上述结果去掉空格\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e8694-181e-496f-895d-7cb7d92b5562",
   "metadata": {},
   "source": [
    "- 我们还需要处理其他标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) ##就是按照常用的符号分割\n",
    "result = [item.strip() for item in result if item.strip()]##去掉两端的空白字符 也是去掉了空字符串与仅包含空白字符的项\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4",
   "metadata": {},
   "source": [
    "- 万事俱备，我们现在来看一下文字处理后的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {},
   "source": [
    "<img src=\"../image/05.webp\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) ##按照符号继续把原文件给分割了\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]##去掉两端的空白字符 也是去掉了空字符串和仅包含空白字符的项\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95",
   "metadata": {},
   "source": [
    "- 查看token的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {},
   "source": [
    "## 2.3 给token编号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "- 通过如下的embedding层,我们可以给token编号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {},
   "source": [
    "<img src=\"../image/06.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5973794-7002-4202-8b12-0900cd779720",
   "metadata": {},
   "source": [
    "- 我们要创建一个表格,给所有的token给映射到不同的标号上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))#从去掉重复的字符\n",
    "vocab_size = len(all_words)#计总的单词书\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}##先把word进行编号,再按照单词或者标点为索引(有HashList那味道了)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {},
   "source": [
    "- 看一下前50个是怎样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break ##遍历到前五十个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {},
   "source": [
    "- 接下来,我们将通过一个短文本来感受下,处理后的效果是怎样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {},
   "source": [
    "<img src=\"../image/07.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {},
   "source": [
    "- 现在将所有内容整合到一个分词器类中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:#一个实例的名字创立\n",
    "    def __init__(self, vocab): ## 初始化一个字符串\n",
    "        self.str_to_int = vocab #单词到整数的映射\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} \n",
    "        #方便解码,进行整数到词汇的反向映射\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)##正则化分词标点符号\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()## 去掉两端空格与全部的空句\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]##整理完的额字符串列表对应到id,从字典出来\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #映射整数id到字符串。join是用前面那个(“ ”)联结成一个完整的字符串\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #使用正则表达式，去除标点符号前的多余空格\n",
    "        # \\s+匹配一个或者多个空白  \\1 替换到匹配\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "- `encode` 函数将文本转换为token ID。\n",
    "- `decode` 函数将token ID 转换回文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"../image/08.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "- 我们可以使用分词器将文本编码（即分词）为数字。\n",
    "- 然后，这些整数可以作为大语言模型（LLM）的输入，进行嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab) #用vocab创造一个实例\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text) #按照这个例子里的encode函数处理text\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {},
   "source": [
    "- 把数字重新映射回文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)#按照这个例子里的decode函数处理text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))#按照这个例子里的decode函数处理(#按照这个例子里的encode函数处理text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
   "metadata": {},
   "source": [
    "## 2.4 添加特殊token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443",
   "metadata": {},
   "source": [
    "- 文本的结尾需要特别的符号来表明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4",
   "metadata": {},
   "source": [
    "<img src=\"../image/09.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
   "metadata": {},
   "source": [
    "- 一些分词器使用特殊token来帮助大语言模型（LLM）获取额外的上下文信息。\n",
    "- 其中一些特殊token包括：\n",
    "  - `[BOS]`（序列开始）表示文本的开始。\n",
    "  - `[EOS]`（序列结束）表示文本的结束（通常用于连接多个不相关的文本，例如两个不同的维基百科文章或两本不同的书籍等）。\n",
    "  - `[PAD]`（填充）如果我们使用大于1的批次大小训练LLM（我们可能会包含不同长度的多篇文本），使用填充token将较短的文本填充至最长的长度，以确保所有文本具有相同的长度。\n",
    "- `[UNK]` 用于表示词汇表中没有的词。\n",
    "\n",
    "- 请注意，GPT-2不需要上述提到的这些token，它只使用 `<|endoftext|>` token。\n",
    "- `<|endoftext|>` 类似于上述提到的 `[EOS]` token。\n",
    "- GPT 还使用 `<|endoftext|>` 进行填充（因为我们在批量输入训练时通常使用掩码，所以无论这些填充token是什么，都不会影响模型的训练，因为填充token不会被关注）。\n",
    "- GPT-2 不使用 `<UNK>` token来表示词汇表外的词；相反，GPT-2 使用字节对编码（BPE）分词器，将单词分解为子词单元，后续将进一步讨论这一点。\n",
    "- 我们在两个独立文本之间使用 `<|endoftext|>` token："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52442951-752c-4855-9752-b121a17fef55",
   "metadata": {},
   "source": [
    "<img src=\"../image/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a397-da06-4a86-ac27-072dbe7cb172",
   "metadata": {},
   "source": [
    "- 看一下接下来会发生什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5767eff-440c-4de1-9289-f789349d6b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(vocab)  \u001b[38;5;66;03m##用vocab创造一个实例\u001b[39;00m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\u001b[38;5;66;03m##正则化分词标点符号\u001b[39;00m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;66;03m## 去掉两端空格与全部的空句\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\u001b[38;5;66;03m##整理完的额字符串列表对应到id,从字典出来\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\u001b[38;5;66;03m##正则化分词标点符号\u001b[39;00m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;66;03m## 去掉两端空格与全部的空句\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\u001b[38;5;66;03m##整理完的额字符串列表对应到id,从字典出来\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)  ##用vocab创造一个实例\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf",
   "metadata": {},
   "source": [
    "- 上述操作会产生一个错误，因为单词“Hello”不在词汇表中。\n",
    "- 为了处理这种情况，我们可以向词汇表中添加类似 `\"<|unk|>\"` 的特殊token，用于表示未知词汇。\n",
    "- 因为我们已经在扩展词汇表，那么我们可以再添加一个token `\"<|endoftext|>\"`，该token在GPT-2训练中用于表示文本的结束（它也用于连接的文本之间，例如当我们的训练数据集包含多篇文章、书籍等时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))#set去重 list把处理后的重新变为列表,然后排序\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])#加上未知的表示\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "#遍历 enumerate(all_tokens) 中的每个元组 (integer, token)，以 token 作为键，integer 作为值创建字典条目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):#输出后五个内容与其标号\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453",
   "metadata": {},
   "source": [
    "- 因此增加 `<unk>`不失为一种好的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948861c5-3f30-4712-a234-725f20d26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:##版本2.0,启动!\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}#s为单词,i是key\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)#正则化按照标点分类\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]#去掉两头与所有空余句\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "            #遍历 preprocessed 中的每个 item，如果 item 存在于 self.str_to_int（即词汇表）中，就保留 item\n",
    "            #如果不存在（即该单词或符号未定义在词汇表中），就替换为特殊token <|unk|>。\n",
    "            #拓展:推导式（如列表推导式）是一种紧凑的语法，专门用于生成新列表（或其他容器）\n",
    "            #与普通 for 循环相比，它更加简洁和高效，但逻辑复杂时可能会降低可读性。\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]#单词或标点映射为整数列表\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2e942",
   "metadata": {},
   "source": [
    "- 用优化后的分词器对文本进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))#用句子分隔符链接两个句子\n",
    "\n",
    "print(text) #跟第一个一样,但不会报错了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "## 2.5 字节对编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "- GPT-2 使用字节对编码（BPE）作为其分词器。\n",
    "- 这种方式允许模型将不在预定义词汇表中的单词分解为更小的子词单元，甚至是单个字符，从而使其能够处理词汇表外的词汇。\n",
    "- 例如，如果 GPT-2 的词汇表中没有“unfamiliarword”这个单词，它可能会将其分词为 [\"unfam\", \"iliar\", \"word\"]，或者根据训练好的 BPE 合并规则进行其他子词分解。\n",
    "- 原始的 BPE 分词器可以在这里找到：[https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- 在本章中，我们使用了 OpenAI 开源的 [tiktoken](https://github.com/openai/tiktoken) 库中的 BPE 分词器，该库在 Rust 中实现了核心算法，以提高计算性能。\n",
    "- 在 [./bytepair_encoder](../02_bonus_bytepair-encoder) 中创建了一个笔记本，对比了这两种实现的效果（在样本文本上，tiktoken 的速度大约快了 5 倍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))#验证下载并输出版本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")#初始化GPT2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})#输出分词的id,可以允许endoftext\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "#按照数字解码回去\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "- BPE tokenizers将未知词汇分解为子词和单个字符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"../image/11.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## 2.6 利用滑动窗口进行数据采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- 现在我们训练的大语言模型（LLMs）时是一次生成一个单词，因此希望根据训练数据的要求进行准备，使得序列中的下一个单词作为预测目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"../image/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)#读入了一个text并编码到enc_text里面\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
   "metadata": {},
   "source": [
    "- 对于每个文本块，我们需要输入和目标。\n",
    "- 由于我们希望模型预测下一个单词，因此我们要生成目标是将输入右移一个位置后的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]#从第五十一个开始向后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #sliding windows4\n",
    "\n",
    "x = enc_sample[:context_size]#开头四个\n",
    "y = enc_sample[1:context_size+1]#第二个开始的四个\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
   "metadata": {},
   "source": [
    "- 就像预言家一个晚上只能预言一个玩家,我们的模型一次也只能预测一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    #文本成输入 context,先输出有什么,然后输出下一个是什么编号\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    #文本成输入 context,先输出有什么,然后输出下一个是什么单词\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
   "metadata": {},
   "source": [
    "- 我们将在后续章节中处理下一个单词预测，届时会介绍注意力机制。\n",
    "- 目前，我们实现一个简单的数据加载器，它遍历输入数据集并返回右移一个位置后的输入和目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
   "metadata": {},
   "source": [
    "- 安装并导入 PyTorch（安装提示请参见附录A）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- 用滑动窗口法运行,窗口位置每次加一\n",
    "\n",
    "<img src=\"../image/13.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "- 创建数据集和数据加载器，从输入文本数据集中提取文本块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"\n",
    "    自定义数据集类，用于将输入文本按滑动窗口切割成GPT训练用的样本对。\n",
    "    每个样本包含：\n",
    "      - 输入序列（input_ids）：长度为 max_length 的token列表\n",
    "      - 目标序列（target_ids）：长度同上，对应input_ids的右移一位版本（即“下一个词”）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        参数说明:\n",
    "            txt:       完整文本字符串，作为训练语料\n",
    "            tokenizer: 分词器（应支持 encode 方法，将文本转为id序列）\n",
    "            max_length: 每个输入序列的最大长度（上下文窗口长度）\n",
    "            stride:    滑窗步长（窗口每次向后滑动的token数，stride < max_length会有重叠）\n",
    "        \"\"\"\n",
    "        self.input_ids = []   # 存储输入序列（每个元素为长度max_length的1D tensor）\n",
    "        self.target_ids = []  # 存储目标序列（与input_ids一一对应）\n",
    "\n",
    "        # 1. 将完整文本序列化为token id序列\n",
    "        # allowed_special={\"<|endoftext|>\"} 允许特殊符号 \"<|endoftext|>\" 出现在编码中\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})  # 例如 [15496, 11, 318, ...]\n",
    "\n",
    "        # 2. 用滑动窗口法切分数据（得到有重叠的输入-目标对）\n",
    "        # 例如: max_length=4, stride=2, token_ids=[1,2,3,4,5,6 ...]\n",
    "        #   第1个样本: input:[1,2,3,4], target:[2,3,4,5]\n",
    "        #   第2个样本: input:[3,4,5,6], target:[4,5,6,7] （for i in range(0, ..., stride)）\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]                   # 当前样本输入\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]          # 当前样本对应的“下一个词”\n",
    "            self.input_ids.append(torch.tensor(input_chunk))              # 转为tensor存储\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "        # 注意: 一共会生成  floor((len(token_ids) - max_length) / stride) 个样本\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回样本数量，即所有滑窗所得的输入序列个数。\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        按索引返回(input_ids, target_ids)的tensor对。\n",
    "        用于dataloader按批次取样本。\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"\n",
    "    根据提供的文本txt，创建一个适用于GPT风格小模型训练的数据加载器（DataLoader）。\n",
    "    支持分批次（batch）、滑动窗口切分、数据混洗等功能。\n",
    "\n",
    "    参数说明:\n",
    "        txt:           完整的文本字符串，将作为训练语料使用\n",
    "        batch_size:    每个批次包含的样本数\n",
    "        max_length:    每个输入序列的最大长度（即上下文窗口大小）\n",
    "        stride:        每次滑窗向后移动的token数（stride < max_length表示有重叠）\n",
    "        shuffle:       是否对数据集中的样本顺序进行混洗（通常训练要True，验证/测试为False）\n",
    "        drop_last:     如果最后一个批次不足batch_size，是否丢弃（True只保留完整批次）\n",
    "        num_workers:   多少个子进程加载数据，0为主进程（小数据为0即可）\n",
    "\n",
    "    返回:\n",
    "        返回一个PyTorch的DataLoader实例，可以迭代得到(batch_size, max_length)大小的张量对。\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 初始化分词器（Tokenizer）\n",
    "    # 这里使用tiktoken库提供的GPT2模型分词器，将原始文本拆分为token（数字id），\n",
    "    # 后续输入到模型中。tiktoken.get_encoding(\"gpt2\")会返回一个“编码对象”。\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 2. 构建自定义数据集对象GPTDatasetV1\n",
    "    # 这个数据集会将文本先编码为token id序列，再按滑动窗口切成一对对的\n",
    "    # (input_ids, target_ids)。这样每一条样本都是模型一次要“看”的上下文窗口。\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    # dataset长度约等于 floor((len(token_id序列) - max_length) / stride)\n",
    "    # 每条样本都是两组长度max_length的id列表\n",
    "\n",
    "    # 3. 构建PyTorch原生的DataLoader\n",
    "    # 用于批量加载dataset里的样本，并支持shuffle、drop_last、并行读取等功能。\n",
    "    dataloader = DataLoader(\n",
    "        dataset,                  # 数据集对象，__getitem__返回(input_ids, target_ids)\n",
    "        batch_size=batch_size,    # 批次大小，即每次返回多少个样本组成一批\n",
    "        shuffle=shuffle,          # 是否混洗样本（训练常开，推理常关）\n",
    "        drop_last=drop_last,      # 若最后一批不足量是否丢弃\n",
    "        num_workers=num_workers   # 子进程数量（文本小可为0；大数据集可加速读取）\n",
    "    )\n",
    "\n",
    "    # 4. 返回构建好的dataloader对象\n",
    "    # 用户可以像for batch in dataloader: ... 这样遍历每个批次。\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    "- 让我们使用批次大小为1、上下文大小为4的设置，测试数据加载器在大语言模型（LLM）中的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个dataloader对象，负责将原始文本raw_text按批次（batch_size=1）、上下文窗口大小（max_length=4）、滑窗步幅（stride=1）分割成训练样本，并且不打乱顺序 (shuffle=False)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "# 将dataloader转换为迭代器，用于逐批次遍历样本\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# 取出第一个批次的数据（每个batch是一个(input_ids, target_ids)的元组，都是长度为max_length的token id张量）\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(first_batch)  # 输出第一个批次的内容，便于检查分割效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- 下面是一个示例，步幅等于上下文长度（此处为4）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"../image/14.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "- 我们还可以批量输出。\n",
    "- 因为过多的重叠可能导致过拟合,这里增加了步幅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 create_dataloader_v1 创建一个数据加载器对象 dataloader\n",
    "# - raw_text: 原始文本字符串，将作为数据集来源\n",
    "# - batch_size=8: 每次迭代返回8个样本组成一个批次\n",
    "# - max_length=4: 每个样本的输入和目标序列的长度（token数量）为4\n",
    "# - stride=4: 窗口滑动步幅为4，即每获取一个样本后，将窗口向右移动4个token，下一个样本不会与前一个样本重叠\n",
    "# - shuffle=False: 不打乱样本顺序，按文本顺序生成\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 将 dataloader 转为迭代器以便用 next() 按批次获取数据\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# 获取数据加载器的下一个批次\n",
    "# - 返回一个二元组(inputs, targets)，它们都是 shape=(batch_size, max_length) 的张量\n",
    "#   - inputs: 当前批次的输入token id序列\n",
    "#   - targets: 当前批次的目标token id序列（一般是inputs右移一位的下一个token）\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "# 打印本批次的输入张量\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "\n",
    "# 打印本批次的目标张量\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
   "metadata": {},
   "source": [
    "## 2.7 创建token嵌入(Creating token embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
   "metadata": {},
   "source": [
    "- 数据集已准备好用于大语言模型（LLM）。\n",
    "- 最后，让我们使用嵌入层将token转换为连续的向量表示。\n",
    "- 通常，这些嵌入层是大语言模型的一部分，并在模型训练过程中进行更新（训练）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
   "metadata": {},
   "source": [
    "<img src=\"../image/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
   "metadata": {},
   "source": [
    "- 假设我们有以下四个输入示例，经过分词后对应的输入 ID 为 2、3、5 和 1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15a6304c-9474-4470-b85d-3991a49fa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])#要加入2,3,5,1的字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6344-2c71-4837-858d-dd120005ba05",
   "metadata": {},
   "source": [
    "- 为了简化问题，假设我们只有一个包含 6 个词的小型词汇表，并且我们希望创建大小为 3 的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定嵌入层支持的唯一token（词汇表）总数，这里我们假设有6个不同的token（编号为0 ~ 5）\n",
    "vocab_size = 6  # 例如 [\"cat\", \"dog\", \"apple\", \"hello\", \"world\", \".\"] 这样的词汇表有6个唯一词\n",
    "\n",
    "# 指定嵌入向量的维度，也就是每个token将被映射为一个3维向量\n",
    "output_dim = 3  # 每个token经过嵌入层后会变成一个长度为3的实数向量\n",
    "\n",
    "# 设置随机种子，保证每次运行输出的结果（如权重初始化）一致，便于调试和复现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建一个嵌入层对象：\n",
    "# - 输入参数 vocab_size 表示一共要学习多少个token的表示\n",
    "# - 输入参数 output_dim 表示每个token的表示有多少维\n",
    "# - 嵌入层可以看作一个学习得到的查找表，其内部参数是一个形状为 (vocab_size, output_dim) 的权重矩阵\n",
    "# - 其中第i行就是第i号token对应的向量表示\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)  # 每一行对应一个token的嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
   "metadata": {},
   "source": [
    "- 结果是个6*3的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a686eb61-e737-4351-8f1c-222913d47468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
   "metadata": {},
   "source": [
    "- 对于熟悉（one-hot encoding）的人来说，上述嵌入层方法本质上只是实现one-hot编码后接矩阵乘法的更高效方式，具体实现可以参考[./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)中的补充代码。\n",
    "- 嵌入层只是对one-hot encoding和矩阵乘法方法的高效实现，因此可以将其视为一个神经网络层，并通过反向传播进行优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
   "metadata": {},
   "source": [
    "- 通过下列操作,我们可以吧id3映射到一个三纬的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
   "metadata": {},
   "source": [
    "- 请注意，上述内容是 `embedding_layer` 权重矩阵中的第四行。\n",
    "- 要嵌入上述所有四个 `input_ids` 值，我们执行以下操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
   "metadata": {},
   "source": [
    "- 嵌入层本质上是一个查找操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
   "metadata": {},
   "source": [
    "<img src=\"../image/16.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
   "metadata": {},
   "source": [
    "- **您可能对比较嵌入层与常规线性层的附加内容感兴趣：[../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
   "metadata": {},
   "source": [
    "## 2.8 位置信息编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24940068-1099-4698-bdc0-e798515e2902",
   "metadata": {},
   "source": [
    "- 嵌入层将 ID 转换为相同的向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
   "metadata": {},
   "source": [
    "<img src=\"../image/17.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
   "metadata": {},
   "source": [
    "- 位置信息与token向量结合，形成大语言模型的最终输入："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
   "metadata": {},
   "source": [
    "<img src=\"../image/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
   "metadata": {},
   "source": [
    "- 字节对编码器的词汇表大小为 50,257：\n",
    "- 假设我们想将输入token编码为一个 256 维的向量表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)#映射为tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
   "metadata": {},
   "source": [
    "- 如果我们从数据加载器中采样数据，我们将每个批次中的token嵌入为一个 256 维的向量。\n",
    "- 如果我们有一个批次大小为 8，每个批次包含 4 个token，这将得到一个 8 x 4 x 256 的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)#调用token_embedding_layer将输入inputs映射为对应的嵌入向量。\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
   "metadata": {},
   "source": [
    "- GPT-2 使用绝对位置嵌入，因此我们只需要创建另一个位置嵌入层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "#目的是为输入序列中的每个位置生成一个向量,表明位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056e371",
   "metadata": {},
   "source": [
    "- 嵌入层本质上是一个查找表,大小为(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# 先生成 [0, 1, ..., max_length-1]，每个位置用一个整数ID表示\n",
    "position_ids = torch.arange(max_length)\n",
    "# 再通过位置嵌入层按ID查表：每个位置ID对应output_dim维的位置向量\n",
    "pos_embeddings = pos_embedding_layer(position_ids)\n",
    "# 打印位置嵌入张量形状，常见为 (max_length, output_dim)\n",
    "\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c33fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4693,  1.0024,  0.6403,  ..., -0.7098, -0.4741,  1.3287],\n",
      "        [-0.3833,  0.5006,  2.1007,  ..., -0.1256,  0.8334, -1.8840],\n",
      "        [ 0.3221,  0.9576, -1.5949,  ...,  0.4771, -0.7206,  0.2753],\n",
      "        [ 0.1482, -1.1207,  1.1867,  ...,  0.5207, -1.0125, -0.3823]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
   "metadata": {},
   "source": [
    "- 为了创建大语言模型（LLM）中使用的输入嵌入，我们只需将token嵌入和位置嵌入相加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b22fab89-526e-43c8-9035-5b7018e34288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings#特征是词语信息跟位置信息的结合\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
   "metadata": {},
   "source": [
    "- 在输入处理流程的初始阶段，输入文本被分割为独立的token。\n",
    "- 在此分割后，这些token根据预定义的单词表转换为token ID："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
   "metadata": {},
   "source": [
    "<img src=\"../image/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63230f2e-258f-4497-9e2e-8deee4530364",
   "metadata": {},
   "source": [
    "# 总结与收获"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f",
   "metadata": {},
   "source": [
    "请参见 [./dataloader.ipynb](./dataloader.ipynb) 代码笔记本，这是我们在本章中实现的数据加载器的简洁版，并将在后续章节中用于训练 GPT 模型。\n",
    "\n",
    "请参见 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 获取习题解答。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="README.md">
<p align="center">
<h1 align="center"> <img src="./imgs/icon/ai.png" width="30" />《从零构建大模型》</h1>
</p>
<p align="center">
    <a href="https://img.shields.io/badge/version-v0.1.0-blue">
      <img alt="version" src="https://img.shields.io/badge/version-v0.1.0-blue?color=FF8000?color=009922" />
    </a>
  <a >
       <img alt="Status-building" src="https://img.shields.io/badge/Status-building-blue" />
    </a>
  <a >
       <img alt="PRs-Welcome" src="https://img.shields.io/badge/PRs-Welcome-red" />
    </a>
    <a href="https://github.com/MLNLP-World/LLMs-from-scratch-CN/stargazers">
       <img alt="stars" src="https://img.shields.io/github/stars/MLNLP-World/LLMs-from-scratch-CN" />
    </a>
    <a href="https://github.com/MLNLP-World/LLMs-from-scratch-CN/network/members">
       <img alt="FORK" src="https://img.shields.io/github/forks/MLNLP-World/LLMs-from-scratch-CN?color=FF8000" />
    </a>
    <a href="https://github.com/MLNLP-World/LLMs-from-scratch-CN/issues">
      <img alt="Issues" src="https://img.shields.io/github/issues/MLNLP-World/LLMs-from-scratch-CN?color=0088ff"/>
    </a>
    <br />
</p>

<div align="center">
<p align="center">
  <a href="#项目动机">项目动机</a>/
  <a href="#课程简介">课程简介</a>/
  <a href="#课程资源">课程资源</a>/
  <a href="#原书Readme">原书Readme</a>/
  <a href="#贡献者">贡献者</a>
</p>
</div>

---

<h2 id="项目动机">
  <img src="./imgs/icon/motivation.png" width="25" /> 项目动机
</h2>


原项目与地址:[《LLMs-from-scratch》](https://github.com/rasbt/LLMs-from-scratch.git)

本项目是对GitHub项目[《LLMs-from-scratch》](https://github.com/rasbt/LLMs-from-scratch.git)内容的中文翻译，包括详细的markdown 笔记和相关的jupyter 代码。翻译过程中，我们尽可能保持原意的准确性，同时对部分内容进行了语序和表达的优化，以更贴合中文学习者的阅读习惯。需要特别说明的是，原作者为该项目的主要贡献者，本汉化版本仅作为学习辅助资料，不对原内容进行修改或延伸。

由于个人能力有限，翻译中可能存在不完善之处，欢迎提出宝贵意见并多多包涵 希望通过这一翻译项目，更多中文学习者能够从中受益，也希望为国内社区的 LLM 学习和研究贡献一份力量。

>本项目的特色：
>**jupyter代码**均有详细中文注释，帮助大家更快上手实践。
>**诸多的附加材料**可以拓展知识

本项目所用徽章来自互联网，如侵犯了您的图片版权请联系我们删除，谢谢。

<h2 id="课程简介">
  <img src="./imgs/icon/intro.png" width="25" /> 课程简介
</h2>

提到大语言模型（LLMs），我们可能会将其视为独立于传统机器学习的领域，但实际上，LLMs 是机器学习的一个重要分支。在深度学习尚未广泛应用之前，机器学习在许多领域（如语音识别、自然语言处理、计算机视觉等）的作用相对有限，因为这些领域往往需要大量的专业知识来应对复杂的现实问题。然而，近几年深度学习的快速发展彻底改变了这一状况，使 LLMs 成为推动人工智能技术革命的关键力量。

原项目与地址:[《LLMs-from-scratch》](https://github.com/rasbt/LLMs-from-scratch.git)
[https://github.com/rasbt/LLMs-from-scratch.git](https://github.com/rasbt/LLMs-from-scratch.git)

在 [《LLMs-from-scratch》](https://github.com/rasbt/LLMs-from-scratch.git)项目中，不仅关注 LLMs 的基础构建，如 Transformer 架构、序列建模 等，还深入探索了 GPT、BERT 等深度学习模型 的底层实现。项目中的每一部分均配备详细的代码实现和学习资源，帮助学习者从零开始构建 LLMs，全面掌握其核心技术。

<h2 id="课程资源">
  <img src="./imgs/icon/resource.png" width="25" /> 课程资源
</h2>

- 英文原版地址：[原版地址](https://github.com/rasbt/LLMs-from-scratch.git)
- 教材网址：[原版教材](amzn.to/4fqvn0D)
- 汉化地址：[https://github.com/MLNLP-World/LLMs-from-scratch-CN.git](https://github.com/MLNLP-World/LLMs-from-scratch-CN.git)

此外，本门课程还有相应的代码实现。**每章都有相应的jupyter记事本，提供模型的完整python代码**，所有的资源都可在网上免费获取。



<h2 id="原书Readme">
  <img src="./imgs/icon/notes.png" width="25" /> 原书Readme
</h2>

# 从零构建大模型

这个仓库包含了开发、预训练和微调一个类似GPT的LLM（大语言模型）的代码，是《从零构建大模型》这本书的官方代码仓库，书籍链接：[从零构建大模型](https://amzn.to/4fqvn0D)。

<br>
<br>

<a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a>

<br>

在《*从零构建大模型*》这本书中，您将逐步了解大语言模型（LLMs）如何从内到外工作，自己动手编写代码，逐步构建一个LLM。在这本书中，我将通过清晰的文字、图示和示例，带您完成构建自己LLM的每一个阶段。

本书描述的训练和开发自己的小型功能性模型的方法，旨在教育用途，类似于用于创建大规模基础模型（如ChatGPT背后的模型）的方法。此外，本书还包括加载更大预训练模型权重进行微调的代码。

- 官方[源代码仓库链接](https://github.com/rasbt/LLMs-from-scratch)
- 汉化版本[汉化仓库链接](https://github.com/GoatCsu/LLMs-from-scratch-CN-CSU)
- [出版商网站上的书籍链接](http://mng.bz/orYv)
- [Amazon.com上的书籍页面链接](https://www.amazon.com/gp/product/1633437167)
- ISBN 9781633437166

<a href="http://mng.bz/orYv#reviews"><img src="https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png" width="220px"></a>

<br>

要下载此仓库的副本，请点击[下载ZIP](https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip)按钮，或者在终端中执行以下命令：

```bash
git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
```

要下载此仓库汉化版本，请点击[下载ZIP](https://github.com/MLNLP-World/LLMs-from-scratch-CN.git)按钮，或者在终端中执行以下命令：

```bash
git clone --depth 1 https://github.com/MLNLP-World/LLMs-from-scratch-CN.git
```

<br>

(如果您是从Manning网站下载的代码包，请访问官方代码仓库 [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) 获取最新的更新
或者汉化版本[https://github.com/MLNLP-World/LLMs-from-scratch-CN.git](https://github.com/MLNLP-World/LLMs-from-scratch-CN.git))

<br>

# 目录

请注意，本文档是一个Markdown (`.md`) 文件。如果您是从Manning网站下载的代码包并在本地查看它，建议使用Markdown编辑器或预览器进行正确查看。如果您尚未安装Markdown编辑器，[MarkText](https://www.marktext.cc) 是一个不错的免费选项。

您也可以通过浏览器访问GitHub上的[代码仓库](https://github.com/rasbt/LLMs-from-scratch)，或者[汉化版](https://github.com/MLNLP-World/LLMs-from-scratch-CN.git)浏览器会自动渲染Markdown。

<br>
<!--  -->

> [!TIP]
> 如果您需要安装Python和Python包以及设置代码环境的指导，建议阅读[README.md](setup/README.md) 文件，该文件位于[setup](setup)目录中。

<br>

| **章节标题**                                       | **主要代码（快速访问）**                                                                                                                                               | **所有代码及补充内容**         | **翻译者** | **校对者** |
|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|------------|------------|
| [安装建议](setup)                                 | -                                                                                                                                                                     | -                              | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第1章：理解大型语言模型**                        | 无代码                                                                                                                                                                | -                              | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第2章：处理文本数据**                            | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb) <br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb)（总结） <br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb) | [ch02](./ch02)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第3章：编码注意力机制**                          | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb) <br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb)（总结） <br/>- [exercise-solutions.ipynb](ch03/01_main-chapter-code/exercise-solutions.ipynb) | [ch03](./ch03)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第4章：从零开始实现 GPT 模型**                   | - [ch04.ipynb](ch04/01_main-chapter-code/ch04.ipynb) <br/>- [gpt.py](ch04/01_main-chapter-code/gpt.py)（总结） <br/>- [exercise-solutions.ipynb](ch04/01_main-chapter-code/exercise-solutions.ipynb) | [ch04](./ch04)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第5章：在无标注数据上进行预训练**                | - [ch05.ipynb](ch05/01_main-chapter-code/ch05.ipynb) <br/>- [gpt_train.py](ch05/01_main-chapter-code/gpt_train.py)（总结） <br/>- [gpt_generate.py](ch05/01_main-chapter-code/gpt_generate.py)（总结） <br/>- [exercise-solutions.ipynb](ch05/01_main-chapter-code/exercise-solutions.ipynb) | [ch05](./ch05)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a> |
| **第6章：进行文本分类的微调**                      | - [ch06.ipynb](ch06/01_main-chapter-code/ch06.ipynb) <br/>- [gpt_class_finetune.py](ch06/01_main-chapter-code/gpt_class_finetune.py) <br/>- [exercise-solutions.ipynb](ch06/01_main-chapter-code/exercise-solutions.ipynb) | [ch06](./ch06)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> | <a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a> |
| **第7章：进行遵循指令的微调**                      | - [ch07.ipynb](ch07/01_main-chapter-code/ch07.ipynb) <br/>- [gpt_instruction_finetuning.py](ch07/01_main-chapter-code/gpt_instruction_finetuning.py)（总结） <br/>- [ollama_evaluate.py](ch07/01_main-chapter-code/ollama_evaluate.py)（总结） <br/>- [exercise-solutions.ipynb](ch07/01_main-chapter-code/exercise-solutions.ipynb) | [ch07](./ch07)                | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a> |
| **附录 A：PyTorch 简介**                           | - [code-part1.ipynb](appendix-A/01_main-chapter-code/code-part1.ipynb) <br/>- [code-part2.ipynb](appendix-A/01_main-chapter-code/code-part2.ipynb) <br/>- [DDP-script.py](appendix-A/01_main-chapter-code/DDP-script.py) <br/>- [exercise-solutions.ipynb](appendix-A/01_main-chapter-code/exercise-solutions.ipynb) | [appendix-A](./appendix-A)    | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> |  <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/chunlin.png"  width="80" /></a> |
| **附录 B：参考文献与进一步阅读**                  | 无代码                                                                                                                                                                | -                              |  |   |
| **附录 C：习题解答**                               | 无代码                                                                                                                                                                | -                              |    |   |
| **附录 D：在训练循环中加入附加功能**               | - [appendix-D.ipynb](appendix-D/01_main-chapter-code/appendix-D.ipynb)                                                                                                | [appendix-D](./appendix-D)    | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/chunlin.png"  width="80" /></a> |
| **附录 E：使用 LoRA 进行参数高效微调**            | - [appendix-E.ipynb](appendix-E/01_main-chapter-code/appendix-E.ipynb)                                                                                                | [appendix-E](./appendix-E)    | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/chunlin.png"  width="80" /></a> |

<br>
&nbsp;

下图是本书内容的总结性思维导图。
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg" width="650px">

<br>

<br>

&nbsp;
## 额外材料

| **章节**                                          | **附加资料**                                                                                                                                                                            | **翻译者**    | **校对者**                                                                                                                                       |
|--------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|
| **设置**                                          | - [Python 设置提示](setup/01_optional-python-setup-preferences) <br> - [安装本书中使用的 Python 包和库](setup/02_installing-python-libraries) <br> - [Docker 环境设置指南](setup/03_optional-docker-environment) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>      |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第二章：处理文本数据**                           | - [从零开始实现字节对编码（BPE）分词器](ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb) <br> - [比较不同字节对编码（BPE）实现](ch02/02_bonus_bytepair-encoder) <br> - [理解嵌入层和线性层的区别](ch02/03_bonus_embedding-vs-matmul) <br> - [用简单数字直观理解数据加载器](ch02/04_bonus_dataloader-intuition) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第三章：编码注意力机制**                         | - [比较高效的多头注意力实现](ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb) <br> - [理解 PyTorch 缓冲区](ch03/03_understanding-buffers/understanding-buffers.ipynb) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               |  <a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a> |
| **第四章：从零开始实现 GPT 模型**                  | - [FLOPS 性能分析](ch04/02_performance-analysis/flops-analysis.ipynb)                                                                                                                   | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               |  <a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a> |
| **第五章：在未标注数据上进行预训练**              | - [使用 Transformers 从 Hugging Face 模型库加载替代权重](ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) <br> - [在古腾堡计划数据集上预训练 GPT](ch05/03_bonus_pretraining_on_gutenberg) <br> - [为训练循环添加附加功能](ch05/04_learning_rate_schedulers) <br> - [优化预训练的超参数](ch05/05_bonus_hparam_tuning) <br> - [构建与预训练 LLM 交互的用户界面](ch05/06_user_interface) <br> - [将 GPT 转换为 Llama](ch05/07_gpt_to_llama) <br> - [从零开始构建 Llama 3.2](ch05/07_gpt_to_llama/standalone-llama32.ipynb) <br> - [内存高效的模型权重加载](ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb) <br> - [扩展 Tiktoken BPE 分词器，添加新 Token](ch05/09_extending-tokenizers/extend-tiktoken.ipynb) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               |  <a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a> |
| **第六章：用于分类的微调**                         | - [微调不同层并使用更大模型的额外实验](ch06/02_bonus_additional-experiments) <br> - [在 50k IMDB 电影评论数据集上微调不同模型](ch06/03_bonus_imdb-classification) <br> - [构建与 GPT 基于垃圾邮件分类器交互的用户界面](ch06/04_user_interface) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               | <a href="https://github.com/lul0308">  <img src="./imgs/profile/chunlin.png"  width="80" /></a>  |
| **第七章：微调以跟随指令**                         | - [查找近重复项和创建被动语态条目的数据集工具](ch07/02_dataset-utilities) <br> - [使用 OpenAI API 和 Ollama 评估指令响应](ch07/03_model-evaluation) <br> - [为指令微调生成数据集](ch07/05_dataset-generation/llama3-ollama.ipynb) <br> - [改进指令微调数据集](ch07/05_dataset-generation/reflection-gpt4.ipynb) <br> - [使用 Llama 3.1 70B 和 Ollama 生成偏好数据集](ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb) <br> - [用于 LLM 对齐的直接偏好优化（DPO）](ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) <br> - [构建与指令微调的 GPT 模型交互的用户界面](ch07/06_user_interface) | <a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a>                                               |  <a href="https://github.com/lul0308">  <img src="./imgs/profile/chunlin.png"  width="80" /></a> |

<br>

&nbsp;

## 硬件要求

本书主要章节中的代码设计为能够在常规笔记本电脑上运行，并且不会占用过长时间，因此不需要专门的硬件。这种方式确保了广泛的读者群体能够参与其中。此外，如果有可用的 GPU，代码会自动使用它们。（更多建议请参考 [setup](https://github.com/MLNLP-World/LLMs-from-scratch-CN.git/blob/main/setup/README.md) 文档。）

&nbsp;


## 问题、反馈和贡献

欢迎各种形式的反馈，最好通过 [Manning 论坛](https://livebook.manning.com/forum?product=raschka&page=1) 或 [GitHub 讨论区](https://github.com/rasbt/LLMs-from-scratch/discussions) 分享。如果你有任何问题或只是想与他人讨论想法，也请随时在论坛中发布。

请注意，由于本存储库包含与印刷书籍相对应的代码，因此目前无法接受扩展主要章节代码内容的贡献，因为这可能会导致与实体书籍的内容不一致。保持一致性有助于确保每个人的顺畅体验。

&nbsp;
## 引用

如果你发现本书或代码对你的研究有帮助，请考虑引用它。

引用：

> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.

BibTeX 条目：
```
@book{build-llms-from-scratch-book,
author       = {Sebastian Raschka},
title        = {Build A Large Language Model (From Scratch)},
publisher    = {Manning},
year         = {2024},
isbn         = {978-1633437166},
url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
github       = {https://github.com/rasbt/LLMs-from-scratch}
}
```


<h2 id="贡献者">
  <img src="./imgs/icon/heart.png" width="25" /> 贡献者
</h2>

<a href="https://github.com/GoatCsu">  <img src="./imgs/profile/GoatCsu.png"  width="80" /></a> 
<a href="https://github.com/BRZ911">  <img src="./imgs/profile/yongheng.jpg"  width="80" /></a>
<a href="https://github.com/WPENGxs">  <img src="./imgs/profile/wpengxs.jpg"  width="80" /></a>
<a href="https://github.com/lul0308">  <img src="./imgs/profile/chunlin.png"  width="80" /></a>
<a href="https://github.com/llm-chaser">  <img src="./imgs/profile/llm-chaser.jpeg"  width="80" /></a>
</file>

</files>
